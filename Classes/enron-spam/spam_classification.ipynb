{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam/ham classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will train a classifier to recognise spam emails. The data was taken from this [enron spam](https://www.kaggle.com/wanderfj/enron-spam)  kaggle dataset. The dataset is described [here](https://www.researchgate.net/publication/221650814_Spam_Filtering_with_Naive_Bayes_-_Which_Naive_Bayes). This will familiarize you with tools  for text analysis  from  the scikit-learn library. \n",
    "\n",
    "The data is provided as a zip archive \"enron_spam.zip\". You can unpack it with command `unzip enron_spam.zip` which will create a directory `data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data  consists of 6 batches of emails. Each batch corresponds to different person and different spam source. Each batch is stored in different subdirectory of `data` named `Enron_1` to `Enron_6`. Each email is stored in separate file :( Spam and ham emails are stored in different subdirectories. Fortunatelly the scikit-learn library provides a functiona `load_files` that can read data in this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load data from given batch using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = load_files('data/Enron_1/', encoding='latin-1', shuffle=True, categories=['ham', 'spam'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a python dictionary. The entry 'data' contains the data from files and the entry 'target' contains the labels assigned according to the subdirectory names. The labels are integers and corresponding names can be found in the entry 'target_names'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['data'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['target'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the multinomial bayes classifier using scikit-learn  for the first batch of data in data/Enron_1 as described in text_analysis notebook.\n",
    "    1. Set asside 20% of data for testings and train the multinomial bayes classifier using the remaining 80% of data.This requires  transforming the data to feature vectors using CountVectoriser  from scikit-learn. How big is the resulting vocabulary ? \n",
    "    1. Using the test set \n",
    "        1. Draw the confusion matrix using `plot_confusion_matrix` from scikit-learn (see the latest update of the text_analysis notebook) using the test set. \n",
    "        1. Calculate recall and precision scores. \n",
    "        1. Draw the ROC curve and calculate the AUC score  using the test set. \n",
    "        1. What percentage of valid mails is classified as spam?\n",
    "        1. Assuming that only mails classified as ham are put in our mailbox what percentage of mail in our inbox is spam?\n",
    "        \n",
    "    1. Find ten most probable and least probable words for each class.     \n",
    "    1. Check the classifier on the remaining datasets data/Enron_2-6. For each set calculate recall and precision. \n",
    "    1. Combine all sets.  Train a new classifier on the combined  set, of course after dividing into test and train sets. Redo point B. using this classifier and combined test set. \n",
    "    1. Assumimg that we want to keep the frequency of misclassified ham mails belowe 5 per mile, what would be the percentage of spam in our inbox?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint__ You can create a dataframe from the data using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'text': data1['data'], 'spam': data1['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.spam.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
