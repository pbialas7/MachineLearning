{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we continue with the probabilistic methods of Machine Learning. Please recall that we are trying the estimate the conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=x|Y=y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $X$ represents our data or features and $Y$ is the  class label. Using Bayes theorem we can then infer the \"inverted\" conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Y=y|X=x)=\n",
    "\\frac{P(X=x|Y=y)P(Y=y)}\n",
    "{\\sum_y P(X=x|Y=y)P(Y=y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the probability that examplar $x$  belongs to class $y$. This probability can be then used to construct a classifier by suitable thresholding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X$ is a categorical variable with finite support, we can list all the probabilities. In case of continous  features we have to bin the data or use some parametrized distribution. The most widely used distribution is of course the Normal (Gaussian) distribution. We have already used  it  in our \"sex  from height and weight\" example. So far we have only used the one dimensional distribution, fitting separately height and weight (or rather BMI). In this notebook we will try to estimate the joint height-weight conditional probability distribution directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the data (we will use the same [kaggle dataset](https://www.kaggle.com/mustafaali96/weight-height)) and converting it to metric units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../Data/HeightWeight/weight-height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inch = 0.01 * 2.54 # m\n",
    "pound = 0.453 # kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Height'] = data['Height'] *inch \n",
    "data['Weight'] = data['Weight'] *pound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we will split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by fixing the seed we guarantee that we can split the set in same way each time\n",
    "#this maybe handy for debuging purposes\n",
    "seed = 77678 \n",
    "train_data, test_data  = train_test_split(data,test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_F = train_data[train_data.Gender=='Female']\n",
    "train_data_M = train_data[train_data.Gender=='Male']\n",
    "print(\"train \", len(train_data_F), len(train_data_M))\n",
    "test_data_F = test_data[test_data.Gender=='Female']\n",
    "test_data_M = test_data[test_data.Gender=='Male']\n",
    "print(\"test  \", len(test_data_F), len(test_data_M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the classes (gender) are balanced  across the training and testig sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting colors for consistency throughout the notebook you can subsitute here your favorite \"gender colors\" :) \n",
    "f_color = 'blue'\n",
    "m_color ='orange'\n",
    "color = 'lightgrey'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution for each gender looks as follows on scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data_F.Height, train_data_F.Weight, alpha=0.2, c = f_color, label='Women');\n",
    "plt.scatter(train_data_M.Height, train_data_M.Weight, alpha=0.2, c = m_color, label='Men');\n",
    "plt.legend();\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and on histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Height [m]');\n",
    "ax.set_ylabel('Weight [kg]');\n",
    "hb = ax.hist2d(train_data['Height'], train_data['Weight'], bins=[100,80], density=True);\n",
    "fig.colorbar(hb[3], ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can get a better looking picture with hexagonal bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Height [m]')\n",
    "ax.set_ylabel('Weight [kg]')\n",
    "hb = ax.hexbin(train_data['Height'], train_data['Weight']);\n",
    "fig.colorbar(hb);\n",
    "ax.set_xlim(1.4,2.0);\n",
    "ax.set_ylim(35,120);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be using those plots often, for convenience I have wrapped them in few functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "fold"
    ]
   },
   "outputs": [],
   "source": [
    "def hw_plot(**kwargs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('Height [m]')\n",
    "    ax.set_ylabel('Weight [kg]')\n",
    "    return fig,ax\n",
    "\n",
    "def hw_scatter(fig,ax, F, M):\n",
    "    ax.scatter(F.Height, F.Weight, alpha=0.2, c = f_color, label='Women');\n",
    "    ax.scatter(M.Height, M.Weight, alpha=0.2, c = m_color, label='Men');\n",
    "    ax.legend();\n",
    "\n",
    "def hw_hist(fig, ax, df):\n",
    "    hb = ax.hist2d(train_data['Height'], train_data['Weight'], bins=[100,80], density=True);\n",
    "    fig.colorbar(hb[3], ax=ax);\n",
    "    return hb\n",
    "\n",
    "def hw_hexbin(fig, ax, df, **kwargs):\n",
    "    hb = ax.hexbin(df['Height'], df['Weight'], **kwargs);\n",
    "    fig.colorbar(hb);\n",
    "    return hb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find the joint probability density function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(h,w|S=s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we will assume is a _multivariate gaussian distribution_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\mathbf{x}|S=s)\\sim \\mathcal{N}(\\mu_s,\\Sigma_s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability density function of this distribution in $D$ dimensions  is given by the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\b}[1]{\\mathbf{#1}}$$\n",
    "$$P(\\mathbf{x}|\\b\\mu,\\b\\Sigma)=\n",
    "\\frac{1}{(2\\pi)^{D/2}|\\mathbf\\Sigma|^{1/2}}\n",
    "e^{\\displaystyle-\\frac{1}{2}\\left(\\mathbf{x}-\\b\\mu\\right)^T\\mathbf{\\Sigma}^{-1}\\left(\\mathbf{x}-\\b\\mu\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{x}$ and $\\mathbf{\\mu}$ are $D$-dimensional vectors $\\mathbf{\\mu}$ being the mean or the center of the distribution. $\\mathbf{\\Sigma}$ is $D\\times D$ dimensional symmetric _covariance_ matrix. $|\\b\\Sigma|$ denotes the _determinant_ of the matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__ How many parameters does the model have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$\\mathbf{\\mu}$ nas $D$ parameters and a symmetric matrix  has $(D^2-D)/2+D=D(D+1)/2$ parameters giving altogether $D(D+3)/2$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE estimate of multivariate gaussian parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\b X$  be $N\\times D$ the measurement matrix. Each row is a $D$ dimensional vector of measurements $\\b X_i$. Assumimg that all measurments come from the Multivariate  Gaussian distribution with parameters $\\b\\mu$ and  $\\Sigma$ the likelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\prod_{i=1}^N P(\\b X_i|\\b\\mu,\\b\\Sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the log likelihood is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^N \\log P(\\b X_i|\\b\\mu,\\b\\Sigma) =  \n",
    "- N \\frac{1}{2}\\log |\\Sigma| -\\sum_{i=1}^N\\frac{1}{2}\\left(\\mathbf{X_i}-\\mu\\right)^T\\mathbf{\\Sigma}^{-1}\\left(\\mathbf{X_i}-\\mu\\right) + const$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last term can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\tr}{\\operatorname{Tr}}$$\n",
    "$$\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j,k=1}^D\n",
    "\\left(X_{ij}-\\mu_j\\right)\n",
    "\\left(\\b{\\Sigma}^{-1}\\right)_{jk}\n",
    "\\left(X_{ik}-\\mu_k\\right)\n",
    "=\\frac{N}{2}\\tr \\b C \\,\\b\\Sigma^{-1} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\b C$ is $D \\times D$  empirical covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_{jk}=\\frac{1}{N}\\sum_{i}\\left(X_{ij}-\\mu_j\\right)\\left(X_{ik}-\\mu_k\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tr$ is the _trace_ operator (the sum of the elements on the diagonal):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tr \\b A\\equiv \\sum_{i=1}^{D} A_{ii}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the MLE estimates we have to differentiate this expression with respect to $\\b \\mu$ and $\\Sigma$. Let's start with $\\b \\mu$. The likelihood depends on $\\mu$ only trough the matrix $\\b C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial \\mu_m}C_{jk}\n",
    "=\\frac{1}{N}\\sum_{i}\\frac{\\partial}{\\partial \\mu_m}\\left(X_{ij}-\\mu_j\\right)\\left(X_{ik}-\\mu_k\\right)\n",
    "=-\\frac{1}{N}\\sum_{i}\\left(\\delta_{m,j}\\left(X_{ik}-\\mu_k\\right) + \\delta_{m,k}\\left(X_{ij}-\\mu_j\\right)\\right)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial \\mu_m}C_{jk}=-\\delta_{m,j}\\left(\\overline{ x}_{k}-\\mu_k\\right) - \\delta_{m,k}\\left(\\overline{ x}_j-\\mu_j\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2}\\frac{\\partial}{\\partial \\mu_m}\\left(\\tr \\b C \\,\\b\\Sigma^{-1}\\right) = \n",
    "\\frac{1}{2}\\left(\\frac{\\partial}{\\partial \\mu_m}\\tr \\b C\\right) \\,\\b\\Sigma^{-1} = \n",
    "-\\frac{1}{2}\\sum_{jk}\n",
    "\\left(\n",
    "\\delta_{m,j}\\left(\\overline{ x}_{k}+\\mu_k\\right) - \\delta_{m,k}\\left(\\overline{ x}_j-\\mu_j\\right))\n",
    "\\right)\n",
    "\\left(\\b\\Sigma^{-1}\\right)_{kj}=-\\sum_{k}\\left(\\b\\Sigma^{-1}\\right)_{mk}\\left(\\overline{ x}_{k}+\\mu_k\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have used the fact that $\\Sigma$ is a symmetric matrix. Or in vector notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2}\\frac{\\partial}{\\partial \\b \\mu}\\tr \\b C \\,\\b\\Sigma^{-1} = -\\b\\Sigma^{-1}(\\b x -\\b \\mu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming thar $\\b\\Sigma^{-1}$ is not singular, the solution to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\b\\Sigma^{-1}(\\overline{\\b x} -\\b \\mu)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equation is unsurprisingly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\b \\mu = \\overline{\\b x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differentiation with respect to $\\b\\Sigma$ is more difficult and instead we will look for the inverse matrix $\\b A= \\b\\Sigma^{-1}$.  Because the determinant of the inverse matrix is the inverse of the determinant the expression we have to minimize is  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2}\\log |\\b A|-\\frac{1}{2}\\tr \\b C \\,\\b A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating with respect to element $A_{kl}$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{2}\\frac{\\partial}{\\partial A_{kl}}\\log |\\b A|-\\frac{1}{2}\\frac{\\partial}{\\partial A_{kl}}\\tr \\b C \\,\\b A \n",
    "=\n",
    "\\frac{1}{2}\\frac{\\partial}{\\partial A_{kl}}\\log |\\b A|-\\frac{1}{2}C_{lk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__  Show that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial A_{kl}}\\log |\\b A|  = \\left({\\b A}^{-1}\\right)_{lk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint__ Use the Laplace expansion of the determinant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above epression we obtain finally the equation for MLE of $\\b A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\left({\\b A}^{-1}\\right)_{lk}\\equiv \\Sigma_{lk}=C_{lk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the  again unsurprisingly the covariance matrix is approximated by its empirical value. As we will find out this estimatiom runs into severe problems in higher dimension but for the moment we can use it \"as is\" for our classifier. Below are two helper functions to calculate those estimators and to construct a corresponing distribution object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_stats(X):\n",
    "    mu = np.mean(X,0)\n",
    "    sigma = np.cov(X,rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def mv_gaussian_mle_fit(X):\n",
    "    mu, sigma = gaussian_stats(X)\n",
    "    return st.multivariate_normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In two dimensions we can visualise the distribution by drawing the countour lines. For multivariate gaussian those are ellipses. I have included a function to plot \n",
    "ellipses of given radius expressed in units of standard deviation $\\sigma$. It is adapted from [Plot a confidence ellipse of a two-dimensional dataset](https://matplotlib.org/3.2.1/gallery/statistics/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py)  and described [here](https://carstenschelp.github.io/2018/09/14/Plot_Confidence_Ellipse_001.html). If you have cloned my repo you can use it by importing the `plotting` module from `mchlearn` package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\") #need to add main repo directory to the system path used for searching packages\n",
    "from mchlearn.plotting import confidence_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_F, cov_F = gaussian_stats(train_data_F[['Height', 'Weight']])\n",
    "mu_M, cov_M = gaussian_stats(train_data_M[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default ellipsed are drawn with a three $\\sigma$ radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = hw_plot()\n",
    "hw_scatter(fig, ax, train_data_F, train_data_M)\n",
    "confidence_ellipse(mu_F, cov_F, ax, edgecolor = f_color)\n",
    "confidence_ellipse(mu_M, cov_M, ax, edgecolor = m_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the class priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually our model has some more parameters: those are the _a priori_ probabilities of each class. We can either set them \"by hand\" as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_F = 0.5\n",
    "p_M = 1- p_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can fit them from the data using MLE using the joint probability distribution both on labels $y_i$ and features $\\b X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\b X,\\b y| \\theta) = \\prod_{i=1}^N  P(\\b X_i|\\b\\mu_{y_i},\\b\\Sigma_{y_i})\\pi_{y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\theta$ denotes all parameter of the model: $\\mu$, $\\Sigma$  and prior $\\pi$ for each class.  The log likehood is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^N \\left(\\log \\pi_{y_i} +\\log P(\\b X_i|\\b\\mu_{y_i},\\b\\Sigma_{y_i}) \\right) = \n",
    "\\sum_c N_c \\log \\pi_c +\\sum_c \\sum_{i:y_i=c}\\log P(\\b X_i|\\b\\mu_{y_i},\\b\\Sigma_{y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_c$ denotes the number of samples in each class. For the last term we proceed as before fitting $\\b\\mu$ and $\\b\\Sigma$ for each class. The priors require slightly more caution. We cannot just differentiate with respect to  priors $\\pi_c$ because the priors are not independent as their sum must be equal one.  We will have to use the [Lagrange multipliers method](https://en.wikipedia.org/wiki/Lagrange_multiplier).  What we differentiate is the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_c N_c \\log \\pi_c+\\lambda \\sum_c \\pi_c$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating with respect to $\\pi_d$ leads to equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{N_d}{\\pi_d}+\\lambda =0\\quad\\text{or}\\quad \\pi_d = -\\frac{N_d}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjustig $\\lambda$ as to fulfill the constraint gives us the final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_d = \\frac{N_d}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the distributions in each class and class priors  we can finally calculate the probability of a person being a woman by Bayes theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(S=f|h,w) = \\frac{P(h,w|S=f) P(S=f)}{P(h,w|S=f) P(S=f)+P(h,w|S=m) P(S=m)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_F = mv_gaussian_mle_fit(train_data_F[['Height', 'Weight']])\n",
    "dist_M = mv_gaussian_mle_fit(train_data_M[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_F_cond_HW(hw):\n",
    "    pf = dist_F.pdf(hw)*p_F\n",
    "    pm = dist_M.pdf(hw)*p_M\n",
    "    return pf/(pf+pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate all the usuall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_proba_gda = prob_F_cond_HW(test_data[['Height','Weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(test_data.Gender=='Female', test_predicted_proba_gda>0.5, normalize='true'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame is used only for \"pretty printing\". The true labels correspond to rows and predicted to columns. The normalisation set to  'true' mean that the numbers in each row add up to one. So the matrix above corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "fold"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({0: ['TNR', 'FPR'], 1: ['FPR','TPR']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import roc_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_roc_curve(fig, ax, y_true, y_score, name):\n",
    "    fprs, tprs, thds = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    ax.plot(fprs, tprs, label=\"{:s}  {:5.3f}\".format(name, auc));\n",
    "    return fprs, tprs, thds, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot()\n",
    "add_roc_curve(fig, ax, test_data.Gender=='Female', test_predicted_proba_gda, \"GDA\")\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look back at our Naive Bayes Height-BMI classfier you can see that the performance is almost identical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an insight into the working of this classifier we will look at the decisions boundaries: lines or surfaces separating the regions of $R^D$ corresponding to each class.  E.g. in our example this will be the line defined by equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(F|h,w)=\\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize this boundary I will draw the countour line(s) of $P(F|h,w)$. This  is slightly technical  and I have provided a function that generates the data\n",
    "suitable to be used directly in the matplotlib `contour`  and `contourf` functions.  Those functions can be found in the `plotting` module in the `mchlearn` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = np.linspace(1.3, 2.1,100)\n",
    "ws = np.linspace(30,120, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = hw_plot()\n",
    "hw_scatter(fig, ax, train_data_F, train_data_M)\n",
    "confidence_ellipse(mu_F, cov_F, ax, edgecolor = f_color)\n",
    "confidence_ellipse(mu_M, cov_M, ax, edgecolor = m_color)\n",
    "cs = ax.contour(*grid(hs,ws,prob_F_cond_HW), [0.25, 0.5, 0.75], colors=['coral','red', 'coral']);\n",
    "ax.clabel(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have included boundaries for three different thresholds. Here is the same picture but using the histogram instead of scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = hw_plot()\n",
    "hw_hexbin(fig,ax, train_data)\n",
    "ax.set_xlim(1.4,2.0)\n",
    "confidence_ellipse(mu_F, cov_F, ax, edgecolor = f_color)\n",
    "confidence_ellipse(mu_M, cov_M, ax, edgecolor = m_color)\n",
    "cs = ax.contour(*grid(hs,ws,prob_F_cond_HW), [0.25, 0.5, 0.75], colors=['coral','red', 'coral']);\n",
    "ax.clabel(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the decision boundary is not a straight line. Actually we can show that this is a quadratic curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A quadratic surface is a ensemble of all points $\\b x$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F(\\b x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and function $F$ is a polynomial of degree at most two in the coordinates of the vector $x$. In two dimension this can be an ellipse, a parabola, a hyperbola, two intersecting lines and parallel lines.  E.g. the ellipse is given by the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a x_1^2 +b x_2^2-c = 0,\\qquad a,b,c > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__ Give the equation for two intersecting lines and two parallel lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem",
     "hint"
    ]
   },
   "source": [
    "__Hint__ The line is defined by equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hint"
    ]
   },
   "source": [
    "$$a x_1+ b x_2 -d =0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general case of more then two classes  the decision boundary between classes is a piecewise quadratic surface. More specifically, a decision boundary between any two classes is a quadratic surface. That's why this method is called _Quadratic Discriminant Analysis_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show this let's write again the conditional class probability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Y=c|X=x)=\\frac{P(X=x|Y=c)\\pi_c}{\\sum_c P(X=x|Y=c)\\pi_c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the usuall majority rule the classifier will return the class with the bigest probability. Because the denominator  does not depend on $c$ this is same as picking biggest of all $P(X=x|Y=c)\\pi_c$. That is classifier picks class $c$ iff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(X=x|Y=c)\\pi_c > P(X=x|Y=c')\\pi_{c'}, \\text{ for all } c'\\neq c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a region that is a intersection of $K-1$ regions defined by a single inequality  $P(X=x|Y=c)\\pi_c > P(X=x|Y=c')\\pi_{c'}$. The boundary of this region is defined by the equality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=x|Y=c)\\pi_c = P(X=x|Y=c')\\pi_{c'}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the multivariate gaussian distribution we obtain equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\pi_c}{(2\\pi)^{D/2}|\\b\\Sigma_c|^{1/2}}\n",
    "e^{\\displaystyle-\\frac{1}{2}\\left(\\mathbf{x}-\\b\\mu_c\\right)^T{\\b\\Sigma_c}^{-1}\\left(\\mathbf{x}-\\b\\mu_c\\right)}\n",
    "=\n",
    "\\frac{\\pi_{c'}}{(2\\pi)^{D/2}|\\b\\Sigma_{c'}|^{1/2}}\n",
    "e^{\\displaystyle-\\frac{1}{2}\\left(\\mathbf{x}-\\b\\mu_{c'}\\right)^T{\\b\\Sigma_{c'}}^{-1}\\left(\\mathbf{x}-\\b\\mu_{c'}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the logarithm of both sides and droping common terms we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log\\pi_c-\n",
    "\\frac{1}{2}\\log|\\b\\Sigma_c|^{1/2}\n",
    "-\\frac{1}{2}\\left(\\mathbf{x}-\\b\\mu_c\\right)^T{\\b\\Sigma_c}^{-1}\\left(\\mathbf{x}-\\b\\mu_c\\right)\n",
    "=\n",
    "\\log\\pi_{c'}-\\frac{1}{2}\\log |\\b\\Sigma_{c'}|^{1/2}\n",
    "-\\frac{1}{2}\\left(\\mathbf{x}-\\b\\mu_{c'}\\right)^T{\\b\\Sigma_{c'}}^{-1}\\left(\\mathbf{x}-\\b\\mu_{c'}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a quadratic equation in $\\b x$.  So the final region is an intersection of regions with quadratic boundaries, so its boundary is piecewise quadratic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we had ample data to fit the distributions. Each distribution requires five parameters so we have ten parameters in  total but the training set contains few thousands examples. Moreover  the classes are balanced. However the number of parameters grows quadratically with the number of dimensions. On (brutal) way of  reducing the number of parameters is to consider only the _diagonal_ $\\b\\Sigma$ matrices. This mean that we treat all features as conditionally independent so this is just Naive Bayes we have already considered.  We will redo this example here but this time  with  height and weight.  Those variables  are evidently correlated, this will enable us to see more clearly what kind of approximations we are making. We will also use the functions supplied in the scikit-learn library instead of constructing the classifier \"by hand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cls = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cls.fit(train_data[['Height', 'Weight']], train_data.Gender=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard classifiers in sklearn return probabilities for all the classes. In case of binary classifier we need only one. \n",
    "test_predicted_proba_nb = nb_cls.predict_proba(test_data[['Height', 'Weight']])[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(test_data.Gender=='Female', test_predicted_proba_nb>0.5, normalize='true')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot()\n",
    "add_roc_curve(fig,ax, test_data.Gender=='Female', test_predicted_proba_gda, \"QDA\")\n",
    "add_roc_curve(fig,ax, test_data.Gender=='Female', test_predicted_proba_nb, \"NB\")\n",
    "ax.legend(title='AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance  of this classifer is not that much worse! Let's look at the fitted distributions and the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = hw_plot()\n",
    "hw_scatter(fig,ax,train_data_F, train_data_M)\n",
    "confidence_ellipse(nb_cls.theta_[1], np.diag(nb_cls.sigma_[1]), ax, edgecolor = f_color)\n",
    "confidence_ellipse(nb_cls.theta_[0], np.diag(nb_cls.sigma_[0]), ax, edgecolor = m_color)\n",
    "cs_qda = ax.contour(*grid(hs,ws,prob_F_cond_HW),  levels=[0.5], colors=['red']);\n",
    "ax.clabel(cs_qda,[0.5], fmt=\"QDA\");\n",
    "cs_nb = ax.contour(*grid(hs,ws,lambda a: nb_cls.predict_proba(a)[:,1]), levels=[0.5], colors=['darkviolet']);\n",
    "ax.clabel(cs_nb,[0.5], fmt=\"NB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions are wide off the mark. Naive Bayes has constrained them to by parallel to the axes.  The decision boundary is also different, but it manages to separate clearly the centers of the clusters from each other. And that's where most of the samples are. This can be better seen on the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hw_hexbin(fig,ax, train_data, extent=(1.3,2.1,30, 120))\n",
    "confidence_ellipse(nb_cls.theta_[1], np.diag(nb_cls.sigma_[1]), ax, edgecolor = f_color)\n",
    "confidence_ellipse(nb_cls.theta_[0], np.diag(nb_cls.sigma_[0]), ax, edgecolor = m_color)\n",
    "cs_qda = ax.contour(*grid(hs,ws,prob_F_cond_HW),  levels=[0.5], colors=['red']);\n",
    "ax.clabel(cs_qda,[0.5], fmt=\"QDA\");\n",
    "cs_nb = ax.contour(*grid(hs,ws,lambda a: nb_cls.predict_proba(a)[:,1]), levels=[0.5], colors=['darkviolet']);\n",
    "ax.clabel(cs_nb,[0.5], fmt=\"NB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violet line also clearly separates the two peaks which explains not so bad performance of the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of reducing the number of parameters is to fit  only one common $\\b\\Sigma$ matrix. That's we assume that all the classes have similar \"shape\" and differ only by the localisation of centers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__ Show that in this case decisions boundaries are straight lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this property This approach is called _Linear Discriminative Analysis_ and you have already encountered it on previous lectures. \n",
    "Let's try it out on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cls = LinearDiscriminantAnalysis(store_covariance=True) # we will need the covariance matrix later for illustrative purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cls.fit(train_data[['Height', 'Weight']], train_data.Gender=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_proba_lda = lda_cls.predict_proba(test_data[['Height', 'Weight']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(test_data.Gender=='Female', test_predicted_proba_lda>0.5, normalize='true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot()\n",
    "add_roc_curve(fig,ax, test_data.Gender=='Female', test_predicted_proba_gda, \"QDA\")\n",
    "add_roc_curve(fig,ax, test_data.Gender=='Female', test_predicted_proba_nb, \"NB\")\n",
    "add_roc_curve(fig,ax, test_data.Gender=='Female', test_predicted_proba_lda, \"LDA\")\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this classifier is undistinguishable from performance of the Quadratic Discriminator.  This is because the two distributions have roughly same shape so the approximation is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = hw_plot()\n",
    "hw_scatter(fig,ax, train_data_F, train_data_M)\n",
    "confidence_ellipse(nb_cls.theta_[1], lda_cls.covariance_, ax, edgecolor = f_color)\n",
    "confidence_ellipse(nb_cls.theta_[0], lda_cls.covariance_, ax, edgecolor = m_color)\n",
    "cs_qda = ax.contour(*grid(hs,ws,prob_F_cond_HW),  levels=[0.5], colors=['red']);\n",
    "ax.clabel(cs_qda,[0.5], fmt=\"QDA\");\n",
    "cs_nb = ax.contour(*grid(hs,ws,lambda a: nb_cls.predict_proba(a)[:,1]), levels=[0.5], colors=['darkviolet']);\n",
    "ax.clabel(cs_nb,[0.5], fmt=\"NB\");\n",
    "cs_lda = ax.contour(*grid(hs,ws,lambda a: lda_cls.predict_proba(a)[:,1]), [0.5], colors=['green']);\n",
    "ax.clabel(cs_lda,[0.5], fmt=\"LDA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the LDA decision boundary (that indeed is a straight line) is very close to quadratic boundary in the region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hw_hexbin(fig, ax, train_data, extent=(1.3, 2.1, 30, 120))\n",
    "confidence_ellipse(nb_cls.theta_[1], lda_cls.covariance_, ax, edgecolor = f_color)\n",
    "confidence_ellipse(nb_cls.theta_[0], lda_cls.covariance_, ax, edgecolor = m_color)\n",
    "cs_qda = ax.contour(*grid(hs,ws,prob_F_cond_HW),  levels=[0.5], colors=['red']);\n",
    "ax.clabel(cs_qda,[0.5], fmt=\"QDA\");\n",
    "cs_nb = ax.contour(*grid(hs,ws,lambda a: nb_cls.predict_proba(a)[:,1]), levels=[0.5], colors=['darkviolet']);\n",
    "ax.clabel(cs_nb,[0.5], fmt=\"NB\");\n",
    "cs_lda = ax.contour(*grid(hs,ws,lambda a: lda_cls.predict_proba(a)[:,1]), [0.5], colors=['green']);\n",
    "ax.clabel(cs_lda,[0.5], fmt=\"LDA\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}