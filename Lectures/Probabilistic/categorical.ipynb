{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (Naive) Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we have shown in the previous lectures training a Bayes classifier amounts to finding the conditional probability distributions of the features $\\mathbf{x}$ given the class label $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(\\mathbf{X}=\\mathbf{x}|C=c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In case of the Naive bayes classifier the $n_f$ features are conditionally independent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\mathbf{X}=\\mathbf{x}|C=c)=\\prod_{i=0}^{n_f-1} P(X_i=x_i|C=c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "so we can estimate   probality distribution  for each feature separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of categorical features each $X_i$ has a finite $m_i$  number of possible values (categories) that, without any loss of generality we can  assume, take values $0,\\ldots,m_i-1$. In the same way we  will assume that the class labels take $n_c$ integer values $c=0,\\ldots,n_c-1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each feature we have  to estimate  $n_c\\times m_i$ probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p^{(c)}_{ij} = P(X_i = j|C = c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course they are not all idependent. Normalisation requires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{j=0}^{m_i-1} p^{(c)}_{ij}=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{x}$ denote the $n_s \\times n_f$ matrix of training data where $n_s$ is the number of samples. So $x_{hi}$ denotes the value of the ith   feature in  sample $h$. Let  $y_h$ denote the corresponding label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{hi} =0,\\ldots,m_i-1,\\quad  y_h=0,\\ldots,k-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce some more notation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{x}^{c}$ will denote the sets of all data points with label $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{x}^c\\equiv\\{\\mathbf{x}_h: y_h=c\\}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of elements in $\\mathbf{x}^{(c)}$ will be denoted by $n^c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $n^{(c)}_{ij}$ denote the number of times that feature $i$ had value $j$ in samples with  class label $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$n^{(c)}_{ij} = \n",
    "\\sum_{h=0}^{n_s-1} \\delta_{x_{hi},j},\\qquad \\delta_{a,b}=\n",
    "\\begin{cases}\n",
    "1 & a=b\\\\\n",
    "0 & a\\neq b\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a smoothed estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$p^{(c)}_{ij} = \\frac{n^{(c)}_{ij}+\\alpha}{n^c+m_i\\alpha} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\alpha\\ge 0$ is a smoothing parameter. The use of  non-zero smoothing parameter ensures a non-vanishing probability even when $n^{(c)}_{ij}=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Car evaluation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we will use the [car evaluation dataset](http://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from [UCI Machine Learning repository](http://archive.ics.uci.edu/ml/). It contains 1728 samples with six atttributes (features) each. The class label is the evaluation of the car: unacc, acc, good, vgood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All six parameters are categorical and the data contains exactly one sample for each possible combination of attributes values (in this respect this is quite peculiar dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we will use pandas to read and proccess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data = pd.read_csv(\"../../Data/Cars/car_data.csv\", names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `groupby`  divides the data frame into goups based on the value of the given colum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_by_class = cars_data.groupby('class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of each group can be calculated using method `size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_by_class.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the classes are not very well ballanced with relatively small number of cars in two best  classes.  So I have decided to join those two classes together introducing a new classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bargain(c):\n",
    "    if c in ['good', 'vgood']:\n",
    "        return 'good'\n",
    "    elif c=='acc':\n",
    "        return 'fair'\n",
    "    else:\n",
    "        return 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data['bargain'] = cars_data['class'].apply(bargain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by dividing the data set into training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 678565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_train, cars_test = train_test_split(cars_data, train_size=0.75, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_train['bargain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_test['bargain'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `train_test_split` has an option to _stratify_ data based on the values of one colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_train, cars_test = train_test_split(cars_data, train_size=0.75, stratify=cars_data['bargain'],\n",
    "                                         random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way the split was done separately for each  class label. That way we obtain as a result slightly more balanced sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_train['bargain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_test['bargain'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Naive Bayes.\n",
    "There are many ways that we can calculate the estimators. We can start by grouping the dataframe according to  feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_training_grouped = cars_train.groupby(['bargain', \n",
    "                                            'buying',\n",
    "                                            'maint',\n",
    "                                            'doors', \n",
    "                                            'persons', \n",
    "                                            'lug_boot',\n",
    "                                            'safety'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and count the size of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts=cars_training_grouped.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sum` method can make a partial sums (see Titanic problem) which we can use to extract $n^{c}_{ij}$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The level argument list the levels not summed over i.e. left in the result.\n",
    "group_counts.sum(level = ['bargain', 'buying'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally calculate the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(group_counts.sum(level = ['bargain', 'buying'] ) +1 )/(group_counts.sum(level='bargain')+4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if you look closely you will notice one problem: the 'good' class does not contain any values for 'high' and 'vhigh'  attribute values! That means of course that they are zero, but it complicates the calculations. Instead of fixing this by hand we will use the tools from the `scikit-learn` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library has a class implementing just what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb = CategoricalNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this classifier requires the  class labels and attributes to be  integer numbers counted from zero. Fortunatelly scikit-learn also posses a class for converting from labels to ordinals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoder = OrdinalEncoder()\n",
    "encoded_features = features_encoder.fit_transform(cars_train.loc[:,'buying':'safety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_encoder = OrdinalEncoder()\n",
    "encoded_class = np.ravel(class_encoder.fit_transform(cars_train.loc[:,'bargain':]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(encoded_features[encoded_class==0][:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb.fit(encoded_features, np.ravel(encoded_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the learned probabilities of this classifier  using its `feature_log_prob` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp( cnb.feature_log_prob_[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with our calculations we see a almost perfect fit. However we get all four probabilities in the last line like required. The two values are not zero because of smoothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the classifier we can use it to  make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_features = features_encoder.transform(cars_test.loc[:,'buying':'safety'])\n",
    "predicted_test_class = cnb.predict(encoded_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_class =  np.ravel(class_encoder.transform(cars_test.loc[:,'bargain':]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(encoded_test_class, predicted_test_class, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually the classifier has a method for predicting and measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb.score(encoded_test_features, encoded_test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which unsuprisingly gives same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last check I will look at the class distribution in the predicted and real labels. As it may happen that with unbalanced  classes  one class can be e.g. totaly misclassified without affecting accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(predicted_test_class.astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(encoded_test_class.astype('int64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach to measuring the performance of the classifier was a little haphazard.  The common and more systematic way of doing this is to treat a $k$ class classifcation problem as $k$ binary classification problems: class $C_i$ against the rest. We combine the final score out of binary metrics for each binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predicted_test_class.astype('int64')\n",
    "true = encoded_test_class.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat(y_true, y_pred, c):\n",
    "    lbl_true = np.where(y_true==c,1,0)\n",
    "    lbl_pred = np.where(y_pred==c,1,0)\n",
    "    TP = np.sum(lbl_true * lbl_pred)\n",
    "    FP = np.sum( (1-lbl_true)*lbl_pred)\n",
    "    TN = np.sum((1-lbl_true) * (1-lbl_pred))\n",
    "    FN = np.sum(lbl_true * (1-lbl_pred))            \n",
    "    return TP, FP,FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print( stat(true,pred, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the statistics for each binary classifier we can combine the together. We will consider _micro_ and _macro_ averaging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In micro averaging we first calculate the summary values of TP, FP, TN and TN and use them to calculate the total score. \n",
    "We will start with _recall_ which is just another name for true positives rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Recall_\\mu = \\frac{\\sum_i TP_i}{\\sum_i(TP_i+FN_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "den = 0\n",
    "for i in range(3):\n",
    "    tp, fp, fn, tn =  stat(true,pred, i)\n",
    "    num += tp\n",
    "    den += tp+fn\n",
    "print(num, den, num/den    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Precision_\\mu = \\frac{\\sum_i TP_i}{\\sum_i(TP_i+FP_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "den = 0\n",
    "for i in range(3):\n",
    "    tp, fp, fn, tn =  stat(true,pred, i)\n",
    "    num += tp\n",
    "    den += tp+fp\n",
    "print(num, den, num/den    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $F_1$ is then  harmonic mean of the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_\\mu = 2\\cdot\\frac{Precision_\\mu\\cdot Recall_\\mu}{Precision_\\mu + Recall_\\mu}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__  Show that $Recall_\\mu$ = $Precision_\\mu = Accuracy$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no surprise that scikit-learn library has functions to calculate those metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    recall_score(encoded_test_class, predicted_test_class, average='micro'),\n",
    "    precision_score(encoded_test_class, predicted_test_class, average='micro'),\n",
    "    f1_score(encoded_test_class, predicted_test_class, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With macro averaging we  calculate  score for each binary classifier separately and average them. So for recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Recall_M = \\frac{1}{k}\\sum_{i=0}^{k-1}\\frac{TP_i}{TP_i+FN_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(3):\n",
    "    tp, fp, fn, tn =  stat(true,pred, i)\n",
    "    tot +=tp/(tp+fn)\n",
    "rec = tot/3    \n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(encoded_test_class, predicted_test_class, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Precision_M = \\frac{1}{k}\\sum_{i=0}^{k-1}\\frac{TP_i}{TP_i+FP_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(3):\n",
    "    tp, fp, fn, tn =  stat(true,pred, i)\n",
    "    tot +=tp/(tp+fp)\n",
    "prec =  tot/3   \n",
    "print(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(encoded_test_class, predicted_test_class, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $F_M$ score is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_M = \\frac{1}{k}\\sum_{i=0}^{k-1}\\frac{2\\cdot TP_i}{TP_i+FP_i +TP_i +FN_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(3):\n",
    "    tp, fp, fn, tn =  stat(true,pred, i)\n",
    "    tot +=2 * tp/(tp+fp+tp+fn)\n",
    "f =  tot/3   \n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(encoded_test_class, predicted_test_class, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted averaging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the weighted averaging is like macro averaging but we  weight the average by the support of each class _i.e._ the number of labels of each class. E.g. for precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "den = 0\n",
    "for i in range(3):\n",
    "    tp, fp, fn, tn =  stat(true,pred, i)\n",
    "    tot +=tp/(tp+fp) *(tp+fn)\n",
    "    den += (tp+fn)\n",
    "prec = tot/den\n",
    "print(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(encoded_test_class, predicted_test_class, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__ Show that weighted averaging for recall gives same result as micro averaging."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}