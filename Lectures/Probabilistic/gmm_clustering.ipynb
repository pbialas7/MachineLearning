{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gaussian mixture models - EM notebook we have used the mixture models for _density estimation_ e.g. modeling the  distribution of features in each class. Another common application is _clustering_. Technically this looks the same as fitting the GMM to a single class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have $K$ components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\b}[1]{\\mathbf{#1}}$\n",
    "$$p(\\b x|\\b \\theta) = \\sum_{k=1}^K\\pi_k p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing the two component example from GMM-EM lecture we can introduce the latent variables $\\b z\\in \\{0,1\\}^K$ i.e. each $\\b z$ is a vector of length $K$ with all entries set to zero except entry $k$ set to one which indicates that this point belongs to $k$-th  component. This is called $1-K$ encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i = \\{0,\\ldots,1,\\ldots,0\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution in $\\b x$ and $\\b z$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\b x, \\b z|\\b \\theta) = \\sum_{k=1}^K\\pi_k z_k p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that only one term in the sum on the right hand side is non-zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes theorem the probability that point $\\b x$ belongs to cluster $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma_k(\\b x)\\equiv p(z_k=1|\\b x, \\theta)=\n",
    "\\frac{ \\pi_k p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)}\n",
    "{\\sum_{k=1}^K\\pi_k  p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma_k(\\b x)$ is called _responsibility_ of $\\b x$ for cluster $k$ and as a probability can take value from zero to one. This is called _soft clustering_. Of course we can use this number to define a _hard clustering_ by _e.g._ assigning points to the cluster with  highest responsibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sex from heigh and weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to  the sex from height&weight example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.datasets import load_height_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_data = load_height_weight('../../Data/HeightWeight/weight-height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_train, hw_test = train_test_split(hw_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_gm = GaussianMixture(n_components=2, tol=1e-5, n_init=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_gm.fit(hw_train[['Height','Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the found clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse\n",
    "plt.scatter(hw_train['Height'], hw_train['Weight'], alpha=0.2, color='grey');\n",
    "colors=['red', 'blue']\n",
    "for i in range(2):\n",
    "    confidence_ellipse(hw_gm.means_[i], hw_gm.covariances_[i], ax = plt.gca(), edgecolor=colors[i], label =f\"{hw_gm.weights_[i]:5.3f}\");\n",
    "    plt.scatter(*np.split(hw_gm.means_,2,1), facecolors=colors)\n",
    "plt.legend(title='$\\pi_k$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks very reasonable :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please recall that the because of the unidentifiability  of the parameters the particular assignment of labels to clusters is meaningless. That's a knowledge that we have to put in \"by hand\". Below I choose the women cluster as the one with smaller average height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female = np.argmin(hw_gm.means_[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results to the separate fit to each cluster _i.e._ quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "qda.fit(hw_train[['Height','Weight']], hw_train.Gender=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse\n",
    "plt.scatter(hw_train['Height'], hw_train['Weight'], alpha=0.2, color='grey');\n",
    "qda_colors = [colors[1-female],colors[female]]\n",
    "for i in range(2):\n",
    "    confidence_ellipse(hw_gm.means_[i], hw_gm.covariances_[i], ax = plt.gca(), edgecolor=colors[i], label =f\"{hw_gm.weights_[i]:5.3f}\");\n",
    "    plt.scatter(*np.split(hw_gm.means_,2,1), facecolors=colors)\n",
    "    confidence_ellipse(qda.means_[i], qda.covariance_[i], ax = plt.gca(), edgecolor=qda_colors[i], label =f\"{qda.priors_[i]:5.3f} QDA\", linestyle='--');\n",
    "    plt.scatter(*np.split(qda.means_,2,1), edgecolors=qda_colors, facecolors='none')\n",
    "plt.legend(title='$\\pi_k$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results are very similar. We can use those (EM fitted) cluster for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_proba = hw_gm.predict_proba(hw_test[['Height','Weight']])\n",
    "confusion_matrix(hw_test.Gender=='Female', test_proba[:,female]>0.5, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import roc_plot, add_roc_curve\n",
    "fig, ax = roc_plot()\n",
    "add_roc_curve(hw_test.Gender=='Female', test_proba[:,female], name='unsupervised', ax =ax);\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are getting results similar to supervised learning. However in this analysis we have overlooked some very important issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing  the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While technically the procedure of fitting is similar to supervised learning with gaussian mixture discriminative analysis (GMDA) the crucial difference lies in the interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GMDA the clusters are only a mean to better approximation of the class probability densities and their interpretation is irrelevant for the functioning of the classifier. We can choose the number of clusters that gives best classification results, because we have clear metrics to measure the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In clustering we usually want to discover the structure of data and assign some interpretation to discovered clusters. But the  number of clusters is an input parameter. In the height-weight examples we knew that  this was two so the results were good. And we have used the real labels to check that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not a case in general. We could experiment with different number of clusters but we need the criteria for evaluating the quality of clustering in absence of real labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As what we are doing is to find the distribution that  fits the data bests, we could use the log likelihood (LL) as the measure of the performance. And that's what EM lagorithm does: it tries to maximize the likelihood. It is customary to use instead the _negative log likelihood_  (NNL) which contrary to the name is positive :) The smaller NNL the better. For calculating the log likehood we can use the `score` method that returns the average LL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-hw_gm.score(hw_train[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we have already used the LL to fit on the same data. I hope that by now you know that you cannot do that and that this estimate is biased. We can try it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-hw_gm.score(hw_test[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and indeed we get a bigger, hence worse value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to fit with three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_gm3 = GaussianMixture(n_components=3, tol=1e-5, n_init=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_gm3.fit(hw_train[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the fitted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse\n",
    "plt.scatter(hw_train['Height'], hw_train['Weight'], alpha=0.2);\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "for i in range(3):\n",
    "    confidence_ellipse(hw_gm3.means_[i], hw_gm3.covariances_[i], ax = plt.gca(), edgecolor=cycle[i], label =f\"{hw_gm3.weights_[i]:5.3f}\");\n",
    "plt.legend(title='$\\pi_k$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that there is not a clear interpretation to be given to different clusters and that their weights are similar. Let's see how good is the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"2 clusters {-hw_gm.score(hw_test[['Height', 'Weight']]):6.4f}  3 clusters {-hw_gm3.score(hw_test[['Height', 'Weight']]):6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is only marginally different then in case of two clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we  should also not use the test set for choosing the number of components.  We should use a separate validation set. If we  do not have enough data we can recourse to cross valdation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv2 = cross_validate(hw_gm, hw_train[['Height', 'Weight']])\n",
    "print(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv3 = cross_validate(hw_gm3, hw_train[['Height', 'Weight']])\n",
    "print(cv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"2 clusters {-cv2['test_score'].mean():6.4f}  3 clusters {-cv3['test_score'].mean():6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are almost indistinguishable. But note that their are also different from the values obtained on the test set. That's because all of those are  statistical estimators so actually they are random variables themself :( The  estimate of variance of those estimators  is  difficult  and beyond the subject of this lecture. Interested readers may consult [No Unbiased Estimator of the Variance of K-Fold Cross-Validation](http://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation, while popular is a time consuming technique as it requires fitting the model several times. There is a method to remove the bias from the likelihood estimator in form of the so called [Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) (AIC). It takes into acount the number of parameters that were used to fit the model and tries to compensate for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to refit the clusters\n",
    "hw_gm3.fit(hw_train[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIC is normalized differently from score method so we have to rescale it\n",
    "0.5*hw_gm3.aic(hw_train[['Height', 'Weight']])/len(hw_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another similar measure is provided by [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (BIC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5*hw_gm3.bic(hw_train[['Height', 'Weight']])/len(hw_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do some more systematic analysis by checking fits in the range from one to five clusters. We start by generating the `GaussianMixture` objects with different number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmms = [GaussianMixture(n_components=i, tol=1e-4, max_iter=500, n_init=3) for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run cross validation on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs =  [cross_validate(gm, hw_train[['Height', 'Weight']], y=None, cv=5) for gm in gmms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we retrain them on the full  training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gm in gmms:\n",
    "    gm.fit(hw_train[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture belowe summarizes  different  performance measures. While  the pictures will differ in each evaluation of the notebook they all show a sharp decline at $N_c=2$ and then level-off,  indicating that two clusters is probably a good choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(hw_train)\n",
    "plt.scatter(np.arange(1,6), [-c['test_score'].mean() for c in cvs], label='NLL CV');\n",
    "plt.scatter(np.arange(1,6), [-gm.score(hw_train[['Height', 'Weight']]) for gm in gmms], label='NLL train');\n",
    "plt.scatter(np.arange(1,6), [-gm.score(hw_test[['Height', 'Weight']]) for gm in gmms], label='NLL test');\n",
    "plt.scatter(np.arange(1,6), [0.5*gm.aic(hw_train[['Height', 'Weight']])/n_samples for gm in gmms], label='AIC train', facecolor='none', edgecolors='green');\n",
    "plt.scatter(np.arange(1,6), [0.5*gm.bic(hw_train[['Height', 'Weight']])/n_samples for gm in gmms], label='BIC train', facecolor='none', edgecolors='red');\n",
    "plt.xticks(np.arange(1,6));\n",
    "plt.xlabel(\"K\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly the situation is not always that clear. Let's look at combination of five clusters. I will use 1D example for easier visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My helper class\n",
    "import mchlearn.mixture as mix\n",
    "#The arguments are means, stan dard deviations and relative weights of the clusters\n",
    "fiver = mix.Gaussian1D([-2,-1,0,1,2],[0.3,1,0.3,0.4,.75],[1,2,3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5=fiver.rvs(5000)\n",
    "x5 = data5[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows all contributing clusters, resulting pdf and the data histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(-5,5,500)\n",
    "plt.plot(ps,fiver.pdf(ps,sep=True));\n",
    "plt.plot(ps,fiver.pdf(ps), color=(0.1, 0.1, 0.1), linewidth=2, label=\"all\");\n",
    "plt.hist(data5[:,0], bins=32, density=True, histtype='step', color=(.4,.4,.4), linewidth=1);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the similar analysis as  previously but extend the range to 11 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data5  = fiver.rvs(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_gmms = [GaussianMixture(n_components=nc, tol=1e-4, max_iter=800, n_init=3) for nc in range(1, 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cvs = [cross_validate(gm, x5.reshape(-1,1),y=None) for gm in f_gmms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gm in f_gmms:\n",
    "    gm.fit(x5.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(x5)\n",
    "plt.scatter(np.arange(1,12), [-c['test_score'].mean() for c in f_cvs], label='LL CV');\n",
    "plt.scatter(np.arange(1,12), [-gm.score(x5.reshape(-1,1)) for gm in f_gmms], label='LL train');\n",
    "plt.scatter(np.arange(1,12), [0.5*gm.aic(x5.reshape(-1,1))/n_samples for gm in f_gmms], label='AIC train', facecolor='none', edgecolors='green');\n",
    "plt.scatter(np.arange(1,12), [0.5*gm.bic(x5.reshape(-1,1))/n_samples for gm in f_gmms], label='BIC train', facecolor='none', edgecolors='red');\n",
    "plt.xticks(np.arange(1,12));\n",
    "plt.xlabel(\"K\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the results are less conclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have plotted the means of the clusters discovered  for different number of clusters. The size of the dots is proportional to the cluster  weight. The label zero denotes the original, true mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.zeros(5),fiver.dists_.mean(), s=5*25*fiver.pis_);\n",
    "for m in fiver.dists_.mean():\n",
    "    plt.axhline(m, linewidth=1, linestyle='--')\n",
    "for gm in f_gmms:\n",
    "    plt.scatter(gm.n_components*np.ones(gm.n_components),gm.means_, s=gm.n_components*25*gm.weights_);\n",
    "plt.xticks(np.arange(0,12));\n",
    "plt.xlabel('n components');\n",
    "plt.ylabel('cluster means');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see even for correct number of components the means are off (at least on my plot). Below I compare the resulting distributions. It seems that starting from five clusters there is very little difference. That illustrates my point that it is relatively easy to fit a good distribution but deciding on the interpretation of clusters is much harder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x5, bins=32, density=True, histtype='step');\n",
    "ps = np.linspace(-5,5,500)\n",
    "for nc in range(3,8):\n",
    "    mixture = mix.Gaussian1D.FromGaussianMixture(f_gmms[nc-1])\n",
    "    #plt.plot(ps,mixture.pdf(ps,sep=True));\n",
    "    plt.plot(ps,mixture.pdf(ps), label='{:3d}'.format(nc));\n",
    "plt.legend(title='n components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using  GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using the  scikit-learn provided estimators for searching the optimal number of components we can use the \"machinery\" provided by this library for hyperparameters tuning e.g. the `GridSearchCV` class. I leave this as an excercise for the reader :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to systematic search in the number of clusters is to perform a stochastic (i.e. random) search in the space of the possible models. In scikit-learn this is provided in a form of BayesianGaussianMixture class which implements sampling from the _Dirichlet process_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class has two important parameters: the maximal number of components and the weight concentration prior. The later controls the weight distribution (priors) of the cluster. Bigger values of this parameter favor the more uniform distribution of weights and hence more clusters being 'active'. The lower values will have tendency to set some weights to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "n_comp = 11\n",
    "wcps = [0.1, 0.25, 0.5,1,5,10,20,50]\n",
    "bgms  = [BayesianGaussianMixture(n_components=n_comp,  \n",
    "                               tol=1e-3, max_iter=2000, weight_concentration_prior=wcp) for wcp in wcps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bgm in bgms:\n",
    "    bgm.fit(x5.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how at the weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(n_comp),bgms[0].weights_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see only six out of eleven clusters have weight significantly greater then zero. \n",
    "Actually in this example this is true for most of the values of the weight concentration prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bgm in bgms:\n",
    "    print(f\"{bgm.weight_concentration_prior_:6.2f}  {np.sum(bgm.weights_>0.001):2d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.zeros(5),fiver.dists_.mean(), s=5*25*fiver.pis_);\n",
    "for m in fiver.dists_.mean():\n",
    "    plt.axhline(m, linewidth=1, linestyle='--')\n",
    "for i,bgm in enumerate(bgms):\n",
    "    plt.scatter((i+1)*np.ones(bgm.n_components),bgm.means_.ravel(), s=6*25*bgm.weights_);\n",
    "plt.xticks(np.arange(9),['org']+wcps)    ;\n",
    "plt.xlabel('weight concentration prior');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice all the fits give very similar distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x5, bins=32, density=True, histtype='step');\n",
    "ps = np.linspace(-5,5,500)\n",
    "for bgm in bgms:\n",
    "    mixture = mix.Gaussian1D.FromGaussianMixture(bgm)   \n",
    "    plt.plot(ps,mixture.pdf(ps), linewidth=1, label=f\"{bgm.weight_concentration_prior_:6.2f}\");\n",
    "plt.plot(ps,fiver.pdf(ps), label='true', color='black', linestyle='--', linewidth=1);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "altought the actual clusters slightly differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(24,12),)\n",
    "ps = np.linspace(-5,5,500)\n",
    "ax = ax.ravel()\n",
    "for i,bgm in enumerate(bgms[:6]):\n",
    "    mixture = mix.Gaussian1D.FromGaussianMixture(bgm)   \n",
    "    ax[i].plot(ps,mixture.pdf(ps, sep=True));\n",
    "    ax[i].plot(ps,mixture.pdf(ps));\n",
    "    ax[i].plot(ps,fiver.pdf(ps,sep=True), label='true', color='black', linestyle='--', linewidth=1);\n",
    "    plt.hist(x5, bins=32, density=True, histtype='step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
