{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Grapical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will require installation of the new Python modules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is the [Python graphiz module](https://graphviz.readthedocs.io/en/stable/). The module can be installed in standard way using `pip` or `conda`,   but this is a Python interface to [graphviz](https://www.graphviz.org) plotting program __that has  to be installed separately__. Please refer to the [documentation](https://www.graphviz.org/download/) for its installation instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is the [pgmpy](https://pgmpy.org) Python library for working with graphical models (see  [this paper](https://conference.scipy.org/proceedings/scipy2015/pdfs/ankur_ankan.pdf)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will take a more general view of the generative approach we have been following so far.\n",
    "Please recall that the starting point for generative models is a _joint probability distribution_ for all the random variables considered in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\b}[1]{\\mathbf{#1}}$$\n",
    "$$P(\\b X_1=\\b x_1, \\ldots, \\b X_N=\\b x_N)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notation is not strictly speaking correct for the continous variables, but the idea is the same. For the moment, without any loss of generality you can assume that   all variables are  discrete.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can distinguish  three separate issues concerning this distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First issue is how do we represent this distribution. \n",
    "For discrete (and finite) random variables we can in principle just make a big table containig  all possible values of random variables with corresponding probabilities. In practice this only possible for rather small systems. If variable $\\b X_i$ has $n_i$ distinct possible values then we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\prod_{i=1}^N n_i - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters. For _e.g._  ten variables with five values each this already gives  almost ten million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5**10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specyfying that number of the parameters by human experts would be clearly impossible. They could be learned, but that would require a large amount of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But often our model has an internal structure that greatly reduces the number of required parameters. Let's take the Naive Bayes as example. Let's assume that all the variables are conditionally independent conditioned on the variable $\\b X_1$. How many parameters we now need? Please try to answer this question by yourself without looking at the answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Under naive Bayes assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$\n",
    "\\begin{align}P(\\b X_1=\\b x_1, \\ldots, \\b X_N=\\b x_N)&=P(\\b X_2=\\b x_2, \\ldots, \\b X_N=\\b x_N|\\b X_1 = \\b x_1)P(\\b X_1=\\b x_1)\\\\\n",
    "&=P(\\b X_1=\\b x_1)\\prod_{i=2}^N P(\\b X_i=\\b x_i|\\b X_1=\\b x_1)\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$P(\\b X_1=\\b x_1)$ requires $n_1-1$ parameters and each $P(\\b X_i=\\b x_i|\\b X_1=\\b x_1)$ requires $n_1\\times(n_i-1)$ parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Together this gives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$ n_1 \\times \\left(\\sum_{i=2}^N (n_i-1)+1\\right) -1 = n_1 \\times \\left(\\sum_{i=2}^N n_i -N+2\\right) -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same example of ten variables with five values each we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5*(4*5-10+2)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge  decrease by __five orders__ of magnitude! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the naive Bayes is an extreme example, in general every independence relation decreases the number  of parameters needed. As we will see modeling the (in)dependecies between the variables can quite intuitively be done using graphical models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we decide on the representation we have to estimate the parameters of the distributions.  In this notebook we will be less concentrated on learning as on the other two aspects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we have our learned representation of the joint probability distribution function the last problem is the _inference_, that is extracting an useful information out of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical task is a _probability query_: given _evidence_ that is a subset of $m$ variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\b E=\\{\\b X_{i_1},\\ldots,\\b X_{i_m}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "together with an instantiations of each variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\b e = \\{\\b x_{i_1},\\ldots,\\b x_{i_m}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $k$  _query variables_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\b Y=\\{\\b X_{j_1},\\ldots,\\b X_{j_k}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are asking what is the distribution of $\\b Y$ given $\\b E$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\b Y| \\b E = \\b e)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote by $\\b Z$ the set of variables not in $\\b Y$ or $\\b E$ then the above distribution can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{\\b z} P(\\b Y = \\b y,\\b Z = \\b z|\\b E = \\b e)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type is the _MAP query_ (Maximal \"a posteriori\"). Given the joint probability distribution $P(\\b X_1, \\ldots,\\b X_N)$ and evidence $\\b E = \\b e$ we are looking for the values of the remaining (non-evidence) variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\b W = \\b X \\setminus \\b E = \\{\\b X_i:i\\neq i_1,\\ldots,i_m\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that maximize the probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MAP(\\b W| \\b e) = \\underset{\\b w}{\\operatorname{arg max}}P(\\b W = \\b w|\\b E = \\b e)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal MAP query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal MAP query is the generalisation of the MAP query to the case when there are some variables $\\b Z$ that we want to sum over $\\b X =  \\b Y \\cup \\b Z \\cup \\b E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MAP(\\b Y| \\b e) = \\underset{\\b y}{\\operatorname{arg max}}\\sum_{\\b z} P(\\b Y = \\b y,\\b Z = \\b z|\\b E = \\b e)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that running a MAP query is not the same as running several marginal MAP queries as exemplified by the problem below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "#### Problem\n",
    "\n",
    "Let $X$ and $Y$ be two discrete random variables taking values $0$ and $1$. Give an example of joint probability distribution $P(X=x, Y=y)$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "$$(x_m,y_m) = \\underset{x,y}{\\operatorname{arg max}} P(X=x, Y=y),\\quad x_M = \\underset{x}{\\operatorname{arg max}} \\sum_y P(X=x, Y=y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "$$x_m\\neq x_M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$\\begin{array}{c|cc|c}\n",
    " & Y=0& Y=1 &\\\\\\hline\n",
    "X=0 & 0.10 & 0.35& 0.45\\\\\n",
    "X=1 & 0.25 & 0.30& 0.55\n",
    "\\end{array}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender from height and weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So consider our standard  sex from height and weight example. The joint probability distribution is  a function of three variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(h,w,s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Witout _any_ loss we can represent it as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(h,w,s)= P(h,w|s)P(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex  is a discrete (binary) random variable so  it is characterised by one number: $P(S=female)$. To proceed further we had to make some assumptions on the form of the conditional probability $P(h,w|s)$. We have so far assumed that this is a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of the naive bayes classifier we have additionally assumed that height and BMI are conditionally independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(h,bmi|s)= P(h|s)P(bmi|s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have choosen the representation the learning consisted of estimating the parameters of the distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the inference amounted to calculating probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(s=female|h,w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case sex was the query variable and height and weight the evidence. In this example we could easilly calculate it directly using the Bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical models: Bayes Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this very simple example we could manipulate all the formulas directly. For more complicated examples it is useful to use the _graphical notation_. In this notation we represent the joint probability distribution as _directed_ graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes represent the random variables and edges the conditional probability distribution. The arrows point from _parent_ nodes to the _child_ nodes. The variable in the child node is conditionally dependent on all variables in the parent nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the sex from height and weight we would have the following rather simple graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shw = graphviz.Digraph(node_attr = {'shape': 'circle'})\n",
    "shw.attr('node', fixedsize='true')\n",
    "shw.node('S')\n",
    "shw.node('HW','H,W')\n",
    "shw.edge('S','HW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to show the explicit dependence of the $P(h,w|s)$ on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering the inference it is customary to show the evidence by coloring the apropriate  nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shw = graphviz.Digraph(node_attr = {'shape': 'circle'})\n",
    "shw.node('S',)\n",
    "shw.attr('node', style='filled', fillcolor='#dfdfdf',fixedsize='true')\n",
    "shw.node('HW','H,W')\n",
    "shw.edge('S','HW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of the naive Bayes we have additional independence assumption and the graph looks as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shw = graphviz.Digraph(node_attr = {'shape': 'circle'})\n",
    "shw.node('S',)\n",
    "shw.attr('node', style='filled', fillcolor='#dfdfdf', fixedsize='true')\n",
    "shw.node('H','H')\n",
    "shw.node('W','BMI')\n",
    "shw.edge('S','H')\n",
    "shw.edge('S','W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monty Hall problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example consider the \"famous\" [Monty Hall](https://en.wikipedia.org/wiki/Monty_Hall_problem) problem: The host shows you three identical doors, behind one of the doors there is a prize (car). Host  ask you to pick up one of them, but does not open it. After that he pics one of the  two remaining doors and opens it to show that there is no prize behind. Then he let's you switch your choice with the one remaining door. Should you do it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The puzzle is simple enough to be solved by considering all the possible cases. Please try to do it without looking at the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    " 1. With probability 1/3 you have  picked the door with the prize. Both remaining doors lead to empty rooms so the probability that the prize is in the other door is zero.\n",
    " 2. With probability 2/3 you have  picked the door without the prize. The host opens the second door without the prize so the probability that the prize is in the other door is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Combining this together gives us "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$\\frac{1}{3}\\times 0 + \\frac{2}{3}\\times 1 =\\frac{2}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "for the probability that the prize is behind the other door. So we should switch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also solve it in a more systematic way using the Bayes theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prize is placed at random behind a door $p$ with probability $P(P=p)=1/3$. Similarly we pick a door $c$ at random with probability $P(C=c)=\\frac{1}{3}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The host picks  the door $h$ with the conditional probability $P(H=h|P=p, C=c)$ depending on the $p$ and $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align: center;\">\n",
    "    <tr><td>p</td> <td/> <td colspan=3 align=\"center\">0</td> <td colspan=3 align=\"center\">1</td> <td colspan=3 align=\"center\">2</td></tr>\n",
    "    <tr style=\"border-bottom: black thin solid\"><td>c</td> <td/> <td style=\"text-align:center;\"> 0 </td> <td> 1 </td> <td> 2 </td> <td> 0 </td> <td> 1 </td> <td> 2 </td> <td> 0 </td> <td> 1 </td> <td> 2 </td> </tr> \n",
    "    <tr><td rowspan=3 >h</td> <td style=\"border-right: black thin solid\">0</td> <td> 0   </td> <td> 0 </td> <td> 0 </td> <td> 0 </td> <td> 1/2 </td> <td> 1 </td> <td> 0 </td> <td > 1 </td> <td> 1/2 </td> </tr> \n",
    "    <tr>              <td style=\"border-right: black thin solid\">1 </td> <td> 1/2 </td> <td> 0 </td> <td> 1 </td> <td> 0 </td> <td> 0 </td>   <td> 0 </td> <td> 1 </td> <td> 0 </td> <td> 1/2 </td> </tr> \n",
    "    <tr>               <td style=\"border-right: black thin solid\">2</td> <td> 1/2 </td> <td> 1 </td> <td> 0 </td> <td> 1 </td> <td> 1/2 </td> <td> 0 </td> <td> 0 </td> <td> 0 </td> <td> 0 </td> </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is implemented in the tables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_p = 1/3* np.ones(3)\n",
    "prob_c = 1/3* np.ones(3)\n",
    "prob_h_c_p = np.asarray([\n",
    "    [ [0,0,0],     [0,1/2,1], [0,1/2,1] ],\n",
    "    [ [1/2, 0, 1], [0,0,0],   [1,0,1/2] ],\n",
    "    [ [1/2, 1, 0], [1,1/2,0], [0,0, 0] ] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are interested in is the conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(P=p|C=c, H=h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can  can be calculated from the Bayes theorem, again try to do it yourself.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$P(P=p|C=c, H=h)= \n",
    "\\frac{P(H=h|P=p,C=c)P(P=p)P(C=c)}\n",
    "{\\sum\\limits_{p=0}^2 P(H=h|P=p,C=c)P(P=p)P(C=c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "So e.g. if $c=1$  and $h=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "c=1\n",
    "h=2\n",
    "num = prob_h_c_p[h,:,c]*prob_c[c]*prob_p\n",
    "num/num.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "As you can see the probability that the prize is behing door $0$ is 2/3 which is twice as large as the probability that it is behind originally picked door. You can check that this does not depend on which door we have picked at the begining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be represented by the graphical model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty_hall = graphviz.Digraph(node_attr = {'shape': 'circle'})\n",
    "monty_hall.node('P') \n",
    "monty_hall.attr('node', style='filled', fillcolor='lightgrey',color='black')\n",
    "monty_hall.edge('P','H')\n",
    "monty_hall.edge('C','H')\n",
    "monty_hall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Probabilistic Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples were simple enough to enable us to get the answers to our queries by using the Bayes theorem directly.  This \"brute force\" approach requires scanning and/or summing over all the values of variables not fixed by the evidence.  This quickly can become prohibitively expensive in more realistic examples. Again we can exploit the structure of the graphical models for efficient inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms for doing this are beyond the scope of this lecture. I will only illustrate them with help of the already mentioned [pgmpy](https://pgmpy.org)  library. For example the Monty Hall problems can be formulated and solved in the following fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network structure\n",
    "model = BayesianModel([('C', 'H'), ('P', 'H')])\n",
    "\n",
    "# Defining the CPDs:\n",
    "cpd_c = TabularCPD('C', 3, [[0.33, 0.33, 0.33]])\n",
    "cpd_p = TabularCPD('P', 3, [[0.33, 0.33, 0.33]])\n",
    "cpd_h = TabularCPD('H', 3, [[0, 0, 0, 0, 0.5, 1, 0, 1, 0.5], \n",
    "                            [0.5, 0, 1, 0, 0, 0, 1, 0, 0.5], \n",
    "                            [0.5, 1, 0, 1, 0.5, 0, 0, 0, 0]],\n",
    "                  evidence=['C', 'P'], evidence_card=[3, 3])\n",
    "\n",
    "# Associating the CPDs with the network structure.\n",
    "model.add_cpds(cpd_c, cpd_p, cpd_h)\n",
    "model.check_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have model we can run queries on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VariableElimination` is one of the many inference algorithm implemented in the `pgmpy`  module. The method `query` performs the probability query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = VariableElimination(model)\n",
    "posterior_p = infer.query(['P'], evidence={'C': 1, 'H': 2})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get same answer as before: we should switch the door. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are only interested in the door with the highest probability of containing the prize we can execute a MAP query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.map_query(['P'], evidence={'C': 1, 'H': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will consider a more elaborate example taken from the textbook \"Probabilistic Graphical Models, Principles and Techniques\" by D. Koller and N. Friedman. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Consider the problem\n",
    "faced by a company trying to hire a recent college graduate. The company’s goal is to hire\n",
    "intelligent employees, but there is no way to test intelligence directly. However, the company\n",
    "has access to the student’s SAT scores, which are informative but not fully indicative.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Elaborating our example, we now assume that the company also has access to the student’s\n",
    "grade G in some course.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The student’s grade, in this case, depends not only on his intelligence but also on the difficulty of\n",
    "the course, represented by a random variable D whose domain is Val(D) = {easy; hard}. Our\n",
    "student asks his professor for a recommendation letter. The professor is absentminded and never\n",
    "remembers the names of her students. She can only look at his grade, and she writes her letter\n",
    "for him based on that information alone. The quality of her letter is a random variable L, whose\n",
    "domain is Val(L) = {strong; weak}. The actual quality of the letter depends stochastically on\n",
    "the grade. (It can vary depending on how stressed the professor is and the quality of the coffee\n",
    "she had that morning.)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__\n",
    "\n",
    "Please draw the graph representing this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "student = graphviz.Digraph(node_attr={'shape' : 'oval'})\n",
    "student.node('D', 'Difficulty')\n",
    "student.node('I', 'Intelligence')\n",
    "student.node('S','Sat')\n",
    "student.node('G', 'Grade')\n",
    "student.edges(['DG','IG'])\n",
    "student.node('L','Letter')\n",
    "student.edges(['IS','GL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph implies the following factorisation of the joint probability distribution function (please try to write it down yourself): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$P(D,I,G,S,L)=P(L|G)\\cdot P(G|D,I)\\cdot P(S|I)\\cdot P(I)\\cdot P(D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters are needed in this model? How many would be needed in full joint probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Taking into acount that all  distribution are normalized we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "1+1 + 4*2 + 3 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Full distribution would require "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "2*2*3*2*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "parameters. More then thrice that many. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set asside the question \"how\" and just assume that we know all the probability distributions in the graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><th colspan=2>Inteligence</th> <th colspan=2> Difficulty</th></tr>\n",
    "    <tr><td> medium</td> <td>0.7</td> <td> low </td>  <td>0.6</td></tr>\n",
    "    <tr><td> high  </td> <td>0.3</td> <td> high </td> <td>0.4</td></tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:80%; \">\n",
    "<table style=\"display: inline-block; margin-right: 20px; margin-left:100px; vertical-align:text-top;\">\n",
    "    <tr><th>Grade</th> <th colspan=4>Inteligence,Difficulty</th></tr>\n",
    "    <tr><td/> <td>medium,low</td> <td>medium,high</td><td>high,low</td><td>high,high</td></tr>\n",
    "    <tr><td>A</td> <td>0.3</td> <td>0.05</td> <td>0.9</td> <td>0.5</td> </tr>\n",
    "    <tr><td>B</td> <td>0.4</td> <td>0.25</td> <td>0.08</td> <td>0.3</td></tr>\n",
    "    <tr><td>C</td> <td>0.3</td> <td>0.7</td> <td>0.02</td> <td>0.2</td></tr>\n",
    "</table>    \n",
    "\n",
    "<table style=\"display: inline-block; margin-right: 20px; vertical-align:text-top;\">\n",
    "    <tr><th>Letter</th><th colspan=3> Grade </th></tr>\n",
    "    <tr><td/> <td>A</td> <td>B</td> <td>C</td></tr>\n",
    "    <tr><td>weak</td><td>0.1</td><td>0.4</td><td>0.99</td></tr>\n",
    "     <tr><td>strong</td><td>0.9</td><td>0.6</td><td>0.01</td></tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"display: inline-block; margin-right: auto; vertical-align:text-top;\">\n",
    "    <tr><th>SAT </th><th colspan=2> Intelligence </th></tr>\n",
    "    <tr><td/> <td>medium</td> <td>high</td> </tr>\n",
    "    <tr><td>low</td><td>0.95</td><td>0.2</td></tr>\n",
    "     <tr><td>high</td><td>0.05</td><td>0.8</td></tr>\n",
    "</table>    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can translate this into the `pgmpy` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network structure\n",
    "student = BayesianModel([('D', 'G'), ('I', 'G'),('I','S'),('G','L')])\n",
    "\n",
    "# Defining the CPDs:\n",
    "cpd_I = TabularCPD('I', 2, [[0.7,0.3]])\n",
    "cpd_D = TabularCPD('D', 2, [[0.6, 0.4]])\n",
    "\n",
    "cpd_G = TabularCPD('G', 3, [[0.3, 0.05, 0.9, 0.5], \n",
    "                            [0.4, 0.25, 0.08, 0.3], \n",
    "                            [0.3, 0.7, 0.02, 0.2]],\n",
    "                  evidence=['I', 'D'], evidence_card=[2, 2])\n",
    "cpd_L = TabularCPD('L',2,[[0.1, 0.4, 0.99],\n",
    "                         [0.9, 0.6, 0.01]], evidence=['G'], evidence_card=[3])\n",
    "\n",
    "cpd_S = TabularCPD('S',2,[[0.95, 0.2],\n",
    "                          [0.05, 0.8]], evidence=['I'], evidence_card=[2])\n",
    "\n",
    "# Associating the CPDs with the network structure.\n",
    "student.add_cpds(cpd_I, cpd_D, cpd_G, cpd_L, cpd_S)\n",
    "\n",
    "student.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having specified the model we can use it for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_infer = VariableElimination(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "What is posterior probability for students high intelligence knowing that he scored high on SAT test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(I=i|SAT=s)=\\frac{P(SAT=s|I=i)P(I=i)}{\\sum_{i'} P(SAT=s|I=i')P(I=i')}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8*0.3/(0.8*0.3+0.05*0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get same answer querring our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['I'], evidence={'S': 1})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of high intelligence given that she received a strong  letter of recomendation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['I'], evidence={'L': 1})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['I'], evidence={'L': 1, 'S':1})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what if we know that the course was diffcult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['I'], evidence={'L': 1, 'S':1, 'D' :1})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what we can say about the difficulty of the course given that student had high SAT score and weak recomendation letter ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['D'], evidence={'L': 0, 'S':1})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['I','D'], evidence={'L': 1}, joint=True)\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list could go on, and on but I hope you have a feeling how those models can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have assumed that all the probability distributions in the graph are known. In reality those are the parameters of the model and estimating them is the crucial part of the modeling process. One possibility is to estimate them based on the experts input. This was the original idea behind the expert systems. Unfortunatelly people in general are not  very good in estimating the probabilities. And also even with the reduction provided by the structured graph models the number of parameters is just to big to be estimated by human experts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nowadays much more realistic to learn those parameters, as in all other models considered so far. As I don't have any real data for this fake example I will generate it using functionality built in the `pgmpy` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.sampling.Sampling import GibbsSampling, BayesianModelSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs = GibbsSampling(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gibbs.sample(size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will define a new clean (\"tabula rasa\") model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tr = BayesianModel([('D', 'G'), ('I', 'G'),('I','S'),('G','L')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tr.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And estimate the parameters using the `fit` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpds = student_tr.get_cpds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cpds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cpds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be compared with the original distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student.get_cpds(node='G'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But probably a better idea is to ask some queries and compare the answer with original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tr_infer = VariableElimination(student_tr)\n",
    "posterior_tr_p = student_tr_infer.query(['I'],  evidence={'L': 1,})\n",
    "print(posterior_tr_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same query on the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_p = student_infer.query(['I'],  evidence={'L': 1,})\n",
    "print(posterior_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results (at least in my realisation) look very good. Let's try to estimate the error on those number. To this end, we will  repeat same procedure 20 times, each time with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "results = []\n",
    "for i in range(20):\n",
    "    data = gibbs.sample(size=n_samples)\n",
    "    student_tr = BayesianModel([('D', 'G'), ('I', 'G'),('I','S'),('G','L')])\n",
    "    student_tr.fit(data)\n",
    "    student_tr_infer = VariableElimination(student_tr)\n",
    "    posterior_p = student_tr_infer.query(['I'], evidence={'L': 1},show_progress=False)\n",
    "    results.append(posterior_p.values)    \n",
    "dist = np.stack(results, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean value is close to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are big enough to switch the maximum estimate (MAP) from I(0) to I(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_results=[]\n",
    "for i in range(20):\n",
    "    data = gibbs.sample(size=n_samples)\n",
    "    student_tr = BayesianModel([('D', 'G'), ('I', 'G'),('I','S'),('G','L')])\n",
    "    student_tr.fit(data)\n",
    "    student_tr_infer = VariableElimination(student_tr)\n",
    "    map_q = student_tr_infer.map_query(['I'], evidence={'L': 1},show_progress=False)\n",
    "    map_results.append(map_q['I'])    \n",
    "max_I = np.asarray(map_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_I.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once in a while we get the wrong answer: the probability that student is highly inteligent is higher then medium inteligent in oposition to the original model.  While this examples implies caution while interpreting the model inference, we should be aware that in case of so small difference between the probabilities we should probably treat them as equal anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional independence and d-separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now briefly discuss what additional information can be extracted from the graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.__\n",
    "\n",
    "Consider the  difficulty of the course and the recomendation letter. Those two random variables are cleary correlated. If the course is more difficult, then the probability of lower grade is higher and hence the probability of strong letter is lower. Formaly we can calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(L=l,D=d)=\\sum_{g,i} P(L=l|G=g)P(G=g|I=i,D=d)P(I=i)P(D=d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general this will not be equal to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(L=l)\\cdot P(D=d)=P(D=d)\\sum_{g,i,d'} P(L=l|G=g)P(G=g|I=i,D=d')P(D=d')P(I=i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but you can check it by  explicit substitution of the probability values from the tables above or using the pgmpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "p_ld = student_infer.query(['L','D']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "print(p_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_l = student_infer.query(['L'])\n",
    "p_d = student_infer.query(['D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "print(p_d.product(p_l, inplace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let' suppose that we know the grade, then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(L=l,D=d|G=g)=\\frac{\\sum_{i} P(L=l|G=g)P(G=g|I=i,D=d)P(I=i)P(D=d)}{P(G=g)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(L=l|G=g)\\cdot P(D=d|G=g)\n",
    "=P(L=l|G=g)\\frac{\\sum_i P(G=g|D=d,I=i)P(I=i)P(D=d)}{\\sum_{i,d} P(G=g|D=d,I=i)P(I=i)P(D=d)}\n",
    "=P(L=l|G=g)\\frac{\\sum_i P(G=g|D=d,I=i)P(I=i)P(D=d)}{P(G=g)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both expressions are the same, so variables $L$ nad $D$ when conditioned on $G$ are conditionally independent. Again this is intuitively clear: the letter by assumption depeds only on the grade. Once grade is fixed there is no more dependence on any other variable. We can check this explicitely using pgmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "p_ld_g = student_infer.query(['L','D'], evidence={'G':0})\n",
    "p_d_g = student_infer.query(['D'], evidence={'G':0})\n",
    "p_l_g = student_infer.query(['L'], evidence={'G':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "print(p_ld_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "print(p_d_g.product(p_l_g, inplace=False) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.__ \n",
    "\n",
    "Now look at the grade $G$  and SAT score $S$. Both variables are  dependent: they both depend on inteligence. E.g. once we know that the SAT score is high the probability of high inteligence goes up and so does the probability of the high grade:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(G=g,S=s)=\\sum_i P(G=g|I=i)P(S=s|I=i)P(I=i)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in general will not factorise into:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(G=g)P(S=s)=\\sum_i P(G=g|I=i)P(i)\\sum_{i'}P(S=s|I=i')P(I=i')$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we fix the intelligence e.g. by measuring it separately then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(G=g,S=s|I=i)=\\frac{P(G=g|I=i)P(S=s|I=i)P(I=i)}{P(I=i)} = P(G=g|I=i)P(S=s|I=i)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and again we found that the variables are conditionally independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.__ \n",
    "\n",
    "Finally let's consider variables $I$ and $D$. Those variables are independent by definition. But supose that we know the grade. If we know that that grade is high and the intelligence is medium than we are more likely  assume that the course was not difficult. This is known as _explaning away_: The high grade is explained away by the low difficulty of the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by fixing the grade we have introduced the correlation betwee two independent variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(I=i,D=d|G=g)= P(G=g|I=i, D=d)P(I=i)P(D=d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in general will not be equal to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(I=i|G=g)P(D=d|G=g)= \\frac{\\sum_{d'} P(G=g|I=i, D=d)P(I=i)P(D=d)}{P(G=g)}\\frac{\\sum_{i'} P(G=g|I=i, D=d)P(I=i)P(D=d)}{P(G=g)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have analysed three possible situations that can be graphically represents as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "head_to_tail = graphviz.Digraph(graph_attr={'rankdir' : 'LR'}, node_attr={'shape':'circle'})\n",
    "head_to_tail.node('L')\n",
    "head_to_tail.node('D')\n",
    "head_to_tail.attr('node', shape='doublecircle')\n",
    "head_to_tail.node('G')\n",
    "\n",
    "head_to_tail.edges([('D','G'), ('G','L')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_to_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "tail_to_tail = graphviz.Digraph(graph_attr={'rankdir' : 'TB'}, node_attr={'shape':'circle'})\n",
    "tail_to_tail.node('G')\n",
    "tail_to_tail.node('S')\n",
    "tail_to_tail.attr('node', shape='doublecircle')\n",
    "tail_to_tail.node('I')\n",
    "tail_to_tail.edges([('I','G'), ('I','S')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_to_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "head_to_head = graphviz.Digraph(graph_attr={'rankdir' : 'TB'}, node_attr={'shape':'circle'})\n",
    "head_to_head.node('I')\n",
    "head_to_head.node('D')\n",
    "head_to_head.attr('node', shape='doublecircle')\n",
    "head_to_head.node('G')\n",
    "head_to_head.edges([('I','G'), ('D','G')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_to_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two first examples setting the value of the double circled node  breaks the connection between the other two variables and makes them conditionally independent. In the last example oposite happens: independent variables become dependent after fixing the value of double circle node.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those observation can be generalised by the notion of $d-separability\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We wish to ascertain whether a particular conditional\n",
    "independence statement $A\\perp B | C$ is implied by a given directed acyclic graph. To\n",
    "do so, we consider all possible paths from any node in A to any node in B. Any such\n",
    "path is said to be blocked if it includes a node such that either\n",
    "\n",
    " 1. the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the\n",
    "node is in the set C, or\n",
    " 2. the arrows meet head-to-head at the node, and neither the node, nor any of its\n",
    "descendants, is in the set C.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If all paths are blocked, then A is said to be _d-separated_ from B by C, and the joint\n",
    "distribution over all of the variables in the graph will satisfy $A\\perp B | C$.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from \"Pattern Recognition and Machine Learning\" Ch. M. Bishop. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
