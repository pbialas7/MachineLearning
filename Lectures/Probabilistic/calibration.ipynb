{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This is a handy extension when using and debuging external modules.\n",
    "# Normally the notebook kernel load the module only once. If we make any changes in the module and want to see them \n",
    "# we have to restart the kernel. Setting autoreload to 2 forces kernel to reaload. all modules before running the code.\n",
    "# (see https://ipython.org/ipython-doc/stable/config/extensions/autoreload.html)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds repo root dir to python system path used to search for packages \n",
    "# and modules. This enables  loading modules from mchlearn package provided\n",
    "# in this repo. \n",
    "import sys\n",
    "sys.path.append('../..') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have tried hard to estimate the probability of examplar belonging to class $c$ given the features $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(C = c|X=x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but  we have not really made use of the fact that this is a probability.  We have treated the output of the classifier as a _score_ that we  have used for thresholding. While the ultimate goal of the classfier is to  classify,  we can treat the probabilistic classifiers presented so far as doing _regression_  by learning the probability function and instead of measuring solely the   classification performance we  can also ask how well is the classifier predicting the  probabilities? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a question about the _calibration_ of the classifier. Let me illustrate this on the example of weather prediction. When discussing the probability calculus at  the begining of those lectures I have stated that it's problematic to give exact meaning to the statement that e.g. \"the probability of rain tomorrow is 25%\". Historically we can however check the  calibration of the predictor as follow: we look at all days where \"25% of rain\" was forecasted. If the forecast is well calibrated we should see rain in about 25% of the cases. We can perform this check for  other values of the rain forecast. If we plot the real observed fraction of rainy days against the predicted value we obtain the _calibration curve_ or _reliability diagram_. If the forecast is well calibrated the points should lie on the $y=x$ diagonal line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I do not have any historical weather forecast data, I will illustrate this on our height and weight example. I will start by loading the dataset and training the  quadratic discriminative analiser classifier that discussed in the  previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# I have added a helper function to mchlearn package that loads the HW data, performs units conversion and adds BMI\n",
    "from  mchlearn.datasets import load_height_weight\n",
    "data = load_height_weight('../../Data/HeightWeight/weight-height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we will split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "seed = 77678  #By fixing the seed we guarantee that we can split the set in same way each time this maybe handy for debuging purposes\n",
    "train_data, test_data  = train_test_split(data,test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda =  QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda.fit(train_data[['Height','Weight']], train_data.Gender=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_proba = qda.predict_proba(test_data[['Height', 'Weight']])[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(test_data.Gender=='Female', test_predicted_proba>0.5, normalize='true')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mchlearn.plotting import roc_plot, add_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot()\n",
    "add_roc_curve(test_data.Gender=='Female', test_predicted_proba, \"QDA\", ax=ax)\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that the  ROC curve does not depend on the fact that classifier returns a probability! All  what is needed is a _score_ value , such that a bigger score indicates bigger chance of positive examplar. The ROC curve will not change if we transform the value returned from classifier by any monoticaly  increasing function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot()\n",
    "add_roc_curve(test_data.Gender=='Female', test_predicted_proba, \"QDA $f(x)=x$\", ax=ax)\n",
    "add_roc_curve(test_data.Gender=='Female', test_predicted_proba**2, \"QDA $f(x)=x^2$\", ax=ax)\n",
    "add_roc_curve(test_data.Gender=='Female', test_predicted_proba/(1+test_predicted_proba), \"QDA $f(x)=x/(1+x)$\", ax=ax)\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the  all the curves are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we test that the classifier returns a probability? As the probability is a continous variable, we cannot check all the possible prediction values. We must first somehow discretize it by  dividing  possible values into a number of bins. Let's divide the probability space in 20 uniform bins and see how the output of the classifier is distributed by plotting a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, edges, _ = plt.hist(test_predicted_proba, bins=np.linspace(0,1,21), edgecolor='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see most in most of the cases the classifier is pretty sure, the scores are concentrated close to zero and one, however the examples are not uniformly distributed in the bins, especially the outer ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots all test data points inside  the histogram. The x value is the predicted probability, the y value is uniform random in the range = [0, bin_height).\n",
    "bins = np.digitize(test_predicted_proba, bins=edges).astype('int32')\n",
    "ys = counts[bins-1]*np.random.uniform(0,1,size = len(test_predicted_proba))\n",
    "women = test_data.Gender=='Female'\n",
    "men = test_data.Gender=='Male'\n",
    "plt.scatter(test_predicted_proba[women], ys[women], s=8, alpha = 0.5, c='blue', label='Women')\n",
    "plt.scatter(test_predicted_proba[men], ys[men], s=5, alpha = 0.5, c= 'orange', label ='Men')\n",
    "counts, edges, _ = plt.hist(test_predicted_proba, bins=np.linspace(0,1,21), histtype='step');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at our cases bin by bin. For each bin the observed probability is the number of positive examples (women) in the bin. As the  predicted value we will take the mean of all predicited probabilities contained in the bin. This in general will not be equal to the bin center as you can see from the  plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented this in the function below. I  have also added error estimates for both observed (true)  and predicted probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.core.series\n",
    "def reliability_diagram1(y_true, y_proba, bins):\n",
    "    # If bins is a list or numpy array uses it as bin edges. If not it assumes that it is an integer\n",
    "    # and  uses bins number of bins of the same width in the range [0,1)\n",
    "    edges = bins if isinstance(bins,(list, np.ndarray)) else np.linspace(0,1,bins+1)\n",
    "    n_bins = len(edges)-1\n",
    "    \n",
    "    # If y_true is a pandas Series it converts it to numpy array. \n",
    "    true_values = y_true.values if isinstance(y_true, pandas.core.series.Series) else y_true\n",
    "    \n",
    "    #Takes the the array of probabilities y_proba and returns the bin corresponding number\n",
    "    #Bin index 0 indicates values smaller the zero (should not happen!)\n",
    "    #Bin index n_bins+1 indicates values greater or equal to 1.0 (can happen)\n",
    "    prob_bins = np.digitize(y_proba, bins=edges).astype('int32')\n",
    "    \n",
    "    #arrays that will accumulate counts in the bins (two more bins for bin 0 and n_bins+1)\n",
    "    total = np.zeros(n_bins+2)\n",
    "    positive  = np.zeros(n_bins+2)\n",
    "    bin_mean  = np.zeros(n_bins+2)\n",
    "    bin_mean2 = np.zeros(n_bins+2)\n",
    "    # accumulation loop\n",
    "    for i,p in enumerate(y_proba):\n",
    "        bin_index = prob_bins[i]\n",
    "        total[bin_index]    += 1\n",
    "        positive[bin_index] += true_values[i]\n",
    "        bin_mean[bin_index] += p\n",
    "        bin_mean2[bin_index] += p*p #needed for mean variance calculations. \n",
    "    \n",
    "    #taking only non empty bins\n",
    "    non_empty = (total>0)\n",
    "    ne_total  = total[non_empty]\n",
    "    p_true = positive[non_empty]/ne_total \n",
    "    p_pred = bin_mean[non_empty]/ne_total \n",
    "    p_pred2 =  bin_mean2[non_empty]/ne_total \n",
    "    \n",
    "    #Error estimates (samples standard deviation)\n",
    "    e_true = np.sqrt(np.clip(p_true*(1-p_true),0,None)/ne_total)\n",
    "    e_pred =  np.sqrt(np.clip(p_pred2-p_pred*p_pred,0,None)/ne_total)\n",
    "    \n",
    "    return  p_true, p_pred, e_true,  e_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some more helper functions for plotting the calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_diagram_plot(**kwargs):\n",
    "    \"\"\"\n",
    "    Returns the fig and ax objects for a aspect 1 plot with predefined axes labels and\n",
    "    diagonal line for plotting the reliability diagram (calibration curve)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(**kwargs)\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_ylabel('p observed')\n",
    "    ax.set_xlabel('p predicted')\n",
    "    ax.plot([0,1],[0,1], linewidth=1, linestyle='--', color = 'grey');\n",
    "    return fig, ax\n",
    "\n",
    "def add_reliability_diagram(p_pred, p_true,  e_pred=None, e_true=None, color ='blue', ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots the  reliability diagram given by p_pred, p_true,  e_pred, e_true on the given axes. If axis is not \n",
    "    given it uses the current Axes instance.\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "    ax.scatter(p_true,p_pred, facecolor = 'white', edgecolor = color, zorder = 10, **kwargs)\n",
    "    if not (e_pred is None):\n",
    "        ax.errorbar(p_true,p_pred, yerr = e_pred, fmt = 'none', ecolor=color, capsize=4, elinewidth=1)    \n",
    "    if not (e_true is None):\n",
    "        ax.errorbar(p_true,p_pred, xerr = e_true, fmt = 'none', ecolor=color, capsize=4, elinewidth=1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally here is our calibration plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_test_rd =  reliability_diagram1(test_data.Gender=='Female', test_predicted_proba, 20)\n",
    "\n",
    "fig, ax = reliability_diagram_plot()\n",
    "#The * notation 'splices' the array into function arguments i.e. f(*[a,b,c]) = f(a,b,c)\n",
    "add_reliability_diagram(*qda_test_rd, ax = ax, label='qda')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see  it look rather well calibrated, especially considering the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe now is the time to  mention that such function as above  is already provided in the scikit-learn library (minus the errors) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(test_data.Gender == 'Female',qda.predict_proba(test_data[['Height','Weight']])[:,1], n_bins=20)\n",
    "\n",
    "fig, ax = reliability_diagram_plot()\n",
    "add_reliability_diagram(*qda_test_rd[:2], color='blue', label ='this notebook')\n",
    "add_reliability_diagram( prob_true, prob_pred+0.007,    color='red', label='sklearn')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the results are exactly the same (I have shifted points slightly to make them distinct). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the distribution of entries in the bins is not uniform as we could see on the histogram above, we can consider divide the interval $[0,1)$  in such a way that each bin contains approximatelly same number of entries. The function below generates such partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equiprobable_bins(proba, n_bins):\n",
    "    \"\"\"\n",
    "    Quick and dirty: uses the numpy.array_splut function that divides array in n\n",
    "    almost equal arrays. We only need to adjust the end points to zero and one.\n",
    "    \"\"\"\n",
    "    sorted_proba = np.sort(proba)\n",
    "    split = np.array_split(sorted_proba,n_bins)\n",
    "    mins = [a[-1] for a in split]\n",
    "    edges = np.asarray([0]+mins)\n",
    "    edges[-1]=1.0\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can check that indeed it woks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_predicted_proba, bins=equiprobable_bins(test_predicted_proba, 12), edgecolor='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the histogram is flat but the bins have differentwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use this partitioning for the calibration plot we get a much nicer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_test_q_rd= reliability_diagram1(test_data.Gender=='Female', test_predicted_proba, bins=equiprobable_bins(test_predicted_proba, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = reliability_diagram_plot()\n",
    "add_reliability_diagram(*qda_test_q_rd, ax = ax, label='qda')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make my function mimick the built if function `calibration_plot` I will slightly change the interface of the `reliability_diagram` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.core.series\n",
    "def reliability_diagram(y_true, y_proba, n_bins, strategy = 'uniform'):\n",
    "    if strategy == 'uniform':\n",
    "        edges =p.linspace(0,1,bins+1) #uniform bins\n",
    "    elif strategy ==  'quantile':\n",
    "         edges = equiprobable_bins(y_proba, n_bins) #same number of points in bin\n",
    "    else:\n",
    "        raise ValueError(\"strategy argument got an unexpected value '{:s}'\".format(strategy))\n",
    "    \n",
    "    # rest is unchanged\n",
    "    true_values = y_true.values if isinstance(y_true, pandas.core.series.Series) else y_true\n",
    "    prob_bins = np.digitize(y_proba, bins=edges).astype('int32')\n",
    "    total = np.zeros(n_bins+2)\n",
    "    positive  = np.zeros(n_bins+2)\n",
    "    bin_mean  = np.zeros(n_bins+2)\n",
    "    bin_mean2 = np.zeros(n_bins+2)\n",
    "    for i,p in enumerate(y_proba):\n",
    "        bin_index = prob_bins[i]\n",
    "        total[bin_index]    += 1\n",
    "        positive[bin_index] += true_values[i]\n",
    "        bin_mean[bin_index] += p\n",
    "        bin_mean2[bin_index] += p*p\n",
    "        \n",
    "    non_empty = (total>0)\n",
    "    ne_total = total[non_empty]\n",
    "    p_true = positive[non_empty]/ne_total \n",
    "    p_pred = bin_mean[non_empty]/ne_total \n",
    "    p_pred2 =  bin_mean2[non_empty]/ne_total \n",
    "    e_true = np.sqrt(np.clip(p_true*(1-p_true),0,None)/ne_total)\n",
    "    e_pred =  np.sqrt(np.clip(p_pred2-p_pred*p_pred,0,None)/ne_total)\n",
    "    return  p_true, p_pred, e_true,  e_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the calibration curve gives a detailed picture of the predictor performance it can be summarized by so called [Brier score](https://en.wikipedia.org/wiki/Brier_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{N}\\sum_{i=1}^N\\left(o_i- p(x_i)\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $o_i$ is the actual (real) result (0 or 1) and $p(x_i)$ is the predicted probability. The sum is over all data examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__ What values can the the Brier score take? When is it maximum? When it is minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how it works consider the case where $p(x_i)$ can take only a finite number of values $p_j, j =1,\\ldots,K$. We can split the sum into $K$ parts, each part $j$ corresponding to examples where classifier predicted probability $p_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{N}\\sum_{j=1}^K \\sum_{i=1}^{N_j}\\left(o_{ji}- p_j\\right)^2\n",
    "=\\frac{1}{N}\\sum_{j=1}^K \\sum_{i=1}^{N_j}\\left(o_{ji}^2-2 o_{ji} p_j+ p_j^2\\right)\n",
    "=\\frac{1}{N}\\sum_{j=1}^K N_j \\left(\\bar{o}_{j}^2-2 \\bar{o}_{j} p_j+ p_j^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_j$ is the number of entries in group $j$. In the last equation we  have used the fact that $o_{ij}=0,1$ so $o_{ji}^2=o_{ji}$.  Fir the same  reason the  average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{o}_{j}\\equiv \\frac{1}{N_j}\\sum_{i=1}^{N_j}o_{ji}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the observed frequncy of positive examples in group $j$.  Putting this all togheter we finally obtain for the Brier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{N}\\sum_{j=1}^K N_j \\left(\\bar{o}_{j}- p_j\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now clear that it measures the discrepancy between predicted and observed frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's check what is the value of Brier score for our QDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((test_predicted_proba-(test_data.Gender=='Female'))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you can probably guess that a suitable function is also provided by the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(test_data.Gender=='Female', test_predicted_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has a suffix 'loss' which indicates that smaller the value, the better the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try  Naive Bayes classifier. As Naive Bayes makes strong independency assumptions which  definitelly are not satisfied in our case (height  and weight are clearly strongly correlated), we can expect that the probabilities are wrong.  Let's check if it shows in the calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(train_data[['Height', 'Weight']], train_data.Gender=='Female')\n",
    "nb_test_predicted_proba = nb.predict_proba(test_data[['Height', 'Weight']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_q_rd = reliability_diagram(test_data.Gender=='Female', nb_test_predicted_proba, n_bins=12, strategy='quantile')\n",
    "fig, ax = reliability_diagram_plot()\n",
    "brier_nb = brier_score_loss(test_data.Gender=='Female', nb_test_predicted_proba)\n",
    "add_reliability_diagram( *nb_test_q_rd, ax = ax, label='nb {:.4f}'.format(brier_nb) )\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a characteristic inverted sigmoid function kind of shape. But observe that the Brier score is still quite low. That is because most of the data points are concetrated around probability zero or one where the predictions are quite accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we correct or _calibrate_ the classifier ? Looking at the reliability diagrams we may think that we could adjust the probabilities output by classifier to correct for miss calibration and indeed this can be done with sufficient data. The simplest method is the _histogram_ method. We take a function that is piecewise constant over each bins that were used to plot the calibration curve. The  value in the bin is the true positive examples frequency in that bin. Thist will correct  the calibration plot _exactly_ (as long as we use same bins). Functions below calculates the correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.core.series\n",
    "#This is a partial repetition of the reliability diagram function\n",
    "def bined_positive_freq(y_true, y_proba, n_bins, strategy = 'uniform'):\n",
    "    if strategy == 'uniform':\n",
    "        edges =p.linspace(0,1,bins+1)\n",
    "    elif strategy ==  'quantile':\n",
    "         edges = equiprobable_bins(y_proba, n_bins)\n",
    "    else:\n",
    "        raise ValueError(\"strategy argument got an unexpected value '{:s}'\".format(strategy))\n",
    "    \n",
    "    \n",
    "    true_values = y_true.values if isinstance(y_true, pandas.core.series.Series) else y_true\n",
    "    prob_bins = np.digitize(y_proba, bins=edges).astype('int32')\n",
    "    total = np.zeros(n_bins+2)\n",
    "    positive  = np.zeros(n_bins+2)\n",
    "    for i,p in enumerate(y_proba):\n",
    "        bin_index = prob_bins[i]\n",
    "        total[bin_index]    += 1\n",
    "        positive[bin_index] += true_values[i]\n",
    "        \n",
    "    p_true = positive/np.clip(total,1,None)#we are  returning all the bins so we must make sure we are not dividing by zero\n",
    "    \n",
    "    return  p_true, edges\n",
    "\n",
    "def make_correction(p_true, bins):\n",
    "    def correction(p):\n",
    "        i = np.digitize(p, bins).astype('int32')\n",
    "        return p_true[i]\n",
    "    \n",
    "    return correction\n",
    "\n",
    "\n",
    "def calibration_function(y_true, y_proba, n_bins, strategy = 'quantile'):\n",
    "    p_true, edges = bined_positive_freq(y_true, y_proba, n_bins, strategy)\n",
    "    \n",
    "    return make_correction(p_true, edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = calibration_function(test_data.Gender=='Female', nb_test_predicted_proba, n_bins=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting in a function mimicking the calibration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(0,0.99999,500) # value one falls outside the bins ... \n",
    "plt.plot(ps, f(ps))\n",
    "plt.plot([0,1],[0,1], color ='grey', linewidth=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After corrections the classifier is calibrated perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_predicted_proba_cal = f(nb_test_predicted_proba)\n",
    "nb_test_q_rd_calibrated = reliability_diagram(test_data.Gender=='Female', nb_test_predicted_proba_cal, n_bins=12, strategy='quantile')\n",
    "fig, ax = reliability_diagram_plot()\n",
    "brier_nb_cal = brier_score_loss(test_data.Gender=='Female', nb_test_predicted_proba_cal)\n",
    "add_reliability_diagram(*nb_test_q_rd, ax = ax, label='nb {:.4f}'.format(brier_nb) )\n",
    "add_reliability_diagram(*nb_test_q_rd_calibrated, ax = ax, label='nb  calibrated {:.4f}'.format(brier_nb_cal), color='red' )\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is however not the right approach, this is cheating! The classifier was calibrated on the same date we used to test it.\n",
    "The proper way is to use different sets for training, calibrating and finally testing. We will use our training set and split it into two sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2, calibration_data = train_test_split(train_data, train_size=0.75, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use it to train a new Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cal = GaussianNB()\n",
    "nb_cal.fit(train_data2[['Height', 'Weight']], train_data2.Gender=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cal_test_predicted_proba = nb_cal.predict_proba(test_data[['Height', 'Weight']])[:,1]\n",
    "nb_cal_test_q_rd = reliability_diagram(test_data.Gender=='Female', \n",
    "                                                                     nb_cal_test_predicted_proba, \n",
    "                                                                     n_bins=12, strategy='quantile')\n",
    "                                                                    \n",
    "fig, ax = reliability_diagram_plot()\n",
    "brier_nb = brier_score_loss(test_data.Gender=='Female', nb_test_predicted_proba)\n",
    "add_reliability_diagram(*nb_cal_test_q_rd, ax = ax, label='nb {:.4f}'.format(brier_nb) )\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this classifier exhibits similar miss-calibration as before. We will use the calibration data for  calculating the correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cal_calibration_predicted_proba = nb_cal.predict_proba(calibration_data[['Height', 'Weight']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cal_calibration_q_rd = reliability_diagram(calibration_data.Gender=='Female', nb_cal_calibration_predicted_proba,\n",
    "                                                                                 n_bins=12, strategy='quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_nb = brier_score_loss(test_data.Gender=='Female', nb_test_predicted_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally here is the effect of the correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = calibration_function(calibration_data.Gender=='Female', nb_cal_calibration_predicted_proba, n_bins=12)\n",
    "nb_test_predicted_proba_cal = f(nb_cal_test_predicted_proba)\n",
    "nb_cal_test_q_rd_calibrated = reliability_diagram(test_data.Gender=='Female', nb_test_predicted_proba_cal, n_bins=12, strategy='quantile')\n",
    "fig, ax = reliability_diagram_plot()\n",
    "brier_nb_cal = brier_score_loss(test_data.Gender=='Female', nb_test_predicted_proba_cal)\n",
    "add_reliability_diagram(*nb_cal_test_q_rd, ax = ax, label='nb {:.4f}'.format(brier_nb) )\n",
    "add_reliability_diagram(*nb_cal_test_q_rd_calibrated, ax = ax, label='nb  calibrated {:.4f}'.format(brier_nb_cal), color='red' )\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks fairly good but please remember that we have discretized the response of the classifier into 12 values! Also the calibration  does not make classifier better at classification. Actually it can make it  worse, at least this method :( That's because the correction function is not a strictly increasing function. And indeed the calibrated classifier has slightly worse AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot()\n",
    "add_roc_curve(test_data.Gender=='Female', nb_test_predicted_proba, \"NB \", ax=ax)\n",
    "add_roc_curve(test_data.Gender=='Female', nb_test_predicted_proba_cal, \"NB calibrated\", ax=ax)\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be corrected if we use more bins, but that means less points in each bin and so bigger errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only a short introduction to the topic of calibrating the classifiers. There are of course more calibration  methods besides the one I have presented. The scikit-learn library contains class `CalibratedClassifierCV` that can perform calibration using two different methods. You can read more about it [here](https://scikit-learn.org/stable/modules/calibration.html#calibration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}