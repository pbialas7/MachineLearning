{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">#### \"One sees, from this Essay, that the theory of probabilities is basically just common sense reduced to calculus; it makes one appreciate with exactness that which accurate minds feel with a sort of instinct, often without being able to account for it.\"\n",
    "> \"Théorie Analytique des Probabilités\" Pierre-Simon Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "description"
    ]
   },
   "source": [
    "Because they deal with uncertain events most of the machine learning methods can be framed in the language of probability calculus. \n",
    "In this notebook I will very briefly recall the basics concepts of the probability calculus and introduce the notation I will be using, hopefully consistently, throughout the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "description"
    ]
   },
   "source": [
    "But keep in mind that this is not a supposed to be a textbook  on probability! Please treat this as a list of concepts and definitions that you have to refresh. It will also serve as a breief introduction to various Python packages. But again this is not a tutorial on Python. The code is provided as a guidance for you and it's up to you to lookup  explanantion in documentation if  needed. I  am of course also happy to help. You can consult me on the chat on Teams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "description"
    ]
   },
   "source": [
    "The lecture includes some simple problems to help you check your understanding. Some problems have answers right in the notebook. I will try to hide the content of this cells, please try to solve the problem before looking at the answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine any process that can have an upredictable outcome. This could be the results of a coin toss,  number of passengers on the bus etc. Let's however assume that we know the set of all possible outcomes of this process and call this set $S$. This set is often called _sample space_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any subset $A$ of $S$ denoted $A\\subseteq S$ will be called an _event_. If process has an outcome $s\\in S$ then we say that the event $A$ happened if $s\\in A$. An event that contain only one set element $\\{s\\}$ will be called an _elementary_ event, _atomic_ event or _sample point_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical textbook example would be a  coin toss. In this case $ S=\\{H, T\\}$ and there are only four possible events (including the empty set).  There are two elementary events $\\{H\\}$ nad $\\{T\\}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Dice roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the sets of all possible outcomes of a roll of two dice? How many elements it contains? Write down the event $A$ - \"the sum of the points is 9\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S=\\{(i,j): i,j\\in\\{1,2,3,4,5,6\\}\\},\\quad \\#S=36,\\quad A=\\{(3,6), (4,5), (5,4), (6,3)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger examples this would be impractical, but just for fun let's code this in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_dice = {(i,j) for i,j in product(range(1,7), range(1,7))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(S_dice))\n",
    "print(S_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = set( filter(lambda s: sum(s)==9, S_dice) )\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of an event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the outcome of a process is unpredictable, so are the events.    However some events are more likely to happen then others and we can quantify this by assigning  a number to each event that we call _probability_ of that event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0\\leq P(A) \\leq 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this number really means is still subject to discussion and interpretation and I will not address this issue. For me this is a measure of \"degree of certainty\" with zero probability denoting impossible event and one denoting a certain event.  What is important is that those numbers cannot be totaly arbitrary. To be considered a valid measure, probabilities must satisfy several  axioms consistent with our common sense: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A)\\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(S)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any integer $k>1$ including $k=\\infty$ if events $A_i$ are mutually disjoint that is for each $i\\neq j$ $A_i \\cap A_j =\\varnothing$ then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A_1\\cup A_2\\cup\\cdots \\cup A_k) = P(A_1)+P(A_2) + \\cdots + P(A_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important colorary is that when the set of outcomes is countable the probability of an event $A$ is the sum of the probabilities for each elementary event contained in $A$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A) = \\sum_{s\\in A}P(\\{s\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set is countable when we can assign an unique natural number to each of its elements, in other word we can count its elements. All finite sets are of course countable. An example of not countable set is provided e.g. by the real numbers or any interval $[a,b)$ with $b>a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that in case of countable outcomes it is enough to specify the probability of each elementary event. \n",
    "\n",
    "In the following  I will ommit the set parenthesis for the elementary events i.e. assume $P(s)\\equiv P(\\{s\\})$. It follows from axiom 2. and 3. that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{s\\in S} P(s) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem: Complementary event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(S\\setminus A)= 1-P(A)\\text{ where } S\\setminus A = \\{s\\in S: s\\notin A\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "It follows directly from the second and third axiom after noting that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$(S\\setminus A) \\cup A = S \\text{ and } (S\\setminus A) \\cap A = \\varnothing$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of the probability can be somewhat hazy and verges upon philosophy. My take on this is that to calculate the probability we need a _model_ of the process. E.g. for the dice example the model is that all elementary events are equiprobable,  leading to assigning probability $1/36$ to every possible two dice roll outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection with experiment (reality) is given by the Borel's law of large number. It states that if we repeat an experiment under same conditions many times, the fraction of times an event happens will converge to the probability of this event. This is a fundation of _frequentist_ interpretation of probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is harder to interpret the probability of one-off events. E.g. \"there is a 30% chance that it will rain tomorrow', or there is 80% chance that Andrzej Duda will win the elections\". By the way the last statement actually should be the conditional probability: \"assuming that the elections will take place\". Please take some time to think how that statements can be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a probability of an event change when we know that some other event happed? That is a central question in machine learning and is  answered by _conditional_ probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A|B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This denotes the probability that event $A$ happened on  condition that the event $B$ happend. The formal definition is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take as an example roll of two dice. What is the probability that  the sum is seven ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now suppose that someone tells us that we have rolled three on one die. Did the the probability change?  Again I will use some Python code althought it is probably faster to   calculate this \"by hand\". Try it before proceding further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's calculate the probability of that event without any conditions.  The event $A$ - \"we have rolled seven points in total\" contains $6$ elementary events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = set( filter(lambda s: sum(s)==7, S_dice) )\n",
    "print(len(A))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to have nice fractions instead of floats\n",
    "from fractions import Fraction \n",
    "P_A =  Fraction(len(A),len(S_dice))\n",
    "print(P_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event $B$ - \"there is a three on one die\" contains $11$ elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = set( filter(lambda s: s[0]==3 or s[1]==3 , S_dice) )\n",
    "print(len(B))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_B = Fraction(len(B), len(S_dice))\n",
    "print(P_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event $A\\cap B$ has only two elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_cap_B = A.intersection(B)\n",
    "P_A_cap_B = Fraction(len(A_cap_B), len(S_dice))\n",
    "print(len(A_cap_B))\n",
    "print(A_cap_B)\n",
    "print(P_A_cap_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_A_cond_B = P_A_cap_B/P_B\n",
    "print(P_A_cond_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is marginally bigger then $P(A)=1/6$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to keep in mind that conditional probability $P(A|B)$ is not symetric! _E.g._ when it rains the probability that sidewalk will be wet is one. On the other hand when the sidewalk is wet it does not imply  with certainty that it has rained, it may have  been _e.g._ washed by our neighbour. But as we will see many times in course of this lecture the ability to \"invert: conditional probability comes in very handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\quad\\text{and}\\quad P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use second expression to calculate $P(A\\cap B)$ and subsitute it into first to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{P(B|A) = \\frac{P(A|B)P(B)}{P(A)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula is know as Bayes theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to the \"wet sidewalk problem\" i.e. we look in the morning through our window and see wet sidewalk. What is the probability that it has rained at night? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A$ is the event \"sidewalk is wet\" and $B$ is the event \"it has rained\" then $P(A|B)=1$ and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(rain|wet)=\\frac{P(rain)}{P(wet)}=\\frac{P(rain)}{P(wet|rain)P(rain)+P(wet|wash)P(wash)}=\\frac{P(rain)}{P(rain)+P(wash)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to a paradox if $P(rain)+P(wash) >1 $. It would imply that after seeing wet sidewalk the probability of rain _decreases_. However in writing the denominator we have made an implicit but reasonsable assumption that events \"rain\" and \"wash\" are mutually exclusive and so the sum of their probabilities must be less then one.  We can write this explicitely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{P(rain)}{P(rain)+P(wash|\\neg rain)(1-P(rain))}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider some \"corner cases\". If our neigbour always washes the sidewalk when it does not rain then the results is $P(rain)$ - sidewalk is always wet, we do not have any additional information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our neigbour never washes the sidewalk then results is one - the only reason for wet sidewalk is rain so when it is wet it must have rained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our neighbour washed the sidewalk only half of the times when it does not rain we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{ 2 P(rain)}{1+P(rain)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if _e.g._ $P(rain)=1/7$  seeing wet sidewalk increses that chance to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2 * Fraction(1,7)/(1+Fraction(1,7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this using `matplotlib`  and `numpy` libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the whole family of plots corresponding to different values of $P(wash|\\neg rain)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(0,1,100)\n",
    "plt.xlabel(\"P(rain)\")\n",
    "plt.ylabel(\"P(rain|wet)\");\n",
    "plt.plot(ps, ps, c='grey', linewidth = 1);\n",
    "for pw in [0.1, 0.2, 0.3, 0.4, 0.5, 0.75]:\n",
    "    plt.plot(ps, ps/(ps+pw*(1-ps)),label = \"P(w|not r) = {:.2f}\".format(pw)); \n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    },
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__: Base rate fallacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "You are tested for a rare disease (1 person in 250). Test has 80%  true positive rate and  10% false positive rate. i.e. test gives positive (you are ill) result for 80% of ill patients and for 10% of healthy patients.   \n",
    "\n",
    "Your are tested positive, what are the chances you have the disease? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "What we need is the  probability that we are ill on condition that we have been tested positive:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$P(ill|P)= \\frac{P(ill, P)}{P(P)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "The probability of being ill and tested positive is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "p_ill_p = 0.004 * 0.8  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "The probability of being tested positive is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$P(P)=P(ill,P)+P(\\neg ill, P)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "p_p = .004*0.8 + 0.996*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "and finally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "p_ill_cond_p = p_ill_p/p_p\n",
    "print(\"{:4.1f}%\".format(100*p_ill_cond_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "So there is no cause to despair yet :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "### Increase of information (learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could say that this test is useless if positive  result gives only $3\\%$ chance of being ill. And  this particular test was actually discarded. But it is not totaly useless. Before taking the test our chance of being ill was $0.4\\%$. After seing the positive result it \"jumped\" more then ten times to $3.1\\%$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$0.004 \\longrightarrow 0.031$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seing a negative result our chances of being ill dropped for times:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$0.004 \\longrightarrow 0.001 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may happen that  knowledge that $B$ happened does not change  the probability of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A|B) = P(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say then that  events $A$ and $B$ are _independent_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example when throwing the coin the outcome of toss does not depend in any way on the outcome of previous tosses or in case of dice the  face they land on are independent etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the definition of conditional independence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{P(A\\cap B)}{P(B)}  = P(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we obtain  a more familiar factorisation criterion for joint probability of independent events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A\\cap B) = P(A)\\cdot P(B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of an unpredictable process is too general and in the following we will restrict ourself to outcome sets that are subsets of $\\mathbb{R}^M$. We will call such a process a _random variable_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the outcome set is countable, in particular if it is finite, then we call such random variable _discrete_. As shown above  to characterise such variable is enough to assign  the probability to each of the elements of the outcome set. This is called _probability mass function_ (pmf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will denote the probability of random variable $X$  taking a value $s\\in S$ by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=s)\\equiv P_X(\\{s\\})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we will often abreviate it further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P_X(s) \\equiv P_X(\\{s\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will omit the subscript $X$ when it's clear from the context  which random variable I have in mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to random variables $X$ and $Y$ with outcome sets $S_X$  and $S_Y$ we can treat them  together as one random variable with outcome set \n",
    "$S_{X\\times  Y}=S_{X}\\times S_Y$ and joint probability mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{X\\times Y}(X=x, Y=y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in only one of the variables we can calculate its probability mass function as _marginal_ pmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_X(X=x)= \\sum_y P(X=x, Y=y)\\qquad P_Y(Y=y)= \\sum_x P(X=x, Y=y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idependent random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of independence applies also to random variables. We say that two random variables $X$ and $Y$ are independent iff (if and only if)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=x|  Y=y)= P(X=x)\\quad\\text{for all }x,y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{X\\times Y}(X=x, Y=y)= P_X(X=x)\\cdot P_Y(Y=y) \\quad\\text{for all }x,y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example when $X$ and $Y$ represents a first and second toss of a coin they are independent random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation value of a function with respect to a random variable $X$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_X[f(X)] \\equiv \\sum_i f(x_i)P(X=x_i),\\quad x_i\\in S_X$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular the expectation value of the random variable _i.e._ its _mean_ or _average_ is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_X[X] \\equiv \\sum_i x_i P(X=x_i)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\operatorname{var}(X)=\\sigma^2 = E[(X-E[X])^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of variance $\\sigma$  is called _standard deviation_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem: linearity of expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{X\\times Y}[a X + b  Y]= a E_X[X] + b E_Y[Y]\\quad\\text{where }a,b\\text{ are constants}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$E_{X\\times Y}[a X + b  Y]=\\sum_{x,y}\\left(a x + b y\\right) P(X=x, Y=y) = a \\sum_{x,y} x  P(X=x, Y=y)+b \\sum_{x,y}  y P(X=x, Y=y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$a \\sum_{x,y} x  P(X=x, Y=y) = a\\sum_x  x \\sum_y P(X=x, Y=y)= a\\sum_x x P(X=x) = a E[X]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "and same for other term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem:__ Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Show that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "$$\\operatorname{var}(X) = E[X^2]-E[X]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$\\operatorname{var}(X) = E[(X-E[X])^2] = E\\left[X^2-2 E[X]+E[X]^2\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$E[X]$ is a constant so using the linearity of expectation value we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$E\\left[X^2-2 E[X]+E[X]^2\\right]=E[X62]+2E[X]E[X]-E[X]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation value of a product of two random variables is given by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{X\\times Y}[X\\cdot Y]=\\sum_{x,y} x\\cdot y\\, P(X=x , Y=y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the two random variables are independent then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{X\\times Y}[X\\cdot Y]=\\sum_{x,y} x y P(X=x , Y=y)\n",
    "=\\sum_{x,y} x y P(X=x)P(Y=y)=\n",
    "\\left(\\sum_{x} x P(X=x)\\right)\n",
    "\\left(\\sum_{y} y P(Y=y)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leading to the familiar result that the expectation value of independent random variables factorises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{X\\times Y}[X\\cdot Y]=E_X[X] E_Y[Y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\operatorname{cov}(X,Y)=E_{X\\times Y}[X\\cdot Y]-E_X[X] E_Y[Y]=E[(X-E[X])(Y-E[Y])]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is called a _covariance_ and when variables $X$ and $Y$ are independent then it is equal to zero. Please take note however that zero covariance does not imply indpendence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnitude of the covariance depeds on the magnitude of random variables e.g. scaling one variable by $a$ will also scale the covariance by $a$. That is why often a normalised version called _correlation_ coeficient is used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\operatorname{corr}(X,Y)=\\frac{E\\left[(X-E[X])(Y-E[Y])\\right]}{\\sqrt{E\\left[(X-E[X])^2\\right]E\\left[(Y-E[Y])^2\\right]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__: Linear dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Please check that when variables $X$ and $Y$ are linearly dependent _i.e._  $Y =a \\cdot X + b$ correlation between them is 1 or -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate this with some Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.uniform(size=10000)\n",
    "ys = np.random.uniform(size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covariance \n",
    "np.mean( (xs-xs.mean())*(ys-ys.mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get same result using build in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation\n",
    "np.mean( (xs-xs.mean())*(ys-ys.mean() ))/np.sqrt(np.mean( (xs-xs.mean())**2)*np.mean((ys-ys.mean() )**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = xs + ys \n",
    "np.corrcoef((xs,ys,zs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem:__ Variance of sum  of independent random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Show that if random variables $X$ and $Y$ are independent then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "$$\\operatorname{var}(X+Y) = \\operatorname{var}(X) +  \\operatorname{var}(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other characteristics of the random variables include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median $m$ is a number that divides the values of the random variable into two  sets as equiprobable as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X\\le m) \\ge \\frac{1}{2}\\text{ and } P(X\\ge m) \\ge \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "#### Problem: Median for coin toss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "What is a median for coin toss if you assign value $1$ to heads and $0$ to tails? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode is the value for which the probability mass function has its maximum. That's an element most likely to be sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\operatorname{mode}(X)=\\underset{x_k}{\\operatorname{argmax}} P(X=x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last characteristic of an distribution that I would like to introduce is the _entropy_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H[X] \\equiv -\\sum_i P_X(x_i) \\log P_X(x_i)=-E[\\log X] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a \"measure of randomnes\", the greater entropy, the greater randomness or harder to predict outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example a coin toss with unfair coin with probability $p$ of comming up heads. The entropy is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-p\\log p - (1-p)\\log(1-p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(0,1,100)[1:-1] # we reject 0 and 1\n",
    "plt.plot(ps, -ps *np.log(ps)-(1-ps)*np.log(1-ps));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the entropy is maximum when $p=1/2$ and zero when $p=0$ or $p=1$ that is when the outcome is certain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some common discrete random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernouli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli distribution has a two elements outcome set $S=\\{0,1\\}$. It is charaterised by a single parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p\\equiv P(X=1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation value of this distribution  is equal to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0\\cdot (1-p) + 1\\cdot p = p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "#### Problem: variance of Bernouli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Calculate the variance of Bernouli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "#### Problem: median of the Bernoulli distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Calculate the median of Bernouli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.stats` module contains large quantity of distribution objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = st.bernoulli(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a Bernouli distribution object with $p=0.7$. In scipy this is calles _frozen_ distribution meaning that its parameters are set (frozen). Using this object we have acces to different properties of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.pmf(0), b.pmf(1), b.mean(), b.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very usefull feature is the posibility to generate random numbers according to the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.rvs(size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to use distributions is to pass the paramaters directly without creating a frozen distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.bernoulli.rvs(p = 0.5,size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial distribution is characterised by two parameters: $n$ and $p$ and it is the number of successes (ones) in $n$ independent Bernouli trials. So the outcome set $S=\\{0,\\ldots,n\\}$\n",
    "has $n+1$ elements. The probability mass function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=k) = \\binom{n}{k}p^k(1-p)^{n^k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "#### Problem: Mean and variance of binomial random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Calculate the mean (expectation value) and variance of binomial random variable.\n",
    "\n",
    "#### Hint\n",
    "Use the fact that binomial random variable can be expressed as a sum of $n$ independent Bernouli random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=12\n",
    "bin = st.binom(n=n,p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(0,n+1), bin.pmf(np.arange(0,n+1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical (multinoulli) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical distribution is a straightforward generalisation of Bernouilli distribution. While Bernoulli distribution has two possible outcomes categorical has \n",
    "random variable has $m$.  It is characterised by providing its probability mass function, that is probability for every category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_k= P(X=x_i),\\; k=1,\\ldots,m\\quad \\sum_{k=1}^m p_k = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the multinoulli distribution in Python as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = np.asarray([0.3,0.4,0.1,0.2])\n",
    "m = st.rv_discrete(values=([1,2,3,4],pk));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise distribution by histograming. `pyplot.hist` function conveniently both creates and plots a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(m.rvs(size = 1000), bins=4, range=(0.5,4.5) , rwidth =0.5,  align = 'mid', histtype='bar');\n",
    "plt.scatter(np.arange(1,5), pk*1000,s=40,c='red', zorder = 5, label=\"$\\\\bar{n}_k$\")\n",
    "plt.xticks(np.arange(1,5));\n",
    "plt.xlabel(\"k\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In analogy to Bernoulli distribution  the entropy of the categorical distribution is  greatest when all categories are equally probable with $p_k=1/m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial distribution generalises the binomial distribution in a same way that mutinoulli distribution generalises Bernoulli distribution. \n",
    "Binomial distributions counts the number of successes in $n$ Bernouilli trials. The multinomial random counts the number of results in each category for $n$ trials of the categorical random variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S=\\left\\{(n_1,\\ldots,n_m): n_k\\gt0, \\sum_{k=1}^m n_k = n \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_k$ is the number of times that number $k$ has showed up in the $n$ trials of the categorical random variable. Its pmf is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(n_1,\\ldots,n_m)=\\frac{n!}{n_1!\\cdots n_m!}p_1^{n_1}\\cdots p_m^{n_m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = multinomial(n=10,p=[0.4,0.5,0]) # when probabilities do not add to one the last probability is  adjusted acordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = mult.rvs(10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.sum(axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that average number of  clients in a shop (or  clicks on a web page) is $\\mu$ per hour. If the probability of visit or click is constant in time then the distribution of the number of clients in one hour is given by the Poisson probability mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X = k) = e^{-\\mu}\\frac{\\mu^k}{k!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 7.7\n",
    "ks = np.arange(0,20)\n",
    "plt.scatter(ks, st.poisson.pmf(ks,mu=mu))\n",
    "plt.axvline(mu, linewidth=1, c='grey')\n",
    "plt.xticks(ks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to check that the expectation value of this distribution is indeed $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[X]=e^{-\\mu}\\sum_{k=0}^\\infty k \\frac{\\mu^k}{k!} \n",
    "e^{-\\mu}\\mu \\frac{\\text{d}}{\\text{d}\\mu}\\sum_{k=0}^\\infty  \\frac{\\mu^k}{k!} = e^{-\\mu}\\mu \\frac{\\text{d}}{\\text{d}\\mu} e^\\mu = \\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    " __Problem__: Variance of Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Calculate the variance of Poisson distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Hint__:  Use same \"differentatiation trick\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long tailed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above distributions were \"well behaved\" in the sense that we could calculate both mean and variance for each. Actually all of them had all the moments. The nth moment is defined as expectation value $E[X^n]$.  But there exists distribution that don't even have the average! Take for example the Zipf distribution with probability mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Z=k|\\alpha)=\\frac{1}{\\zeta(\\alpha)}\\frac{1}{k^\\alpha}\\quad k>0,\\; \\alpha>1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\zeta(\\alpha)$ is the Riemann zeta function. This distribution does not have any moments with $n\\ge\\alpha-1$  as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_Z[k^n]=\\frac{1}{\\zeta(\\alpha)}\\sum_{k=1}^\\infty \\frac{1}{k^{\\alpha-n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which diverges when $\\alpha-n\\le1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.arange(1,20)\n",
    "plt.bar(ns,  st.zipf.pmf( ns, 1.7), width=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution above does not have a finite expectation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.zipf.mean(1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that median is well defined even in such case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.zipf.median(1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a random variable does not have a well defined average the sample average does not converge! When gathering more and more data we will get eratic, eventually diverging  behaviour. Below I have plotted the average calculated on sample size from 1 to 100000. For comparison I have included same plot with Poisson distribution with same mean. Please run this cell several times, see how the output changes! Then experiment with different values of the parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "alpha = 2.1\n",
    "zipf_mu  = st.zipf.mean(alpha)\n",
    "zipf_data = st.zipf.rvs(alpha,size = n)\n",
    "plt.plot(np.cumsum(zipf_data)/np.arange(1,n+1),label=\"Zipf $\\\\alpha = {:5.2f}\\\\; \\\\mu = {:5.2f}$\".format(alpha,zipf_mu));\n",
    "\n",
    "mu = 10 if np.isinf(zipf_mu) else zipf_mu\n",
    "poisson_data = st.poisson(mu).rvs(n)\n",
    "plt.plot(np.cumsum(poisson_data)/np.arange(1,n+1), label=\"Poisson $\\\\mu = {:5.2f}$\".format(mu));\n",
    "plt.axhline(mu,c = 'red', linewidth = 1, linestyle ='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the average can change abruply after adding one more sample.  You may think that does distributions are \"pathological\". Nevertheless they occur quite frequetly in real settings _e.g._ the wealth distribution follows similar curve. So when for example you take 1000 people and measure average height you will not change this average drasticaly even when she is a midget or a giant. But when you calculate the average income it may happen that the person you add is Bill Gates and the average will change dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By continous random variables we will understand variables with have a connected subset of $\\mathbb{R}$ e.g. an interval as the outcome set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability density function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the set of the outcomes is not countable _i.e._ we cannot enumerate them, we cannot  specify probability of the event by adding probabilities of elementary events it contains.  Actually for most of the interesting continous random variables the probability of a single outcome is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we can ask for the probability that the outcome is smaller then some number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F(x) = P(X\\le x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a cummulative distribution function (cdf) or _cummulant_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask for the probability that the outcome lies in a small interval $\\Delta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x<X<x+\\Delta x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small intervals and \"well behaved\" random variables we expect that this probability will be proportional to $\\Delta x$, so let's take the ratio and go to the limit $\\Delta x\\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{P(x<X<x+\\Delta x)}{\\Delta x}\\underset{\\Delta x\\rightarrow 0}{\\longrightarrow} P_X(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this limit exists it's called probability density function (pdf).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a relation between cdf and pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P_X(x) =\\frac{\\text{d}}{\\text{d}x}F_X(x)\\qquad F_X(x) = \\int\\limits_{-\\infty}^x P_X(x')\\text{d}x'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the definitions and properties of the probability mass function apply to probability density function with summation changed to integral _e.g._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_X[f(X)]\\equiv \\int\\text{d}x f(x) P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful continuous random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably  the most known, if not the only known, continuous distribution is the _normal_ or Gaussian distribution. It is characterised by its mean $\\mu$  and  variance $\\sigma^2$. Its probability density function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\displaystyle -\\frac{(x-\\mu)^2}{2\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it has a characteristic bell-like shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-5,7,500)\n",
    "for s in [0.25, 0.5, 1,2]:\n",
    "    plt.plot(xs,st.norm.pdf(xs, loc=1, scale=s), label=\"$\\\\sigma = {:4.2f}$\".format(s));\n",
    "plt.axvline(1, c='grey', linewidth=1);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prevalence of this  random variable can be attributed to central limit theorem that states that, under some mild assumptions,  the sum of independent random variables  approaches the normal random variable as the number of variables tends to infinity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature  of the normal distribution is that it is the distribution with highest entropy given mean and variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the probability density function $P_X(x)$ is not restricted to be less then one. That's because this is a _density_. We  can meaningfully only ask about probability of $X$ having an outcome in an interval  which is given by the area under a fragment of the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib  = st.norm(loc=1, scale=0.25)\n",
    "a = 0.75\n",
    "b = 0.90\n",
    "xs = np.linspace(0,2,500)\n",
    "ab = np.linspace(a,b,100)\n",
    "\n",
    "plt.plot(xs,distrib.pdf(xs));\n",
    "plt.fill_between(ab,distrib.pdf(ab), alpha=0.5 )\n",
    "plt.axvline(1, c='grey', linewidth=1);\n",
    "area = distrib.cdf(b)-distrib.cdf(a)\n",
    "plt.text(0.2, 1.4, \"$P(a<X<b) = {:2f}$\".format(area), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100000\n",
    "sample = distrib.rvs(size=n)\n",
    "( (a<sample) & (sample<b)).sum()/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area was calculated using the cumulative distribution function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(a<X<b)=F_X(b)-F_X(a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0,2,500)\n",
    "plt.plot(xs,distrib.cdf(xs));\n",
    "plt.plot([a,a,0],[0,distrib.cdf(a), distrib.cdf(a)], c='grey')\n",
    "plt.plot([b,b,0],[0,distrib.cdf(b),distrib.cdf(b)], c='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "### Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) has two parameters  $\\alpha$ and $\\beta$  and its probability density function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(x|\\alpha,\\beta) =  \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n",
    "x^{\\alpha-1}(1-x)^{\\beta-1},\\quad 0\\leq x\\leq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its importance stems from  the fact that it is a _conjugate_ prior to Bernoulli distribution so it is used to set the \"probability on probability\". You will learn more about this  in bayesian_analysis notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are plots of the probability density function for some values of $\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs =np.linspace(0,1,250)\n",
    "for a in [0.25,0.5,1,2,5,10]:\n",
    "    ys = st.beta(a,a).pdf(xs)\n",
    "    plt.plot(xs,ys, label='%4.2f' %(a,))\n",
    "plt.legend(loc='best', title='$\\\\alpha=\\\\beta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here for some values of $\\alpha\\neq\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs =np.linspace(0,1,250)\n",
    "for a in [0.25,0.5,1,5]:\n",
    "    ys = st.beta(a,2.0).pdf(xs)\n",
    "    plt.plot(xs,ys, label='%4.2f' %(a,))\n",
    "plt.legend(loc=1, title='$\\\\alpha$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be more convenient to parametrise  Beta distrubution by its mean and variance. The mean and variance of Beta distribution are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\quad\\text{and}\\quad \\sigma^2=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing a new auxiliary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nu = \\alpha+\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha = \\mu \\nu,\\quad \\beta = (1-\\mu)\\nu,\\quad \\sigma^2=\\frac{\\mu(1-\\mu)}{\\nu +1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nu=\\frac{\\mu(1-\\mu)}{\\sigma^2}-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha = \\mu \\left(\\frac{\\mu(1-\\mu)}{\\sigma^2}-1\\right)\\quad\\text{and}\\quad\\beta = (1-\\mu) \\left(\\frac{\\mu(1-\\mu)}{\\sigma^2}-1\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be continued ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}