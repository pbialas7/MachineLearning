{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This prevents execution of lengthy grid search at the end of notebook. \n",
    "## A shorter taking around six minutes grid search will still be executed.     \n",
    "RUN_GRID_SEARCH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text classification - Multinomial Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes classifier  is often used in text classification e.g. for sentiment analysis or spam recognition. It assumes the so called \"bag of words model\". In this approach we treat a document as an unordered colllection of words (tokens) and only count the number of times each word was used in the document. The list of all worlds that we  consider is called the _vocabulary_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We treat the vocabulary as __one__ giantic categorical feature $X$. Then each document of length $n$ can be viewed as  draw from the multinomial distribution i.e. we select _with replecement_ $n$ words at random from the vocabulary. For more information see the references below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    " 1. A. McCallum and K. Nigam (1998). [A comparison of event models for Naive Bayes text classification](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529). Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\n",
    " 1. V. Metsis, I. Androutsopoulos and G. Paliouras (2006). [Spam filtering with Naive Bayes â€“ Which Naive Bayes?](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542) 3rd Conf. on Email and Anti-Spam (CEAS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data will consist of number of  documents with labels assigning them to $n_c$ different classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by converting documents  into vectors. Each element of the vector corresponds to one word in the vocabulary and contains the number of times this word was used in the document. If $n_s $ is the number of documents(samples) nad $n_w$ is the number of words in the vocabulary then our data can be represented as $n_s\\times n_w$ matrix $\\mathbf{x}$ and  $y_i=0,\\ldots,n_c-1$ will represent  corresponding class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $m_i$ denote the  number of words in document $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_i = \\sum_{j=0}^{n_w-1} x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the categorical Naive Bayes we split the matrix $\\mathbf{x}$ into rows corresponding to different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{x}^{(c)}\\equiv\\{\\mathbf{x}_h: y_h=c\\}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of times the ith word was used in documents belonging to class $c$  will be denoted by $n^{(c)}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n^{(c)}_i= \\sum_{j=0}^{n_s-1} x_{ji}\\delta_{y_j,c},\n",
    "\\qquad \\delta_{a,b}=  \\begin{cases}\n",
    "1 & a=b\\\\\n",
    "0 & a\\neq b\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of worlds in $\\mathbf{x}^{(c)}$ will be denoted by $n^c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n^{(c)}= \\sum_{i=0}^{n_w-1} n^{(c)}_i = \\sum_{i=0}^{n_w-1}\\sum_{j=0}^{n_s-1} x_{ji}\\delta_{y_j,c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the smoothed estimator of the conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=i|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p^{(c)}_{i} = \\frac{n^{(c)}_i+\\alpha}{n^{(c)}+n_w \\alpha}, \\quad \\alpha\\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the $p^{(c)}_i$  are estimated we estimate the conditional probability for document encoded as vector $\\mathbf{x}_j$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(C=c|\\mathbf{x}_j) = \\frac{P(\\mathbf{x}_j|C=c)P(C=c)}{\\sum_c P(\\mathbf{x}_j|C=c)P(C=c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where  $P(\\mathbf{x}_j|C=c)$ is given by the [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\mathbf{x}_j|C=c)\n",
    "=\n",
    "\\frac{(\\sum_{i=0}^{n_w-1} x_{ji})!}{\\prod_{i=0}^{n_w-1} x_{ji}!}\n",
    "\\prod_{i=0}^{n_w-1} \\left(p^{(c)}_i\\right)^{\\displaystyle x_{ji}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice most of the $x_{ji}$ are zero and sums and products in the above expression have range only over the number of unique tokesn in the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Amazon review data set. This data set is very handy because it contains both documents (reviews) and labels (ratings). We can use it to train classifier predict ratings based on the review. The original data set is HUGE  and can be  _e.g._ found [here](http://jmcauley.ucsd.edu/data/amazon/). We will use the preprocessed data from\n",
    "[ Xiang Zhang's Google Drive dir](https://drive.google.com/open?id=0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M) that can be downloaded as a tar archive. However it still has 1.5GB of data. So for the sake of this lecture I have prepared a smaller sample that I have additionally compressed with 'bz2' reducing the size to \"only\" 41MB. You can play with original file by downloading it directly from the above link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was selected using the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 85865\n",
    "data = pd.read_csv(\"../../Data/amazon_reviews/train.csv\",\n",
    "                   names=[\"rating\", \"title\", \"review\"])\n",
    "small_data,_ = train_test_split(data,train_size=300000,  stratify=data['rating'], random_state=seed)\n",
    "small_data.to_csv(\"../../Data/amazon_reviews/small.csv.bz2\", index=False, compression='bz2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of the 'stratify' argument  guarantee that proportion of each ratings will be  preserved. In this case we will have same number of documents with each rating. The smaller file can be read in using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../Data/amazon_reviews/small.csv.bz2\", compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews. We will split this set into training and testing sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 57576\n",
    "\n",
    "train_data, test_data = train_test_split(data,train_size=250000, test_size=50000, stratify=data['rating'], random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that indeed we have equal number of documents for each rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert each document into vector as described in the introduction.  This is actually a non trivial task. It requires first spliting text into _tokens_ (e.g. words). Not all letter combinations are meaningfull and we have decide which tokens are valid. Also it may be advantageous to  do _steming_: reduce the word to its _root_ word e.g. 'flying'->'fly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library contains a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class that implements a simple tokenizer and occurence counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consult the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  for the full description of the available arguments. I used only one: `stop_words = 'english'` which enables use of the internal list of tokens to reject (stop words) (but [see](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `fit_transform` takes the dataset and prepares the vocabulary and then encodes the dataset. It's a  composition of `fit` and `transform` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "review_features = vectorizer.fit_transform(train_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the resulting matrix is huge!! And it's a  only a small version of data... But even this \"small\" version would take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(review_features.shape)/2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gigabytes of data assuming only one byte of data per entry. However each document conatins only a very small subset of total vocabulary. So each row of this matrix contains mostly zeros e.g. in first five rows it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(review_features[:5,:].toarray(), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "row length. That's why it is stored in the compressed matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(review_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we had to  use `toarray` method above to convert it to normal numpy array. Be carefull not to call this methods on the whole matrix!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nnz` attribute stores the number of actuall entries in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_features.nnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting in only  small proportion of all the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_features.nnz/np.prod(review_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer object stores the `vocabulary_` dictionary that maps words to   vector indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_i = vectorizer.vocabulary_['price']\n",
    "print(price_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and word list that does the oposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[price_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the invaluable scikit-learn library which implements the Multinomial classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rating_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rating_classifier.fit(review_features, train_data.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_features = vectorizer.transform(test_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rating_classifier.score(test_features, test_data.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score` method implements the acccuracy metric which  gives the precentage of correct classfications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predicted =  base_rating_classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_data.rating, base_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy does not look very good but keep in mind that  this is not binary classification.  We have five  ratings so random guessing would give us accuracy around 20%.  All the scores we have introduced in the 'categorical' notebook give similar (or identical) results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_data.rating, base_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(test_data.rating, base_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_data.rating, base_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy counts only the ratings we got exactly right. However we are not dealing with real categorical (nominal) data. Our labels are _ordinals_ meaning that there is an order to ratings  and clearly predicting one instead of five is a bigger error then predicting four. In practice we may tolerate  _e.g._  to be one rating off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look more closely how close we are with our classification.\n",
    "The function below calculates the histogram of differences between  predictions and true ratings and plots it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(true_ratings, predictated_ratings):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(18,5))\n",
    "    diffs = predictated_ratings - true_ratings\n",
    "    ax[0].hist(diffs, bins=9, range=(-4.5,4.5), density=True);\n",
    "    ax[0].set_xlabel(\"$\\delta$\")\n",
    "    ax[0].set_ylabel(\"$P(\\Delta r = \\delta)$\")\n",
    "    ax[1].hist(np.abs(diffs), bins=5, range=(-0.5,4.5), density=True);\n",
    "    ax[1].set_xlabel(\"$\\delta$\")\n",
    "    ax[1].set_ylabel(\"$P(|\\Delta r| = \\delta)$\")\n",
    "\n",
    "    ax[2].hist(np.abs(diffs), bins=5, range=(-0.5,4.5), density=True, cumulative=True, histtype='step');\n",
    "    ax[2].axhline(0.80, linewidth=0.75, color='grey');\n",
    "    ax[2].set_xlabel(\"$\\delta$\")\n",
    "    ax[2].set_ylabel(\"$P(\\Delta r \\leq \\delta)$\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(test_data.rating, base_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that im 80% of cases we are just one rating off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most extreme ratings are most often  predicted correctly, which maybe is not that suprising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get  similar results ) with `confusion_matrix` functions from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please pay attention to `normalize` parameter. This displays  the relative frequencies of all possible outcomes combinations:   rows correspond to true labels, columns to predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_freq = confusion_matrix(test_data.rating, base_predicted, normalize='all')\n",
    "outcome_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ads up to one as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_freq.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the counts of each outcomes by not requesting normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_data.rating, base_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides also function for plotting  confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "disp = plot_confusion_matrix(base_rating_classifier, test_features, test_data.rating, normalize='true', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The normalization set to 'true' normalizes each row separately. We can check this by accesing the computed confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.confusion_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to display an already computed confusion matrix we can use the `ConfusionMatrixDisplay` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm_display = ConfusionMatrixDisplay(outcome_freq, display_labels=range(1,6))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "cm_display.plot(ax=ax);\n",
    "ax.set_xlabel('predicted ratings')\n",
    "ax.set_ylabel('true ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most probable words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what are the most probable words in each category. The MultinomialNB makes the  logs of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log P(X_i=i|R=r)=\\log p^{(r)}_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "available as `feature_log_prob_` attribute. We can extract the index of highest number in an array using numpy function `argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_probable = np.argmax(base_rating_classifier.feature_log_prob_,axis=1)\n",
    "most_probable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the indices to word in the vocabulary we will use an auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word(i):\n",
    "    return vectorizer.get_feature_names()[i]\n",
    "\n",
    "word = np.vectorize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy `vectorize` function takes a Python function and makes it _threadable_ over numpy arrays, but please note that this may be quite inneficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word(most_probable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at more words using the `argsort` function which returns the indices of the array in the sorted by (ascending) values order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(0,1,6)\n",
    "print(a)\n",
    "ia = np.argsort(a)\n",
    "print(ia)\n",
    "print(a[ia])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take $n$ most probable words  for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 \n",
    "most_probable = np.argsort(base_rating_classifier.feature_log_prob_,axis=1)[:,-1:-n-1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`argsort` sorts in ascending order. To list last $n$ elements of an array  in reverse order we use  slice with negative stride. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[-1:-n-1:-1]` means: start from last element (first from the end) and  go down  to  but not including n+1 element from the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word(most_probable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas dataframe to \"pretty print\" the  resulting array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(words, index =[1,2,3,4,5], columns=range(1,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While developing the text classifier above we had to set  a number of parameters. The `MultinomialNB` has only  one: `alpha`, but   vectorizer has several. Those parameters are called _hyperparameters_ to distinguish them from the parameters like $p^{(r)}_i$  that we are training. It is not clear a priori  what values of those hyperparameters will give us the best results. Unfortunatelly here we  are moving away from nice science of probability calculus  and enter the realm  of black magic of hyperparameters tuning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious and conceptually easiest strategy is the search of the space of possible parameters. In practice this is of course not possible, but we can at least scan a part of the parameters space and get some feeling about performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin we must however think a little bit about how are we going to measure the performance of  the classifier. I have already stated that we must not use the training set for evaluation. What about the testing set then? While not so obvious, when we are tuning the hyperparameters we are also in some sense training our model. So if we use the test set to choose best hyperparameters we cannot use this set for overall final evaluation. The solution is to  set aside yet another set called the _validation_ dataset and use this set for tuning hyperparameters. For more explanation see [this article](https://machinelearningmastery.com/difference-test-validation-datasets/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly  we usually have a finite amount of data and leaving aside test and validatation sets leaves less data for training. One of the strategies used to mitigate that is _k-fold cross validation_. This works like that: We divide our data in $k$ equal parts called folds.  Then we train our model $k$ times each time using different fold for testing and $k-1$ remaining folds for training. That way we get $k$ evaluations results that we can e.g. average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunatelly scikit-learn  provides several functions to help us with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [0,1,2,3,4,5,10,20,30,50]:\n",
    "    scores = cross_val_score(MultinomialNB(alpha=a), review_features, train_data.rating,cv = 5, n_jobs=-1 )\n",
    "    print(\"{:4.1f} {:6.4f}\".format(a, scores.mean()), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_val_score` function can run its  training and scoring tasks concurrently. This is enabled by the `n_jobs` parameter  which indicated the number of concurrent processes we want to use. Be default this is `None` which is equvalent to one (no concurrency). Setting   it to -1 results in  automatic determination of number of processes to run and on computers with hyperthreading this will \n",
    "be twice the number of cores. For such small jobs as above this does not matter. But for larger jobs you should try different values of this parameter. Not always more processes is the better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By deafult `cross_val_score` uses `score` function of the estimator which in this case was the accuracy score. We may change this by providing  `scoring` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [0,1,2,3,4,5,10,20,30,50]:\n",
    "    scores = cross_val_score(MultinomialNB(alpha=a), review_features, train_data.rating,cv = 5, n_jobs=-1, scoring='f1_macro' )\n",
    "    print(\"alpha = {:4.1f} {:6.4f}\".format(a, scores.mean()), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `cross_validate` does the same but allows for multiple metrics  and reports more information _e.g._ time used for fit and scoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results = cross_validate(MultinomialNB(alpha=20), review_features, train_data.rating,cv = 5, n_jobs=-1, scoring=['accuracy', 'f1_macro', 'precision_macro'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_validata` returns its results in  format suitable for directly using it in pandas  dataframe which resulst in nicer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of bigger number of different parameters writing our explicit search loops like above can be tedious. We can use the `GridSearchCV` class to combine search with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0,1,2,3,4,5,10,20,30,40,50,60]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(MultinomialNB(), param_grid=param_grid, n_jobs=6, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search.fit(review_features, train_data.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search results are reported in `cv_results_` attribute which can be  used directly  to construct a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method by deafult selects the best set of paremters according to scoring metric and refits the estimator to all data using those parameters. The resulting object is stored in `best_estimator_` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = grid_search.best_estimator_\n",
    "best_predicted = best_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_data.rating, best_predicted, average='macro' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the \"base\" accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_best = accuracy_score(test_data.rating, best_predicted)\n",
    "acc_base = accuracy_score(test_data.rating, base_predicted)\n",
    "print(\"base {:.3f} best {:.3f} diff {:.3f} rel diff {:3.0f}%\".format(acc_base, acc_best, acc_best-acc_base,100*(acc_best-acc_base)/acc_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a very slight increase in accuracy.  This does not seem as a lot and for this particular example it isn't. But for some applications if accuracy translates  e.g. directly to our profit this gives us a 1% increase \"for free\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(data, label, estimator, metrics):\n",
    "        result={}\n",
    "        predicted = estimator.predict(data)\n",
    "        for name, func in metrics.items():\n",
    "            result[name]= func(label, predicted)\n",
    "        return result    \n",
    "        \n",
    "def evaluate_estimators_on_metrics(data, labels, estimators, metrics):\n",
    "    results = {}\n",
    "    \n",
    "    for name, est in estimators.items():\n",
    "        results[name] = evaluate_metrics(data, labels, est, metrics)\n",
    "        \n",
    "    return results     \n",
    "            \n",
    "def compare_to(evaluation,base):\n",
    "    result = {}\n",
    "    base_res = evaluation[base]\n",
    "    for m_name, m in base_res.items():\n",
    "            metric_column = result.setdefault('metric',[])\n",
    "            metric_column.append(m_name)\n",
    "            base_m_column = result.setdefault(base,[])\n",
    "            base_m_column.append(m)\n",
    "            for e_name, res in evaluation.items():\n",
    "                if e_name != base:\n",
    "                    m_column = result.setdefault(e_name,[])\n",
    "                    current = evaluation[e_name]\n",
    "                    m_value = current[m_name]\n",
    "                    m_column.append(m_value)\n",
    "                    column = result.setdefault(e_name+'_diff',[])\n",
    "                    column.append(m_value-m)\n",
    "                    column = result.setdefault(e_name+'_rel',[])\n",
    "                    column.append((m_value-m)/m)\n",
    "                    \n",
    "                    \n",
    "    return  result     \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = lambda x,y: f1_score(x,y, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    compare_to(\n",
    "        evaluate_estimators_on_metrics(test_features, test_data.rating, {'base': base_rating_classifier, 'best' :best_clf}, {'accuracy': accuracy_score, 'f1_macro': f1_macro}),\n",
    "        'base')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on  scikit-learn : [Working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to optimize also the parameters of the `CountVectoriser`, but `GridSearchCV`accepts a single estimator object as input. We could write a  class that combines together `CountVectoriser` and `MultinomialNB` but we don't have to :) Scikit-learn provides an easy way to do it using it's `Pipeline` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_clf_pipe =  Pipeline([('vect', CountVectorizer(stop_words='english', max_df=0.25, min_df=10, max_features=None)),\n",
    "                       ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a  `Pipeline` object that has all the methods of the estimator object. The pipeline can consist of several transformer objects and an estimator object at the end (see [documentation](https://scikit-learn.org/stable/developers/develop.html) for description of estimators and other interfaces). The resulting object accepts data in format suitable for the first transformer, passes it through all transformers and to the estimator at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can fit  the review data with one method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_clf_pipe.fit(train_data.review, train_data.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_data.rating,rating_clf_pipe.predict(test_data.review), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this calls now take more time as they include costly tokenizing and word counting.  So our grid search will take much more time. The parameters to the grid search are passed as a  dictionary or a list of dictionaries. The keys in dictionaries denote the names of the parameters. They have  the name of the pipeline stage they refer to prepended to them with two underscores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_smaller_grid  = {'clf__alpha' : [1,5,20,30,40,50], 'vect__max_features':[5000, 10000, None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_search = GridSearchCV(rating_clf_pipe, ratings_smaller_grid, n_jobs=6, cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be warned that the command below takes around six minutes on 6 cores i9 laptop :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline_search.fit(train_data.review, train_data.rating)\n",
    "pd.DataFrame(pipeline_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should of course also try to parametrize other parameters at the expense of more CPU time ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_bigger_grid  = {'clf__alpha' : [25,30,35,40], 'vect__max_features':[5000, 10000, 15000, None], 'vect__max_df': [0.25, 0.5], 'vect__min_df':[1,10,25]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_bigger_search = GridSearchCV(rating_clf_pipe, ratings_bigger_grid, n_jobs=6, cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be warned that if `RUN_GRID_SEARCH` is true, the command below takes over 30 minutes (!) on 6 cores i9 laptop :( I have included in the repository results for this run so you do not have to wait :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.base import clone\n",
    "\n",
    "if RUN_GRID_SEARCH:\n",
    "    pipeline_bigger_search.fit(train_data.review, train_data.rating)\n",
    "    bigger_search_results = pd.DataFrame(pipeline_bigger_search.cv_results_)  \n",
    "    best_estimator = pipeline_bigger_search.best_estimator_\n",
    "else:\n",
    "    bigger_search_results = pd.read_csv(\"grid_search.csv\")\n",
    "    params = eval(bigger_search_results.params.iloc[bigger_search_results.rank_test_score.argmax()])\n",
    "    best_estimator = clone(rating_clf_pipe)\n",
    "    best_estimator.set_params(**params)\n",
    "    best_estimator.fit(train_data.review, train_data.rating)\n",
    "    \n",
    "\n",
    "predicted = best_estimator.predict(test_data.review)\n",
    "bigger_search_results_sorted = bigger_search_results.sort_values(by='rank_test_score')\n",
    "\n",
    "print(\"f1 score = {:.4f} accuracy = {:.4f}\".format(f1_score(test_data.rating,predicted , average='macro'), accuracy_score(test_data.rating, predicted) ))\n",
    "bigger_search_results_sorted.head(4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please not that now we get much bigger optimal value for the parameter $\\alpha$. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}