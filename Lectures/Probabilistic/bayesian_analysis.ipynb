{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some latex definitions for later convenience. \n",
    "$$\\newcommand{\\lbl}[2]{{#1}^{(#2)}}$$\n",
    "$$\\newcommand{\\no}{n^{(1)}}$$\n",
    "$$\\newcommand{\\nz}{n^{(0)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (Naive) Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we have shown in the previous lectures training a Bayes classifier amounts to finding the conditional probability distributions of the features $\\mathbf{x}$ given the class label $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(\\mathbf{X}=\\mathbf{x}|C=c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In case of the Naive bayes classifier the features are conditionally independent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\mathbf{X}=\\mathbf{x}|C=c)=\\prod_{i}P(X_i=x_i|C=c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "so we can estimate   probality distribution  for each feature separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating parameters of the probability distribution  from samples drawn from that distribution is the domain of the statistics. We have already used the maximal likelihood estimator in case of the normal distribution while training the \"sex form height & bmi\"  classifier. In this notebook I will concentrate  on estimating parameters of discrete (categorical) features distribution.  I will use this oportunity to introduce  Bayesian data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "## Coin toss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with binary features. To be more specyfic you can imagine a series of $n$ coin tosses and treat head as success. The $\\no$ is the number of heads and $\\nz$ is the number of tails. Please take a coin and throw it now 10 times and count $\\no$ and $\\nz$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given those numbers how  would you estimate the probability of heads $p$ ? You will be probably tempted to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p = \\frac{\\no}{\\nz+\\no}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a bad choice but requires few comments. First let's recall how this results is obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know $P(\\nz, \\no|p)$ considered as a function of $\\nz$  and $\\no$ is a probability distribution also called a _sampling_ distribution. \n",
    "However when considered as a function of $p$ is it called _likelihood_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$L(p|n^0, n^1) = P(n^0,n^1|p) = \\binom{n^0+n^1}{n^1}p^{n^1}(1-p)^{n^0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Maximal likelihood__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose $p$ as the number that maximises this quantity. Actually when dealing with probabilities it's often more convenient to use logarithms. Logarithm is a monotonicaly increasing funcion, so the maximum of logarithm of an function will correspond to maximum of the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$\\log P(n^0,n^1|p) = \\log\\binom{n^0+n^1}{n^1}+\\log p^{n^1}+\\log (1-p)^{n^0} =  \\log\\binom{n^0+n^1}{n^1}+n^1\\log p+{n^0}\\log (1-p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating this expression with respect to $p$ we obtain equation for minimum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n^1}{p}-\\frac{n^0}{1-p}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading to the result stated above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach works, the estimator is _consistent_, that is as the number of tosses goes to infinity its value will converge to the true value  of $p$. However it has some problems.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the likelihood has no clear interpretation. This is NOT a probability distribution on $p$ ! Ideally we would like to have the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(p|n^1, n^0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be interpreted as probability of $p$ given the measured $\\nz$ amd $\\no$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly with maximal likelihood estimator there is no clear way of estimating the error on the $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thirdly we do not have a way of incorporating our prior knowledge. For example when tossing a coin we are practically sure that $p=1/2$. If we toss the coin and get  values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosses = np.asarray([0, 1, 1, 1, 0, 1, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will rather not conclude that $p=0.6$ as indicated by the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(tosses)\n",
    "n1 = tosses.sum()\n",
    "n0 = n-n1 \n",
    "p_est = n1/n\n",
    "print(p_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would need much more evidence  to overcome our _prior_ knowledge about a normal coin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually in case of 10 tosses we have only 25% chance of geting $p$ exactly right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(11)/10,st.binom(10,0.5).pmf(np.arange(11)), drawstyle = 'steps-mid' );\n",
    "plt.xticks(np.arange(11)/10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In extreme case we can get $n^{(1)}=0$ and conclude that succes is impossible, skewing the classifier. This seems unlikely but consider that if all of the 1000 students  at our faculty throw the coin 10 times. Probability that at least one of them tosses ten  tails in a row is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 1 - (1-p^{10})^{1000} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(1-2**(-10.0))**1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so not that small. In practical applications for some features $p$ can be very small and the probability of zero successes in our sample is big. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those issues are addressed by the Bayesian statistics. In this approach we concentrate on finding the posterior probability distribution of $p$ given the observed outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem gives us a more systematic way of estimating $p$. Recall that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(p|n^1,n^0) =  \\frac{P(n^1,n^0|p) P(p)}{P(n^1,n^0)} = \\frac{P(n^1,n^0|p) P(p)}{\\int_0^1\\text{d}p P(n^1,n^0|p) P(p)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called _posterior_ probability refering to the fact that it is calculated after we have collected the data. It represents our knowledge of $p$ after we have collected some evidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first factor in the numerator is the likelihood/sampling distribution. The second factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is more problematic. This is  _a priori_ probability of the probability of succes $p$ usually caled \"prior\". What does that mean ? According to Bayesians this is our degree of belief about $p$ before we see any data. I agree that this is vague, but that's the best we can do. This maybe very subjective but at least that subjectivity is stated openly. Anyway when we gather enough data the prior will not matter. If we do not have data we have to guess anyway :) The Bayes theorem gives us a way of changing our beliefs (probability distribution) according to the data collected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the prior  maybe well defined. In principle we could take thousands  of coins and toss them milions of times measuring $p$. Then we could get the distribution of $p$ due to manufacturing defects. The same could be said for any other object that in principle can be produced in many copies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So imagine that we have coins produced by some sloppy blacksmith. We have one of such coins and we want to estimate $p$. We have previously examined many such coins so we have some idea how the  $P(p)$ distribution looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only paractical way of specfying a continous probability distribution is by giving a formula for the probability density function, usually parametrised by few parameters. In case of binary features the most convenient distribution is\n",
    "he  [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution). It  has two parameters $\\alpha$ and $\\beta$  and its probability density function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(x|\\alpha,\\beta) =  \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n",
    "x^{\\alpha-1}(1-x)^{\\beta-1},\\quad 0\\leq x\\leq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are plots of the probability density function for some values of $\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs =np.linspace(0,1,250)\n",
    "for a in [0.25,0.5,1,2,5,10]:\n",
    "    ys = st.beta(a,a).pdf(xs)\n",
    "    plt.plot(xs,ys, label='%4.2f' %(a,))\n",
    "plt.legend(loc='best', title='$\\\\alpha=\\\\beta$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here for some values of $\\alpha\\neq\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs =np.linspace(0,1,250)\n",
    "for a in [0.25,0.5,1,5]:\n",
    "    ys = st.beta(a,2.0).pdf(xs)\n",
    "    plt.plot(xs,ys, label='%4.2f' %(a,))\n",
    "plt.legend(loc=1, title='$\\\\alpha$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be more convenient to parametrise  Beta distribution by its mean and variance. The mean and variance of Beta distribution are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\quad\\text{and}\\quad \\sigma^2=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha = \\mu \\left(\\frac{\\mu(1-\\mu)}{\\sigma^2}-1\\right)\\quad\\text{and}\\quad\\beta = (1-\\mu) \\left(\\frac{\\mu(1-\\mu)}{\\sigma^2}-1\\right),\\quad \\sigma^2<\\mu(1-\\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "function"
    ]
   },
   "outputs": [],
   "source": [
    "def beta_mu_var(mu, s2):\n",
    "    \"\"\"Returns Beta distribution object (from scipy.stats) with specified mean and variance\"\"\"\n",
    "    \n",
    "    nu = mu*(1-mu)/s2 -1\n",
    "    if nu>0:\n",
    "        alpha = mu*nu\n",
    "        beta = (1-mu)*nu\n",
    "        return st.beta(a=alpha,b=beta)\n",
    "    else:\n",
    "        print(\"s2 must be less then {:6.4f}\".format(mu*(1-mu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = beta_mu_var(0.3, 0.1)\n",
    "print(bd.mean(), bd.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to coin toss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's assume that the $p$ values of coins produced by our sloppy blacksmith have Beta distribution  with mean $\\mu=0.45$ and standard deviation $\\sigma=0.1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = beta_mu_var(0.45, 0.1*0.1)\n",
    "pars = prior.kwds\n",
    "alpha =pars['a']\n",
    "beta = pars['b']\n",
    "print(\"alpha = {:.2f}, beta={:.2f}\".format(alpha,beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare this to uniform prior with $\\alpha=\\beta=1$. This gives a constant probability density function $P(p)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_prior = st.beta(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs =np.linspace(0,1,2000)\n",
    "plt.plot(xs, prior.pdf(xs),    label=\"$\\\\alpha = {:5.2f}$ $\\\\beta = {:5.2f}$\".format(alpha, beta));\n",
    "plt.plot(xs,uni_prior.pdf(xs), label=\"$\\\\alpha = {:5.2f}$ $\\\\beta = {:5.2f}$\".format(1, 1));\n",
    "plt.xlabel('p');\n",
    "plt.ylabel('P(p)');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "### Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(p|\\nz,\\no)=\\frac{P(\\nz,\\no|p)P(p)}{P(\\nz,\\no)},\\qquad P(\\nz,\\no) = \\int\\text{d}p\\,P(\\nz,\\no|p)P(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we chose $Beta(\\alpha, \\beta)$ as the prior $P(p)$ the  numerator of the posterior distribution has the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\binom{\\nz+\\no}{\\no}p^{\\no}(1-p)^{\\nz}  \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n",
    "p^{\\alpha-1}(1-p)^{\\beta-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator is an integral of the numerator but we don't have to do it explicitely. By noting that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p^{\\no}(1-p)^{\\nz} p^{\\alpha-1}(1-p)^{\\beta-1}=\n",
    "p^{\\no+\\alpha-1}(1-p)^{\\nz+\\beta-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the functional dependence on $p$ is same as for $Beta(n^1+\\alpha, n^0+\\beta)$. So this is the same distribution and  finally we have the formula for the posterior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(p| \\nz, \\no) \\sim  Beta(\\no+\\alpha, \\nz+\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prior with a property that the posterior distribution has same form as the prior is called _conjugate_ prior to the sampling distribution. So the Beta distribution is a conjugate prior to Bernouilli distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So continuing our example let's suppose that real $p$ of our coin is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "p_coin = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = st.bernoulli(p=p_coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will \"toss\" it 10000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_tosses = 10000\n",
    "tosses = coin.rvs(n_tosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "function"
    ]
   },
   "outputs": [],
   "source": [
    "def ht(tosses):\n",
    "    \"\"\"Takes a list of toss results and returns number of successes and failures\"\"\"\n",
    "    h = tosses.sum()\n",
    "    t = len(tosses)-h\n",
    "    return (h,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht(tosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "function"
    ]
   },
   "outputs": [],
   "source": [
    "def ab_string(a,b):\n",
    "    return \"$\\\\alpha = {:.2f}$ $\\\\beta = {:.2f}$\".format(a,b)\n",
    "\n",
    "def draw_beta_prior(a,b,**kargs):\n",
    "    xs=np.linspace(0,1,1000)\n",
    "    plt.plot(xs, st.beta(a,b).pdf(\n",
    "        xs), **kargs)\n",
    "\n",
    "def draw_beta_posterior(a,b,tosses, **kargs):\n",
    "    \"\"\"Draw posterior distribution after  seing tosses assuming Beta(a,b) prior\"\"\"\n",
    "    (h,t)=ht(tosses)\n",
    "    xs=np.linspace(0,1,1000)\n",
    "    plt.plot(xs, st.beta(a+h,b+t).pdf(\n",
    "        xs), **kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the posterior after 10 tosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_beta_posterior(alpha,beta,tosses[:10], label=ab_string(alpha, beta))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss again what does this probability distribution mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can thing about it as an outcome of following experiment:\n",
    " 1. You draw a value for $p$  from the prior distribution\n",
    " 1. You draw 10 ten times from the Bernoulli distribution with $p$ selected above,  which is equivalent to drawing from Binomial distribution with same $p$  and $n=10$.\n",
    " 1. You  repeat the two points above noting each time  $p$ and number of successes.\n",
    " 1. From the results you select only those where number of successes was equal to `tosses[:10].sum()`\n",
    " 1. The distributiion of $p$ in this selected results should match our posterior!\n",
    " \n",
    " Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(prior,n,size):\n",
    "    p = prior.rvs(size=size)\n",
    "    return np.stack((p,st.binom(n=n, p=p).rvs()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = experiment(st.beta(alpha, beta),10,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res[res[:,1]==tosses[:10].sum()][:,0], bins=50, density=True, label=ab_string(alpha, beta));\n",
    "draw_beta_posterior(alpha,beta,tosses[:10], label=ab_string(alpha, beta))\n",
    "draw_beta_posterior(3,3,tosses[:10], label=ab_string(3, 3))\n",
    "draw_beta_posterior(1,1,tosses[:10], label=ab_string(1, 1))\n",
    "plt.legend(title = 'Priors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we indeed get the predicted posterior distribution. Unfortunatelly this requires us to get our prior right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more data the dependence of the posterior on the prior diminishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_100 = experiment(st.beta(alpha, beta),100,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res_100[res_100[:,1]==tosses[:100].sum()][:,0], bins=50, density=True);\n",
    "draw_beta_posterior(alpha,beta,tosses[:100], label=ab_string(alpha, beta))\n",
    "draw_beta_posterior(3,3,tosses[:100], label=ab_string(1,1), c='red')\n",
    "draw_beta_posterior(1,1,tosses[:100], label=ab_string(1,1))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's see how the posterior distribution evolves with increasing number of tosses. Below we draw posterior distribution after different number of tosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for n in [0, 1,2,3,4,5,10,20,50,100]:\n",
    "    draw_beta_posterior(alpha,beta,tosses[:n], label=\"{:d}\".format(n))\n",
    "plt.legend();\n",
    "plt.axvline(p_coin);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below after some more tosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim(0.2,0.5)\n",
    "for n in [100,200,500,1000,5000]:\n",
    "    draw_beta_posterior(alpha,beta,tosses[:n], label=\"{:d}\".format(n))\n",
    "plt.legend();\n",
    "plt.axvline(p_coin);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare  how the estimated value converges to the real one for different priors. We will use the maximal a posteriori estimate of $p$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "#### MAP (Maximal a posteriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because mode of the Beta distribution is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$\\frac{\\alpha-1}{\\alpha+\\beta-2},\\qquad \\alpha, \\beta>1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mode of posterior is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$p_{MAP}=\\frac{\\alpha-1+\\no}{\\alpha-1+\\no+\\beta-1+\\nz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So adding a Beta prior amounts to adding $\\alpha-1$  and $\\beta-1$  repectively to  $\\no$ and $\\nz$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs  = np.cumsum(tosses)\n",
    "ns  = np.arange(1.0, len(cs)+1)\n",
    "avs = cs/ns\n",
    "post_avs = (cs + alpha-1)/(ns+alpha+beta -2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 500\n",
    "plt.plot(ns[:l],avs[:l],'.', label='uniform prior');\n",
    "plt.plot(ns[:l],post_avs[:l],'.', label='prior');\n",
    "plt.axhline(p_coin, linewidth=1, c='grey')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after few tens/houndreds of tosses both estimate behave in the same way, but with informative prior  we get better results for small number of tosses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "### Posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask what is the probability of the coin comming head up after seing it come head up $\\no$ times in $n$ trials? The answer is the integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(X=1|\\no,\\nz)=\\int\\limits_0^1\\text{d}p \\,P(X=1|p) P(p|\\no,\\nz) = \\int\\limits_0^1\\text{d}p\\, p\\, P(p|\\no,\\nz)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is an expectation value (mean) of the posterior  distribution leading to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(X=1|\\no,\\nz)=\\frac{\\alpha+\\no}{\\alpha+\\no+\\beta+\\nz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With uniform prior we obtain the so called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide",
     "slideshow": {
      "slide_type": "slide"
     }
    }
   },
   "source": [
    "#### Laplace Rule of succession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of succes after seing $\\no$ successes   and $\\nz$ failures  is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "fragment"
     }
    }
   },
   "source": [
    "$$P(succes) = \\frac{\\no+1}{\\no+\\nz +2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also known as  _Laplace smoothing_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__  Amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment",
     "slideshow": {
      "slide_type": "-"
     }
    },
    "tags": [
     "problem"
    ]
   },
   "source": [
    "You can buy same item from two  sellers one with 90 positive  and 10 negative reviews and another with 6 positive  and no negative reviews.\n",
    "From which you should buy ? What assumption you had to make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Let's assume that for each seller sale is  an independent Bernoulli trial with success denoting no problems for the buyer. The other assuption that we are going to make is that all buyers write the reviews.  If so then by the rule of succetion probability of success for the first buyer on the next deal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "(90+1)/(100+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "and for the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "(6+1)/(6+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "We should buy from the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural generalisation of the Bernoulli distribution is the multinouilli or categorical distribution and the generilsation of the binomial distribution is the _multinomial_ distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have $m$ categories with probability $p_k$ for each category. Then after $n$ trials the probability that we $n_k$ results in category $k$ is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(n_1,\\ldots, n_{m}|p_1,\\ldots, p_{m}) = \\frac{N!}{n_1!\\cdots n_{m}!}p_1^{n_1}\\cdots p_{m}^{n_{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjugate prior  to this distribution is the Dirichlet distribution which is a generalisation of the Beta distribution. It has $m$ parameters $\\alpha_k$ and its probability mass function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(p_1,\\ldots,p_{m}|\\alpha_1,\\ldots,\\alpha_{m}) = \\frac{\\Gamma\\left(\\sum\\limits_{i=1}^{m} \\alpha_i\\right)}{\\prod\\limits_{i=1}^{m}\\Gamma(\\alpha_i)}\n",
    "\\prod\\limits_{i=1}^{m}p_i^{\\alpha_i-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is easy to check that the posterior on $p_k$ density is given by the  Dirichet distribution with paramerters $\\alpha_1+n_1,\\ldots, \\alpha_{m}+n_{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The maximal a posteriori estimate is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_k = \\frac{n_k+\\alpha_k-1}{n + \\sum_i \\alpha_k-m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and Laplace smoothing takes the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_k = \\frac{n_k+1}{\\sum_{k=1}^m n_k  + m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was taken from Kaggle [Mobile games: A/B Testing](https://www.kaggle.com/yufengsui/mobile-games-ab-testing). It concerns an A/B test with a mobile game [Cookie Cats](https://tactilegames.com/cookie-cats/). The players were given two versions of the game differing by the level were the first gate that forced them to wait was placed. Measured was the retention on day one and  day seven as well as total number of game rounds played  during firts 14 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data=pd.read_csv(\"../../Data/AB/cookie_cats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This descriptions shows a suspiciously large maximum number of  rounds played. Let's look more closely at the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.sum_gamerounds.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram indicated that this is  a outlier, probably an error. A look at ten biggest values confirms that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.sum_gamerounds.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will eliminate this row from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imax = ab_data.sum_gamerounds.argmax()\n",
    "ab_data = ab_data.drop(imax, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data.sum_gamerounds.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this preliminary cleaning of data we have to split it into two groups according to game version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_version = ab_data.groupby('version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = by_version.get_group('gate_30')\n",
    "B = by_version.get_group('gate_40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(series):\n",
    "    n  = series.count()\n",
    "    n1 = series.sum()\n",
    "    return (n1, n-n1)\n",
    "\n",
    "def posterior(n1, n0, a=1,b=1):\n",
    "    return st.beta(a = n1+a, b = n0+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the posterior distribution of of retention rates assuming uniform prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_A = get_stats(A.retention_1)\n",
    "r1_A_post = posterior(*r1_A)\n",
    "r1_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_B = get_stats(B.retention_1)\n",
    "r1_B_post = posterior(*r1_B)\n",
    "r1_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot those  distributions, first in the whole domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=np.linspace(0,1,500)\n",
    "plt.plot(ps, r1_A_post.pdf(ps), label='gate_30');\n",
    "plt.plot(ps, r1_B_post.pdf(ps), label='gate_40');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then  once we know where to look, in more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=np.linspace(0.4,.5,500)\n",
    "plt.plot(ps, r1_A_post.pdf(ps),label  = \"gate_30\");\n",
    "plt.plot(ps, r1_B_post.pdf(ps), label = \"gate_40\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that increasing the first gate level decreased the retention rate on average by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_B_post.mean() - r1_A_post.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a one percent effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(r1_B_post.mean() - r1_A_post.mean())/r1_A_post.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea if effect of this size is relevant for business, but we can ask is it statistically relevant? Because we have the posterior distributions of $r_A$ and $r_B$, one way of answering that question is to sample from distributions of differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_sample_A = r1_A_post.rvs(size=100000)\n",
    "r1_sample_B = r1_B_post.rvs(size=100000)\n",
    "r1_diff = r1_sample_B-r1_sample_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r1_diff, bins=100, density=True, histtype='step');\n",
    "plt.axvline(r1_diff.mean())\n",
    "plt.axvline(0, c='grey');\n",
    "plt.xlabel('$\\Delta r_1$')\n",
    "plt.ylabel('P(\\Delta r_1)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram strongly suggest that  observed difference is a real effect. We can  formulate this more formally by using  highest density regions (HDR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest density region (HDR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta$ HDR is a region where at least $\\beta$ of probability is concentrated and has smallest possible volume in the sample space, hence highest density. More formal definition given below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $P_X(p)$ be de density function of  some random variable $X$ with values in $R_X$. Let' $R_X(p)$ be the subsets of $R_X$ such  that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ R(p) = \\{x\\in R_X: P_X(x)\\ge p\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\beta$ HDR is equal to $R(p_\\beta)$ where $p_\\beta$ is the largest constant such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\left(X\\in R(p_\\beta)\\right)\\ge \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this definition functions below find and plot such region for given histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges(prob,f):\n",
    "    \n",
    "    pr = np.zeros(len(prob)+2)\n",
    "    pr[1:-1]=prob\n",
    "    in_r = pr>f\n",
    "    return np.logical_xor(in_r[1:] , in_r[:-1])\n",
    "    \n",
    "\n",
    "def hdr(prob, edges, beta, tol=0.00001):\n",
    "    widths = edges[1:]-edges[:-1]\n",
    "    \n",
    "    def area_under(f):\n",
    "        return np.sum(widths*prob*(prob>f))\n",
    "    \n",
    "    fb = prob.max()\n",
    "    mb = area_under(fb)\n",
    "    fa = 0\n",
    "    ma = area_under(fa)\n",
    " \n",
    "    \n",
    "    while (fb-fa) > tol:\n",
    "       \n",
    "        fc = (fb+fa)/2\n",
    "        mc = area_under(fc)\n",
    "       \n",
    "        if mc > beta:\n",
    "            fa = fc\n",
    "            ma = mc\n",
    "        else:\n",
    "            fb = fc\n",
    "            mb = mc\n",
    "     \n",
    "    fmax = (fb+fa)/2\n",
    "    \n",
    "    e = find_edges(prob, fmax)\n",
    "    return (edges[e], fmax,np.argwhere(e).flatten())\n",
    "\n",
    "def plot_hdr(prob, edges, ie, **kwargs):\n",
    "    eprobs = np.append(prob, 0.0)\n",
    "    for i in range(0,len(ie),2):\n",
    "        plt.fill_between(edges[ie[i]:ie[i+1]+1], eprobs[ie[i]:ie[i+1]+1], \n",
    "                     step = 'post', **kwargs);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "prob, bin_edges,_ = plt.hist(r1_diff, bins=100, density=True, histtype='step')\n",
    "R_intervals, fmax,  i_R_intervals = hdr(prob, bin_edges, beta)\n",
    "plot_hdr(prob, bin_edges, i_R_intervals, alpha=0.2, color='blue', label = \"{:3.2f} HDR\".format(beta))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that zero difference lies outside the 0.9 HDR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belowe we repeat this analysis foe day seven retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r7_A = get_stats(A.retention_7)\n",
    "r7_A_post = posterior(*r7_A)\n",
    "r7_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r7_B = get_stats(B.retention_7)\n",
    "r7_B_post = posterior(*r7_B)\n",
    "r7_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=np.linspace(0,1,500)\n",
    "plt.plot(ps, r7_A_post.pdf(ps), label='gate_30');\n",
    "plt.plot(ps, r7_B_post.pdf(ps), label='gate_40');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=np.linspace(0.1,.3,500)\n",
    "plt.plot(ps, r7_A_post.pdf(ps),label  = \"gate_30\");\n",
    "plt.plot(ps, r7_B_post.pdf(ps), label = \"gate_40\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r7_sample_A = r7_A_post.rvs(size=100000)\n",
    "r7_sample_B = r7_B_post.rvs(size=100000)\n",
    "r7_diff = r7_sample_B-r7_sample_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r7_diff, bins=100, density=True, histtype='step');\n",
    "plt.axvline(r7_diff.mean())\n",
    "plt.axvline(0, c='grey');\n",
    "plt.xlabel('$\\Delta r_1$')\n",
    "plt.ylabel('P(\\Delta r_1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, bin_edges,_ = plt.hist(r7_diff, bins=100, density=True, histtype='step')\n",
    "R_intervals, fmax,  i_R_intervals = hdr(prob, bin_edges, 0.9)\n",
    "plot_hdr(prob, bin_edges, i_R_intervals, alpha =0.2 , color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte-Carlo technique is every powerfull and allows for more elaborate analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__  Difference between the number of persons playing the game on day one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Assume that each day 10000 persons on average downloads and installs the game. The distribution of this number is Poisson distribution. Plot the distribution of the number of people that will play the game on day one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "n_samples = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "First we create the Poisson distribution object and use it to generate a sample of number of downloads per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "n_average = 10000\n",
    "n_dist = st.poisson(mu = n_average)\n",
    "n_in_day = n_dist.rvs(size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Then we simulate same number od retention rates samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "r1_As = r1_A_post.rvs(size=n_samples)\n",
    "r1_Bs = r1_B_post.rvs(size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "We combine  for each sample simulating the binomial distribution with given number of downloads and  given retention rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "n_ret1_A = st.binom.rvs(n=n_in_day, p=r1_As, size=n_samples)\n",
    "n_ret1_B = st.binom.rvs(n=n_in_day, p=r1_Bs, size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "And finally we plot the histogram of differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(n_ret1_B-n_ret1_A, bins=np.arange(-500,300,10), density=True, histtype='step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}