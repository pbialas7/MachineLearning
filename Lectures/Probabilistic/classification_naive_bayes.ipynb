{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-features classification and  Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will continue to work with the gender classifier based on height. We will use some real(?) data. We will also add weight as additional  feature. This will introduce us to Naive bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Height & weight data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data I could find was a [kaggle dataset](https://www.kaggle.com/mustafaali96/weight-height). This data set does not contain any  description of origin but use of inches and pounds suggests an american or english population. The data consists of 10000 points each specifying sex, height and weight of one person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in popular \"coma separated values\" (CSV) format. We will use  [pandas library](https://pandas.pydata.org) to read it. Pandas is a very popular library for working with dataframes and series. It would be really worth your time to become acquinted with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../Data/HeightWeight/weight-height.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` is a [Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a kind of \"table on steroids\". We can look at first _n_ rows using `head` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives a more detailed summary of the numerical data contained in the dataframe. Looking at the numbers we can determine that the units are not metric. A good guess would be inches and pounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inch = 0.01 * 2.54 # m\n",
    "pound = 0.453 # kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Height'] = data['Height'] *inch \n",
    "data['Weight'] = data['Weight'] *pound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I have used `data.Height` notation to access the 'Height' column of the data frame. This is \"syntactic sugar\" and is equivalent to `data['Height']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Height'] is data.Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the numbers look realistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the file contains equal number of women and men data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.Gender=='Female').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed any further we will \"do the right thing\" and split the data into training and testing set using a function from another very useful library `scikit-learn`. By now you should know why this is  neccesary but let me reiterate this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning our goal __is not to obtain good classfication on the  labeled data__ we have! We want it to perform well on the  new data that we do not know the labels of. Of course if it cannot classify the training data correctly it will not classify other data correctly as well. But the oposite statement is not true! In principle the classifier could just learn the training examples \"by heart\" and get a perfect score.  That would be an extreme example of _overfitting_. Overfitting as the name implies means that the classifier has learned to exploite some peculiarities of the training data set and does not _generalise_ well to other data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus it's the performance on unseen data i.e. generalisation that is our real goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why we must __always__ keep a portion of our dataset for testings purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by fixing the seed we guarantee that we can split the set in same way each time\n",
    "#this maybe handy for debuging purposes\n",
    "seed = 77678 \n",
    "train_data, test_data  = train_test_split(data,test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks like. To visualise a  distribution we will use a _histogram_ of the values. Pandas have a handy built in method that calculates and plots histogram, althought it just delegates the job to numpy and matplotlib. We will create the figure and matplotlib axes ourselfs for more control and pass axes to pandas hist function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"height (m)\")\n",
    "ax.set_ylabel(\"P(h)\")\n",
    "train_data.Height.hist(bins=32, density=True, ax=ax , label ='train', color='lightgrey');\n",
    "test_data.Height.hist(bins=32, density=True,  ax=ax, histtype='step', label ='test' , color='red');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up the description of parameters in [pandas.DataFrame.hist](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html) and [matplotlib.pyplot.hist](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.hist.html) documentation. The one  I have used indicate that the height values should be divided into 32 bins with range of the bins calculated automatically. The `density` parameter set to `True` makes the histogram normalised  so the area under it is  equal to one. This enables an easy comparison with probability density functions and to compare different histograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have superposed test data histogram to check if the distributions are similar. This is quite important we need the  training set to be representative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accesing one column  returns a [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data['Height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the histogram we see that the  distribution looks somewhat \"normal-like\" but it is rather flat at the top. But actually we do not expect it to be normal, as it contains both women and men data. Let's look at each gender separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_F = train_data[train_data.Gender=='Female']\n",
    "train_data_M = train_data[train_data.Gender=='Male']\n",
    "print(len(train_data_F), len(train_data_M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_F = test_data[test_data.Gender=='Female']\n",
    "test_data_M = test_data[test_data.Gender=='Male']\n",
    "print(len(test_data_F), len(test_data_M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting colors for consistency throughout the notebook you can subsitute here your favorite \"gender colors\" :) \n",
    "f_color = 'blue'\n",
    "m_color ='orange'\n",
    "color = 'lightgrey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"height (m)\")\n",
    "ax.set_ylabel(\"P(h|S)\")\n",
    "train_data_F.Height.hist(bins=32, density=True, ax=ax , color=f_color, alpha=0.7, label='Women');\n",
    "train_data_M.Height.hist(bins=32, density=True, ax=ax , color=m_color, alpha=0.7, label='Men');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a good idea to check again if the train and test data have roughly same distribution as we have alredy done for the joint data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"height (m)\")\n",
    "ax.set_ylabel(\"P(h|S)\")\n",
    "train_data_F.Height.hist(bins=32, density=True, \n",
    "                         ax=ax , color=f_color, alpha=0.7, histtype='step', label='Women train');\n",
    "train_data_M.Height.hist(bins=32, density=True, \n",
    "                         ax=ax , color=m_color, alpha=0.7, histtype='step', label=\"Men train\");\n",
    "test_data_F.Height.hist(bins=32, density=True, \n",
    "                         ax=ax , color=f_color, alpha=0.7, histtype='step', linestyle=\"--\", label='Women test');\n",
    "test_data_M.Height.hist(bins=32, density=True, \n",
    "                         ax=ax , color=m_color, alpha=0.7, histtype='step', linestyle=\"--\", label='Men test');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms look OK but check what happens if we pass the `shuffle=False` parameter to `train_test_split` function. Can you explain what is happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare those histograms with normal distribution. We will use the `fit` function of the `scipy.stats` normal distribution object. This function calculates the _maximal likelihood_ estimates of the distribution parameters:  mean and standard deviation in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_F, std_F) = st.norm.fit(train_data_F.Height)\n",
    "(mean_M, std_M) = st.norm.fit(train_data_M.Height)\n",
    "print(mean_F, std_F)\n",
    "print(mean_M, std_M) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see those number differ slightly from the ones we have used in the classification notebook so  they clearly correspond to some other population, but it does not matter for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `scipy.stats` distribution objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_F = st.norm(mean_F, std_F)\n",
    "pd_M = st.norm(mean_M, std_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use them to plot the probability distribution functions over the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"height (m)\")\n",
    "ax.set_ylabel(\"P(h|S)\")\n",
    "train_data_F.Height.hist(bins=32, density=True, ax=ax , color=f_color, alpha=0.7);\n",
    "train_data_M.Height.hist(bins=32, density=True, ax=ax , color=m_color, alpha=0.7);\n",
    "hs =np.linspace(1.3,2.1,100)\n",
    "ax.plot(hs, pd_F.pdf(hs), c=f_color);\n",
    "ax.plot(hs, pd_M.pdf(hs), c=m_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look reasonable and we will assume the normal distribution of heights for each sex throughout this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open up the black box of the `fit` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating parameters of the Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _maximal likelihood_ estimates returned by the `fit` function are the usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{y}\\equiv\\frac{1}{n}\\sum_{i=1}^n y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s^2\\equiv \\overline{y^2}-\\bar{y}^2\\qquad \\bar{y^2}\\equiv\\frac{1}{n}\\sum_{i=1}^n y^2_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caveat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance estimator $s^2$ is [_biased_](https://en.wikipedia.org/wiki/Bias_of_an_estimator) and very often its unbiased version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n}{n-1}s^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is used instead. Which version is calculated is controled by the parameter `ddof` of `std` function in numpy or pandas. But please be warned that in numpy the default value of this parameter is zero, correpsonding to $s$ while in pandas this is one and returns the unbiased version. Of course for large $n$ the difference is negligible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the basic facts from statistics and I assume you are acquainted with them :) But to refresh your memory I will present here the derivation of the maximal likelihood estimator for normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mean $\\mu$ and standard deviation $\\sigma$ the  probability of obtaining a sequence of $n$ random  values $y_i$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y_1,\\ldots,y_n|\\mu,\\sigma) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\displaystyle -\\frac{(y_i-\\mu)^2}{2\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping an unimportant constant factor this can rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma^{-n} e^{\\displaystyle -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum in the exponential can be expanded as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^n(y_i-\\mu)^2 \n",
    "= \\sum_{i=1}^n\\left( y_i^2-2y_i\\mu + \\mu^2\\right)\n",
    "=  \\sum_{i=1}^n y_i^2 - 2\\sum_{i=1}^n y_i\\mu +n \\mu^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is equal to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$n \\bar{y^2} - 2n\\bar{y} \\mu +n \\mu^2 = n \\left(\\bar{y^2} - 2\\bar{y} \\mu + \\mu^2\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be further rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{y^2} - 2\\bar{y} \\mu + \\mu^2 \n",
    "= \\bar{y^2} -\\bar{y}^2 +\\bar{y}^2 - 2\\bar{y} \\mu + \\mu^2\n",
    "= \\bar{y^2} -\\bar{y}^2 +\\left(\\bar{y}-\\mu\\right)^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y_1,\\ldots,y_n|\\mu,\\sigma)\\propto \\sigma^{n} e^{\\displaystyle -\\frac{n}{2\\sigma^2}\\left(\\bar{y}-\\mu\\right)^2-\\frac{n}{2\\sigma^2}s^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewed as function of $y_i$ this is a _sampling distribution_. But we can view this as a function of $\\mu$ and $\\sigma$. Then this is no longer a probability distribution and is called the _likelihood_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximal likelihood estimators are obtained by finding the values of $\\mu$ and $\\sigma$ that maximise the likelihood. We are looking for the parameters that make our data most probable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because logarithm is a monotonicaly increasing function we can look for the maximum of the logarithm of the likelihood which has a much simpler form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\frac{n}{2}\\log\\sigma^2-\\frac{n}{2\\sigma^2}\\left(\\bar{y}-\\mu\\right)^2-\\frac{n}{2\\sigma^2}s^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating with repect to $\\mu$ we obtain equation\n",
    ":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n}{2\\sigma^2}\\left(\\bar{y}-\\mu\\right)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with an obvious solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu=\\bar{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiatig with respect to $\\sigma^2$ gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\frac{n}{2\\sigma^2} +\\frac{n}{2\\sigma^4}(\\bar{y}-\\mu)^2+\\frac{n}{2\\sigma^4}s^2=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inserting the solution for $\\mu$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n}{2\\sigma^2} =\\frac{n}{2\\sigma^4}s^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma^2 = s^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier can be now constructed in the same way as in previous lecture. To make this notebook self contained I will repeat those calculations but wrap them up in couple of functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pdf_P_cond(labels, data):\n",
    "    \"\"\"Takes labels (0,1) and a single feature and returns the conditional \n",
    "    probability distribution function of the   positive label given the feature assuming\n",
    "    normal distribution of the  feature values.\n",
    "    \"\"\"\n",
    "    \n",
    "    positives = data[labels==1]\n",
    "    negatives = data[labels==0]\n",
    "    \n",
    "    pdf_cond_P = st.norm(*st.norm.fit(positives)).pdf\n",
    "    pdf_cond_N = st.norm(*st.norm.fit(negatives)).pdf\n",
    "    \n",
    "    P_P = labels.mean()\n",
    "    P_N = 1-P_P\n",
    "    \n",
    "    def pdf(x):\n",
    "        return pdf_cond_P(x)*P_P/(pdf_cond_P(x)*P_P+pdf_cond_N(x)*P_N)\n",
    "        \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_F_cond_h  = make_pdf_P_cond(train_data.Gender=='Female', train_data.Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hs =np.linspace(1.3, 2.1,100)\n",
    "ax.plot(hs, pdf_F_cond_h(hs));\n",
    "ax.set_xlabel(\"Height [m]\");\n",
    "ax.set_ylabel(\"P(F|h)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "threshold = fsolve(lambda h: pdf_F_cond_h(h)-0.5, 1.7)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix = namedtuple('ConfusionMatrix',['TP','FP', 'FN', 'TN'])\n",
    "\n",
    "def confusion_matrix(labels, probability, th=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the confusion matrix from true labels and predicted probabilities\n",
    "    \n",
    "    Parameters:\n",
    "    labels -- \n",
    "    \"\"\"\n",
    "    p = (labels == 1).sum()\n",
    "    n = (labels == 0).sum()\n",
    "    \n",
    "    pred = (probability >= th)\n",
    "    fp = (pred > labels).sum()\n",
    "    fn = (pred < labels).sum()\n",
    "    \n",
    "    return ConfusionMatrix(p-fn, fp, fn, n-fp)\n",
    "\n",
    "def rates(cm):\n",
    "    P = cm.TP + cm.FN\n",
    "    N = cm.FP + cm.TN\n",
    "    \n",
    "    tpr = cm.TP/P\n",
    "    fpr = cm.FP/N\n",
    "    fnr = cm.FN/P\n",
    "    tnr = cm.TN/N\n",
    "    \n",
    "    return tpr, fpr, fnr, tnr\n",
    "    \n",
    "def accuracy(cm):    \n",
    "    P = cm.TP + cm.FN\n",
    "    N = cm.FP + cm.TN\n",
    "    \n",
    "    return (cm.TP+cm.TN)/(P+N)\n",
    "\n",
    "def precission(cm):\n",
    "    return (cm.TP)/(cm.TP+cm.FP)\n",
    "    \n",
    "\n",
    "def describe(cm):\n",
    "    P = cm.TP + cm.FN\n",
    "    N = cm.FP + cm.TN\n",
    "    \n",
    "    tpr, fpr,fnr, tnr = rates(cm)\n",
    "    \n",
    "    print(\"Total = {:d} (P = {:d}, N = {:d})\".format(P+N, P,N))\n",
    "    print(\"TPR = {:5.3f} FPR = {:5.3f}\".format(tpr,fpr))\n",
    "    print(\"FNR = {:5.3f} TNR = {:5.3f}\".format(fnr,tnr))\n",
    "    \n",
    "    print(\"Accuracy  = {:5.3f}\".format(accuracy(cm)) )\n",
    "    print(\"Precision = {:5.3f}\".format(precission(cm)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(confusion_matrix(train_data.Gender=='Female', pdf_F_cond_h(train_data.Height)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(test_data.Gender=='Female', pdf_F_cond_h(test_data.Height))\n",
    "tpr, fpr,_,_=rates(cm_test)\n",
    "describe(cm_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs, tprs, thds = roc_curve(test_data.Gender=='Female', pdf_F_cond_h(test_data.Height))\n",
    "auc = roc_auc_score(test_data.Gender=='Female', pdf_F_cond_h(test_data.Height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(plt, title ='ROC', figsize=[8,8]):\n",
    "    \"\"\"\n",
    "    Returns pyplot figure and axes object designed for plotting ROC curve. \n",
    "    \n",
    "    It sets aspect ratio to one, labels the axes and sets the title.\n",
    "    \n",
    "    Parameters:\n",
    "    plt : pyplot module\n",
    "    title : the title of the plot\n",
    "    figsize : Figure size in inches \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_xlabel('FPR');\n",
    "    ax.set_ylabel('TPR');\n",
    "    ax.set_title(title)\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = roc_plot(plt, 'Test data ROC')\n",
    "roc = ax.plot(fprs,tprs, color='blue', linewidth=1);\n",
    "ax.scatter([fpr],[tpr],s = 30, edgecolor='blue', zorder=5, facecolor='blue');\n",
    "ax.text(0.7, 0.8, \"AUC = {:4.2f}\".format(auc), fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks slightly worse then the one  we did last time, but that's becase we have only 2500 data points instead of 100000 that we have generated by Monte-Carlo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish with some graphical iterpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"height (m)\")\n",
    "ax.set_ylabel(\"P(h|S)\")\n",
    "test_data_F.Height.hist(bins=32, density=True, ax=ax , color=f_color, alpha=0.7, label='Women');\n",
    "test_data_M.Height.hist(bins=32, density=True, ax=ax , color=m_color, alpha=0.7, label='Men');\n",
    "hs =np.linspace(1.3,2.1,100)\n",
    "ax.plot(hs, pd_F.pdf(hs), c=f_color);\n",
    "ax.plot(hs, pd_M.pdf(hs), c=m_color);\n",
    "plt.axvline(threshold, c='red', linewidth = 1);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the histograms of the test data with distributions derived from the traning data. The red vertical line marks the threshold for classfication. Blue on the left are true positives, orange on the left are false positives, blue on the right are false negatives and orange on right are true negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one feature is not very realistic. Normally we would use tens, hundreds or even thousands of different features. Each feature potentially adds information and increases the quality of the classifier. In our data set we have one more feature that we can use: weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising multidimensional data is hard. We can start by looking at them one by one. We have already looked at the distribution of heights so now we will plot the distribution of weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Weight [m]\")\n",
    "ax.set_ylabel(\"P(w|S)\")\n",
    "train_data_F.Weight.hist(bins=32, density=True, ax=ax , color=f_color, alpha=0.7, label='Women');\n",
    "train_data_M.Weight.hist(bins=32, density=True, ax=ax , color=m_color, alpha=0.7, label='Men');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of weights by gender looks reasonably normal, but what we need is the join distribution of height and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(h,w|S)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This we can visualize using a _scatter_ plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data_F.Height, train_data_F.Weight, alpha=0.2, c = f_color, label='Women');\n",
    "plt.scatter(train_data_M.Height, train_data_M.Weight, alpha=0.2, c = m_color, label='Men');\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Weight');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly this plot quickly saturates and it is hard to judge the density of points in different regions (you can try to play with the alpha parameter). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use 2D histograms, but then it is not possible to separate data for women and men on the same plot. Histogram below displays all the data together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb = plt.hist2d(train_data['Height'], train_data['Weight'], bins=32, density=True);\n",
    "plt.xlabel('Height [m]')\n",
    "plt.ylabel('Weight [kg]')\n",
    "plt.colorbar(hb[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can get a nicer picture with hexagonal bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb = plt.hexbin(train_data['Height'], train_data['Weight'], bins=20);\n",
    "plt.xlabel('Height [m]')\n",
    "plt.ylabel('Weight [kg]')\n",
    "plt.colorbar(hb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to proceed would be to try to model the joint distribution of  height and weight for each sex as a two dimensional normal distribution. Another, and that is the one we will pursue here is to model them independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (In)dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that we require the features to be at least approximately independent. That is obviously not the case for height and weight. Common sense tells us that taller persons will be usually heavier. That is borne out the plots above as the points cluster around the diagonal. We can check this by calculating the _correlation_ between the  height and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation can be easily calculated using [pandas.Series.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.corr.html) or [numpy.corrcoef](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) build in functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['Height', 'Weight']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a symmetric correlation matrix. On the diagonal we always have 1.0. The off diagonal  entries correspond to correlation between two variables. As expected height and weight are highly correlated (the maximal value of correlation  coefficient is one). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to  reduce the correlations we will use the [Body Mass Index](https://en.wikipedia.org/wiki/Body_mass_index) instead of weight. Its idea was exactly to provide a height independent characterisation of persons weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['BMI'] = data['Weight']/data['Height']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have changed the data we have to redo the train test split. Using same seed will gives us exactly same sets as before which will be convenient for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data  = train_test_split(data,test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data['Height'],train_data['BMI'], alpha=0.2, c=color);\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('BMI');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot  still indicates rather strong correlation, but correlation coefficient is actually lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['Height', 'BMI']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look separately at each sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_F = train_data.loc[data['Gender']=='Female']\n",
    "train_data_M   = train_data.loc[data['Gender']=='Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data_F['Height'],train_data_F['BMI'], alpha=0.2, c = f_color, label='Women');\n",
    "plt.scatter(train_data_M['Height'], train_data_M['BMI'], alpha=0.2, c = m_color, label='Men');\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('BMI');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each cluster look less correlated, especially men. And indeed the correlation coefficients are lower  for each sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_F[['Height', 'BMI']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_M[['Height', 'BMI']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done  was to check for _conditional independence_. Two random variables $X$ and $Y$ are idenpendent conditioned on third variable $Z$ if "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X=x,Y=y|Z=z)=P(Y=y|Z=z)P(Y=y|Z=z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important point to note here is that two variables can be dependent on each other but be independent when coditioned on third variable.  This happens when _e.g._ both variables $X$ and $Y$ depend on $Z$ but not on each other.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example the number of ice creams sold and number of men in t-shirts on the street. Those variables are most probably correlated as they both depend on the air temperature. If we see lots of men in t-shirts it is probable that it is a nice warm day and consequently the ice creams sales are higher. If we condition on temperature that is  look only at the days with same air temperature we will probably find  that number of ice cream sold is idenpendent of number of men in t-shirts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not established that the height and BMI are independent conditioned on sex, but we how shown that that dependence is significantly reduced. This is a rationale behind _Naive Bayes_ classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have $k$ features $X_i$ that we want to use to predict class $C$. Approach that we have used so far would require knowledge of the join probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X_1=x_1,\\ldots,X_k=k_k|C=c)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Naive Bayes we __assume__ that those features are independent conditionned on class random variable $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X_1=x_1,\\ldots,X_k=x_k|C=c)=P(X_1=x_1|C=c)\\cdots P(X_k=x_k|C=c)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(C=c|X_1=x_1,\\ldots,X_k=x_k)= \\frac{P(X_1=x_1|C=c)\\cdots P(X_k=x_k|C=c) P(C=c)}{\\sum_c P(X_1=x_1|C=c)\\cdots P(X_k=x_k|C=c) P(C=c)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example this translates to (I have used $B$ and $b$ to denote BMI random variable and its value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(S=f|H=h, B = b) = \\frac{P(H=h|S=f)P(B=b|S=f)P(S=f)}\n",
    "{P(H=h|S=f)P(B=b|S=f)P(S=f)+\n",
    "P(H=h|S=m)P(B=b|S=m)P(S=m)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the height distribution for each sex can be approximated as normal. We have to check if this is also true for BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"BMI (m)\")\n",
    "ax.set_ylabel(\"P(bmi|S)\")\n",
    "train_data_F.BMI.hist(bins=32, density=True, ax=ax , color=f_color, alpha=0.7);\n",
    "train_data_M.BMI.hist(bins=32, density=True, ax=ax , color=m_color, alpha=0.7);\n",
    "bmis =np.linspace(15,33,100)\n",
    "pdf_bmi_F = st.norm(*st.norm.fit(train_data_F.BMI)).pdf\n",
    "ax.plot(bmis, pdf_bmi_F(bmis), c=f_color);\n",
    "pdf_bmi_M = st.norm(*st.norm.fit(train_data_M.BMI)).pdf\n",
    "ax.plot(bmis, pdf_bmi_M(bmis), c=m_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks resonable so we will stick with this choice. The function below  constructs the  $P(S=f|H=h, B=b)$ function  from height and BMI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pdf_P_cond_NB(labels, h, bmi ):\n",
    "    \n",
    "    positives = labels==1\n",
    "    negatives = labels==0\n",
    "    \n",
    "    pdf_h_P = st.norm(*st.norm.fit(h[positives])).pdf\n",
    "    pdf_h_N = st.norm(*st.norm.fit(h[negatives])).pdf \n",
    "    \n",
    "    pdf_bmi_P = st.norm(*st.norm.fit(bmi[positives])).pdf\n",
    "    pdf_bmi_N = st.norm(*st.norm.fit(bmi[negatives])).pdf                  \n",
    "                      \n",
    "    \n",
    "    P_P = labels.mean()\n",
    "    P_N = 1-P_P\n",
    "    \n",
    "    def pdf(ha, bmia):\n",
    "        p_prod = pdf_h_P(ha)*pdf_bmi_P(bmia)*P_P\n",
    "        n_prod = pdf_h_N(ha)*pdf_bmi_N(bmia)*P_N\n",
    "        \n",
    "        \n",
    "        return p_prod/(p_prod +n_prod)\n",
    "        \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_prob = make_pdf_P_cond_NB(train_data['Gender']=='Female', train_data.Height, train_data.BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cm  = confusion_matrix(test_data.Gender=='Female', nb_prob(test_data.Height, test_data.BMI) )\n",
    "nb_tpr, nb_fpr, _,_ = rates(nb_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(nb_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fprs, nb_tprs, nb_thds = roc_curve(test_data.Gender=='Female', nb_prob(test_data.Height, test_data.BMI) )\n",
    "nb_auc = roc_auc_score(test_data.Gender=='Female', nb_prob(test_data.Height, test_data.BMI) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = roc_plot(plt)\n",
    "ax.plot(fprs, tprs, label='Heigh')\n",
    "ax.scatter([fpr],[tpr], color='blue', zorder = 5)\n",
    "ax.plot(nb_fprs, nb_tprs,  label = \"Heigh & BMI\")\n",
    "ax.scatter([nb_fpr],[nb_tpr], color='orange', zorder = 5)\n",
    "ax.text(0.6,0.8,\"AUC = {:.2f} NB AUC = {:.3f}\".format(auc, nb_auc))\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see adding weight  feature in form of the BMI substantially increased the quality of our classifier. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}