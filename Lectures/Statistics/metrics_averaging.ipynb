{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics averaging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will explain  in more detail what is the meaning of the `average` parameter in the scikit-learn scores and how to use the `confusion_matrix` function. \n",
    "I will discuss only the binary classifiers and for this purpose I will use a simple  Gaussian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 100\n",
    "n_neg = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = norm(0,0.9).rvs(size=n_pos)\n",
    "negatives = norm(1,0.5).rvs(size=n_neg)\n",
    "features = np.concatenate((positives, negatives))\n",
    "labels = np.concatenate((np.ones_like(positives), np.zeros_like(negatives))).astype('int32')\n",
    "\n",
    "train_f, test_f, train_l, test_l = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "gaus = GaussianNB()\n",
    "\n",
    "gaus.fit(train_f.reshape(-1,1), train_l)\n",
    "test_pred = gaus.predict(test_f.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix gives most detailed information about the performance of the classifier. The $(i,j)$ entry of this matrix counts the number of instances of class $i$ classified as $j$. However different users use different conventions (orientations) as to whether the true labels correspond to rows or to columns. E.g. \n",
    "scikit-learn provides a confusion matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but its layout is diffrent from the [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) or my classification notebook! The rows of this matrix correspond to true labels and columns to  predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;font-size:14pt\">\n",
    "    <tr><td/><td/><th colspan=2>test_pred labels</th></tr>\n",
    "    <tr><td/><td/><th>N</th><th>P</th></tr>\n",
    "    <tr> <th rowspan=2> True labels</th><th>N</th>   <td> TN</td><td>FP</td> </tr>\n",
    "    <tr> <th>P</th>                                  <td> FN</td><td>TP</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this by calculating the entries \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(test_l>test_pred) # False negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(test_l<test_pred)# False positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((test_l==test_pred) & (test_l==1)) # True positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((test_l==test_pred) & (test_l==0)) # True negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_confusion_matrix` function labels the axes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gaus, test_f.reshape(-1,1), test_l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign all four quantities at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(test_l, test_pred).ravel() #ravel \"flattens\"  a multidimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix can be normalized in several ways. The `'true'` normalisation normalizes the rows (true labels) as to make the sum in each row equal to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred, normalize='true').sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the rates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;font-size:14pt\">\n",
    "    <tr><td/><td/><th colspan=2>test_pred labels</th></tr>\n",
    "    <tr><td/><td/><th>N</th><th>P</th></tr>\n",
    "    <tr> <th rowspan=2> True labels</th><th>N</th>   <td> TNR</td><td>FPR</td> </tr>\n",
    "    <tr> <th>P</th>                                  <td> FNR</td><td>TPR</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `'pred'` normalizes the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred, normalize='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred, normalize='pred').sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;font-size:14pt\">\n",
    "    <tr><td/><td/><th colspan=2>test_pred labels</th></tr>\n",
    "    <tr><td/><td/><th>N</th><th>P</th></tr>\n",
    "    <tr> <th rowspan=2> True labels</th><th>N</th>   <td> $\\frac{TN}{TN+FN}$</td><td>$\\frac{FP}{FP+TP}$</td> </tr>\n",
    "    <tr> <th>P</th>                                  <td> $\\frac{FN}{TN+FN}$</td><td>$\\frac{FP}{FP+TP}$</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) entry this corresponds to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;font-size:14pt;\">\n",
    "    <tr><td/><td/><th colspan=2>test_pred labels</th></tr>\n",
    "    <tr><td/><td/><th>N</th><th>P</th></tr>\n",
    "    <tr> <th rowspan=2> True labels</th><th>N</th>   <td> NPV</td><td>FDR</td> </tr>\n",
    "    <tr> <th>P</th>                                  <td> FOR</td><td>PPV</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"font-size:12pt;\"> \n",
    "    <tr><td> NPV = negative predictive value </td></tr>\n",
    "    <tr><td> FOR = false ommision rate </td></tr>\n",
    "    <tr><td> FDR = false discovery rate </td></tr>\n",
    "    <tr><td> PPV = positive predictive rate, precision </td></tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can normalize across the whole matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred, normalize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_l, test_pred, normalize='all').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check the orientation and normalization of the confusion matrix before interpreting its entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the scores were designed for binary classifiers with two classes  denoted traditionally as \"positives\" and \"negatives\". When there are more then  two labels there are no \"positives\" and \"negatives\". So what is done is that each class is in turn treated as positives and all other as negatives. Then we calculate the scores as for the binary classifier. That way obtain as many scores as there are classes. The averaging describes how those \"partial\" scores are combined in one finall score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  take recall as example. [Recall](https://en.wikipedia.org/wiki/Precision_and_recall) by definition is the true positives rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we use it with default value ('binary') for the `average` parameter we get just that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(recall_score(test_l, test_pred))\n",
    "print(recall_score(test_l, test_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classifier we have two ways of assigning  classes to positives and negatives. What we have done above with `average='binary'` was to  treat label \"one\" as positives and \"zero\" as negatives. If we switch this assignment, that is switch positives with negatives, the recall would be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this in the `recall_score` function by   explicitelly  indicating which label should be treated as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_l, test_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting no averaging returns scores for all possible assignments of the positives label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_l, test_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possibility of combining the scores is to take their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_l, test_pred, average=None).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That coresponds `'macro'` averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_l, test_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The micro averaging is slightly more complicated: we first calculate the average TP and  FN  for each  assignment and then use those averages to calculate the final score. Let's assume that $i$ is the label of the positive class. For each $i$ we can calculate the number true positives $TP_i$ and false negatives $FN_i$. We define the averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\overline{TP} = \\frac{1}{K}\\sum_i TP_i,\\quad \\overline{FN} = \\frac{1}{K}\\sum_i FN_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $K$ is the number of classes (two in our case). We then use  those averages to calculate the final recall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\overline{TP}}{\\overline{TP}+\\overline{FN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TP_0=TN,\\quad FN_0= FP,\\quad TP_1=TP,\\quad FN_1= FN$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\overline{TP}}{\\overline{TP}+\\overline{FN}} = \\frac{TN+TP}{TN+FP+TP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TN+TP)/(TN+TP+FN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed this is what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_l, test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the weighted averaging is like macro averaging but we weight the average by the support of each class _i.e._ the number of labels of each class. For binary classifier that is the number of negatives and the number of positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = np.asarray([TN+FP,TP+FN])\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted average is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(recall_score(test_l, test_pred, average=None) * support)/support.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_l, test_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "The weighted average gives same answer as micro averaging. Show that for recall score this is always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Recall (no pun intendend) that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$R_0 = \\frac{TN}{TN+FP},\\quad R_1 = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "The support is the number of positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$S_0=TN+FP,\\quad S_1 = TP+FN$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "The weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$\\frac{1}{TN+FP+TP+FN}\\left(\\frac{TN}{TN+FP}(TN+FP)+\\frac{TP}{TP+FN}(TP+FN)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "$$\\frac{TN+TP}{TN+FP+TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "which is exactly what we get for micro averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "### Take away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for binary classifiers the different averaging methods gives different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = ['binary', 'macro', 'micro', 'weighted']\n",
    "for a in average:\n",
    "    print(f\"recall(average='{a:s}') = {recall_score(test_l, test_pred, average=a):8.7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no \"error\" in using any of them, but you should be aware of the differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classification_report` function reports several scores listing the partial scores as well as averaged ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "print(classification_report(test_l, test_pred))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}